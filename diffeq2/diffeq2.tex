\documentclass[11pt, oneside]{article}   	% use ``amsart'' instead of ``article'' for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --\rangle pdf in pdflatex		
\usepackage{amssymb}
\usepackage{amsmath}

\usepackage{verbatim}
\usepackage{tikz} 

\usepackage{syntonly}
% \syntaxonly \langle -- use this for checking syntax only
% \mbox {text} - keep together
% \fbox {text} - keep together and draw around

%\pagestyle{plain|headings|empty} % header and footer p.27
%SetFonts
%\include{filename}, \includeonly{filename1, filename2} , \input[fiename}

%SetFonts% 

\title{Brilliant: Differential Equations II}
\author{Dave Fetterman}
\date{9/2/22}							% Activate to display a given date or no date

\begin{document}
\maketitle
Note: Latex reference: http://tug.ctan.org/info/undergradmath/undergradmath.pdf

\section{Chapter 1: Basics}
\subsection{Chapter 1: Nonlinear Equations}



The two types of problems in this course are: 


\begin{itemize}
\item Nonlinear equations (several equations on one independent variable)
\item Partial differential equations (single equation with several independent variables)
\end{itemize}

\textbf{Linear} equations have solutions like $y_1,y_2$ that can be combined using any $c \in \mathbb{R}$ like $y_1 + cy_2$.

\textbf{Example}: Bacteria in a dish with a lot of food, no deaths
\begin{itemize}
\item $b'(t) = r_bb(t), r_b > 0. r_b$ would be the rate of growth.
\item This is linear. Reason 1: $\frac{d}{dt}(y_1 +cy_2) = y_1' +cy_2' = r_b(y_1 +c_y2)$ since $y' = r_by(t)$, and same for y2.
\item Also, this works because the solution is $b(t) = b(0)e^{r_b t}$, so $b_1(t) + cb_2(t) = b_1(0)e^{r_b t} +cb_2(0)e^{r_b t} = (b_1(0) + cb_2(0))e^{r_bt}$
\end{itemize}

\textbf{Example}: \textbf{Logistic} equation: Bacteria in a dish with a lot of food, limited by carrying capacity $M$.
\begin{itemize}
\item $b'(t) = r_bb(t)[M-b(t)]$.
\item This is nonlinear. Reason: $\frac{d}{dt}(y_1' +cy_2') = y_1'+cy_2' = r_b[y_1 + cy_2][M-y_1-cy_2] = My_1+Mcy_2-y_1^2-2cy_1y_2 -cy_1^2y_2^2$
\item $\neq My_1 - y_1^2 + Mcy_2-c^2y_2^2$ because of the extra $-2cy_1y_2$ term.
\end{itemize}

Sidebar: Note that this equation $b' = r_bb[M-b]$ is \emph{separable}, so it can be solved.
\begin{itemize}
\item $\frac{db}{dt} = rb[M-b]$
\item $\frac{db}{b(M-b)} = r dt$
\item $\frac{1}{M }(\frac{1}{b} + \frac{1}{M-b}) db = r dt$ after partial fractions work
\item $(\ln(b)-\ln(M-b)) = Mrt + C \Rightarrow \ln(\frac{b}{M-b}) = Mrt + C$
\item $\frac{b}{M-b} = e^{Mrt}e^{C}$
\item Initial conditions $b=b(0), t=0 \Rightarrow \frac{b}{M-b} = \frac{b(0)}{M-b(0)} e^{Mrt}$
\item  $b(1+\frac{b(0)}{M-b(0)} e^{Mrt}) = M \frac{b(0)}{M-b(0)} e^{Mrt}$
\item  $b(M-b(0)+b(0)e^{Mrt}) = M b(0) e^{Mrt}$
\item $b = \frac{Mb(0)e^{Mrt}}{M+b(0)[e^{Mrt} -1]}$
\end{itemize}


This logistic solution will taper off to M at some point. Note that $\lim_{t \rightarrow \infty} b(t) = M$ since the non-exponential terms stop mattering.  Also $b(t) = M$ sticks as a constant solution or \textbf{equilibrium} immediately.  \emph{These equilibria tell us what matters - the long-term behavior of solutions!}

Another \textbf{Example}: Lotka-Volterra equation pairs: Bacteria (b) and bacteria-killing phages (p), with kill rate $k$.
\begin{itemize}
\item The ``product'' $k b(t)p(t)$ measures the interactions and kills resulting from this.
\item $b'(t) = r_b b(t) - k p(t) b(t)$, or the normal growht rate minus kill rate
\item $p'(t) = kp(t)b(t)$ since its population grows as it kills bacteria.
\item Equilibria include $b = 0, p = 0$ and $b = 0, p > 0$, since these are \emph{constant} solutions, or places where $b'(t) = 0, p'(t) = 0$.
\end{itemize}

\textbf{Direction fields}, with vector pointing towards $\langle b'(t), p'(t) \rangle$ (TODO - I think) let us follow the arrows to determine the curve over time.  In this case, the bacteria will always go extinct.

\ 
However, if we add a new death rate term $-d_pp(t)$ so $p'(t) = -d_pp(t) + kp(t)b(t)$:
\begin{itemize}
\item We get an equilibrium at $b = \frac{d_p}{k}, p = \frac{r_b}{k}$. (Since $0 = b'(t) = rb - kpb, (\Rightarrow pk = r), 0 = p'(t)  -dp + kpb,(\Rightarrow bk = d)$)
\item But otherwise the solutions swirl around this point.  This is called a \textbf{cycle}.  TODO What is a \textbf{limit cycle}?
\end{itemize}

Note that there are systems where the ``solution particle'' neither reaches an equilibrium or cycles around one point.  The \textbf{Lorenz system} famously has this owl-eye shaped double attractor (an example of \textbf{strange sets}) where initially close particles diverge unpredictably if the constants $\rho, \sigma, b$ are chosen right:

\begin{itemize}
\item $x'(t) = \sigma(y-x)$
\item $y'(t) = x(\rho - z) - y$
\item $z'(t) = xy-bz$
\end{itemize}

\begin{itemize}
\item TODO
\end{itemize}

\subsection{Chapter 1.2: PDEs}

Many methods of attack for PDEs

\begin{itemize}
\item Separation of variables
\item Power series (Note: did we actually touch on this?)
\item Fourier Transform
\end{itemize}

\textbf{Example}: Standing wave, where one end of a rope is fixed.  
\begin{itemize}
\item Vertical displacement from a line of rope: $u(x,t)$ depends on where ($x$) and when ($t$t).
\item Rope's \textbf{wave equation} is $u_{tt} = v^2u_{xx}$, where $v$ is the ``constant wave speed'', and the others are the space, time partials.
\item Note that $u = \cos(vt)\sin(x)$ and $u = \sin(vt)cos(x)$ both work.
\item If you guess the solution has split variables like $u = X(x)Y(y)T(t)$, then, upon substitution and division by $X(x)Y(y)T(t)$, $\frac{\delta^2 u}{\delta t^2} = v^2 [\frac{\delta^2 u}{\delta x^2} + \frac{\delta^2 u}{\delta y^2}]$ yields $\frac{T''(t)}{T(t)} = v^2 [\frac{X''(x)}{X(x)} + \frac{Y''(y)}{Y(y)} ]$
\item This method may or may not work.  But if it does, it means that since $x, y, $ and $t$ are independent variables, each individual piece must be constant.
\item So, for example, if we know $\frac{X''(x)}{X(x)} = -4\pi^2$, we can get to $X(x) = \sin(2\pi x)$
\item The wave equation is similar in 3D: $u_{tt} = v^2[u_{xx}+u_{yy}+u_{zz}]$, or using the Laplacian, $u_{tt} = v^2\nabla^2 u$.  Here, $u$ measures not displacement but expansion/compression of air at $(x,y,z)$, time $t$.
\end{itemize}


Using Fourier transforms helps turn difficult PDEs into an easier problem like an ODE.  \emph{Fourier transforms work best} when 
\begin{itemize}
\item The domain is all of $\mathbb{R}^n$
\item The function $u$ vanishes at infinity.
\end{itemize}
 
The Fourier transform changes the domain of $x$ to that of $\omega$.  It comes with the (highly simplified) rule (see Vector Calculus course): $F[\frac{\delta f}{\delta x}] = i \omega F[f]$.  
\textbf{Example}: Drunkard's walk.
\begin{itemize}
\item One dimensional: moves left or right in a random way.  Starts at $x=0, t=0$.
\item $u(x,t)$ is probability of being at point $x$ at time $t$.  Naturally, $\int_{x=-\infty}^{x=\infty} u(x,t)dx = 1$.
\item Also, it obeys the 1-dD diffusion equation $\frac{\delta u}{\delta t} = \frac{\delta ^2 u}{\delta x^2}$
\item The Fourier transform doesn't affect $t$ at all.
\item So by taking Fourier transform of both sides of diffusion equation we get 
\begin{itemize}
\item $F(u_t)=  \frac{\delta}{\delta t} F(u) $ since $F$ doesn't care about $t$.
\item $\frac{\delta ^2 u}{\delta x^2} = i\omega F(\frac{\delta u}{\delta x}) = -\omega^2 F(u)$
\item So $  \frac{\delta}{\delta t} F(u)  = -\omega^2 F(u)$
\item This is solvable as $F(u) = ce^{-\omega^2 t}$.  Take it on faith that $c = \frac{1}{2\pi}$ for now. TODO
\item Known fact: $F[Ae^-{\frac{ax^2}{2}}] = \sqrt{\frac{1}{2 \pi a}} Ae^{\frac{-\omega^2}{2a}}$
\item This means $t = \frac{1}{2a}$ and $a = \frac{1}{2t}$
\item $F(u)  = 	\frac{1}{2\pi}e^{-\omega ^2 t}, F[Ae^-{\frac{ax^2}{2}}] = \sqrt{\frac{1}{2 \pi a}} Ae^{\frac{-\omega^2}{2a}}$ so $u  = Ae^{\frac{-ax^2}{2}}$
\item Solving, you get $A = \sqrt{\frac{1}{4\pi t}}, a=\frac{1}{2t}$, so $u(x,t) = \sqrt{\frac{1}{4\pi t}} e^{-\frac{x^2}{4t}}$
\end{itemize}

\end{itemize}


\section{Chapter 2: Nonlinear Equations}
\subsection{2.1: Lotka-Volterra I}


Major ideas:
\begin{itemize}
\item \textbf{phase plane}: TODO
\item \textbf{nullcline}: TODO
\item \textbf{direction field}: TODO
\item \textbf{equilibria}: TODO
\end{itemize}
 
\textbf{Example}: Bacteria vs. phages (again)
\begin{itemize}
\item Bacteria unrestrained grow in proportion to their population, so $\frac{db}{dt} = r_b b(t)$ (solved: $b(t) = b(0)e^{r_b t}$)
\item Phages unfed decrease in proportion to current size, so $\frac{dp}{dt} = -d_p p(t)$  (solved: $p(t) = p(0)e^{-d_p t}$)
\item Bacteria die with likelihood of meeting a phage, and phages increase with likelihood of meeting a bacterium.  So the set of equations, for constant $k$, becomes:
\begin{itemize}
\item $b'(t) = r_b b(t) - kb(t)p(t)$
\item $p'(t) = -d_p p(t) + kb(t)p(t)$
\item \emph{The product of p and b makes our equations nonlinear} (WHY?)
\item I guess, very generally, $b_1p_1 = k, b_2p_2 = k,$ but $(b_1+b_2)(p_1+p_2) = b_1p_1 + b_2p_2 + b_1p_2+b_2p_1 = 2k + b_1p_2+b_2p_1 \neq 2k$, so the last two ``mixed'' terms mean you can't just add solutions $(b_1, p_1)$ and  $(b_2, p_2)$.
\end{itemize}
\end{itemize}

General thoughts on this solution:
\begin{itemize}
\item So a solution $(b(t), p(t))$, traces out a curve on the bp-phase plane (b is x-axis, p is y-axis) as time (unrepresented in the plane) continues.
\item If we add a unit tangent vector at every point $(B, P)$ aligned with $(b'(t), p'(t)) = ( r_bB - kBP, -d_p P + kBP)$, we can follow the arrows to see the solution over time.
\item The above is called a \textbf{direction field}
\item This is sometimes hard to sketch analytically, so we can look to the \textbf{nullclines}: places where one of the components of the direction field is zero.
\item In this case, $r_bB - kBP = (r_b-kP)B = 0$ when $P = 0$ or $P = \frac{r_b}{k}$, and $-d_p P + kBP = (kB-d_p)P = 0$ when $P=0$ or $B = \frac{d_p}{k}$.
\item The \textbf{upshot of nullclines} (since we don't care about $P, B \leq 0$): The lines $B = \frac{d_p}{k}, P = \frac{r_b}{k}$ \emph{divide the plane into pieces where the components of this (continuous) function pair can't change sign}.  
\item For instance, $B > \frac{d_p}{k}, P <  \frac{r_b}{k}$  means $r_b b - kbp > 0, -d_p p+ kdp > 0$, so both populations are growing here.  This helps to sketch the curve.
\item The curve looks like a counterclockwise whirlpool around the $(B, P) = ( \frac{d_p}{k}, \frac{r_b}{k})$.  (bacteria grow with low but growing phages; bacteria decrease as phages overwhelm; both decrease as phages starve; bacteria start coming back)
\item The center point is a (constant \textbf{equilibrium}) solution, and other solutions swirl around it but don't get attracted or repelled.
\end{itemize}

There are a few types of equilibria:
\begin{itemize}
\item This one is a \textbf{center} around which solutions circle.
\item A \textbf{stable equilibrum} would see small upsets come back to an unchanging state.
\item An \textbf{unstable equilibrum} would see small upsets create wildly divergent paths.


\end{itemize}

\subsection{2.2: Lotka-Volterra II}

In the Bacteria-Phage system, we can't yet prove everything rotates around the \textbf{center}.  Let's do that.

Developing a \textbf{conserved quantity} will help to do that.  \textbf{Example}: Block on a horizontal spring with mass $m$, spring constant $k_s$:
\begin{itemize}
\item $x(t)$: Displacement from rest position.
\item $v(t) = \frac{dx}{dt}$: Horizontal velocity
\item $\frac{dv}{dt} = -\frac{k_s}{m}x(t)$ by Hooke's law, I think.
\item Suppose there's some Energy function $E(x,v)$.  By chain rule $\frac{d}{dt}E(x(t), v(t)) = \frac{dE}{dx}\frac{dx}{dt} +  \frac{dE}{dv}\frac{dv}{dt}$
\item $=  \frac{dE}{dx}v - \frac{k_s}{m} \frac{dE}{dv}x$.  If we set E as conserved, as in $E'(t) = 0$, then $\frac{dE}{dx}v = \frac{k_s}{m} \frac{dE}{dv}x$
\item We can eyeball and see that $E = \frac{1}{2}k_sx^2 +  \frac{1}{2}mv^2$ solves this equation, or we can assume $E(x,v) = F(x) + G(v) \Rightarrow 0 = E'(t) = F'(x)v - \frac{k_s}{m}G'(v)x = 0$ from the above equations and guess from there.
\item This means in the xv phase space, that there's a fixed E such that the particle follows the ellipse $E = \frac{1}{2}k_sx^2 +  \frac{1}{2}mv^2$ in phase space around the solution point (0,0).
\end{itemize}

\textbf{Extended Example}: Continuing on finding a conserved quantity for Bacteria / Phage:
\begin{itemize}
\item We need to find $U(b(t), p(t))$ such that $U'(t) = 0$, or by chain rule $\frac{\delta U}{\delta b}\frac{\delta b}{\delta t}  + \frac{\delta U}{\delta p}\frac{\delta p}{\delta t} = 0$
\item Subbing in, $\frac{\delta U}{\delta b}[r_bb-kbp]  + \frac{\delta U}{\delta p}[-d_pp+kbp] = 0$
\item A hint suggests finding $U$ such that $\frac{\delta U}{\delta b} = -\frac{d_p}{b} + k, \frac{\delta U}{\delta p} = -\frac{r_b}{p} + k$ to make terms cancel.
\item Integrating these gives us $U$ as both $-d_p\ln(b)+kb+Q(p)$ and $-r_b\ln(p)+kp+R(b)$ so $U = -d_p\ln(b)-r_b\ln(p)+kb+kp$.  This weird curve consistutes a level set in pb-space upon which a solution sits.
\item The spring example has an elliptic paraboloid solution.  There's an absolute minimum ($E = 0$ at $(0,0)$) but level sets become closed loops away from it.
\item For the Lotka example, there is a critical point ($\nabla U = \vec{0}$) when $\nabla U(b,p) = (\frac{\delta U}{\delta b}, \frac{\delta U}{\delta p}) = (k-\frac{d_p}{b}, k - \frac{r_b}{p})$, which is $(0, 0)$ at our known center $(\frac{d_p}{k}, \frac{r_b}{k})$
\item Showing that we always increase gong away from the point $(\frac{d_p}{k}, \frac{r_b}{k})$ should guarantee us closed level sets.
\item One method: Assume we're picking a unit vector $\vec{v} = \langle \hat{v_b}, \hat{v_p} \rangle$ so that our line from our center is $\vec{v} = \langle \frac{d_p}{k}+t{v_b}, \frac{r_b}{k}+t{v_b} \rangle$.  $U=F(b) + G(p)$ in this case, so sub the $b$ part into $F$ to get $F (\frac{d_p}{k} +t\hat{v_b}) = d_p[1-\ln(\frac{d_p}{k} + t\hat{v_b})]+kt\vec{v_b}$.  Taking derivative of that w.r.t $t$ shows it is always positive.  Same goes for the G(p) portion of U.
\item Another (DF) method: Note that $\nabla U = (k - \frac{d_p}{b}, k - \frac{r_b}{p})$'s grad (second derivative) is always positive.  So derivative always has positive curvature (maybe using that term wrong), and we'll always increase around this point.
\item Also, we know that the particle travels around the level set (loop) and doesn't reverse course, because then, $b'(t) = p'(t) = 0$, and we only have that at the center point (nullcline intersection).
\end{itemize}


\subsection{2.3: Linearization}

\textbf{Extended Example}: Suppose there's a limit to bacterial growth, so we cap our population at $M_b$.
\begin{itemize}
\item If $b(t) << M_b$, things should be similar.  If $b(t)$ is nearly $M_b$, then growth should approach 0.  So, this implies $\frac{db}{dt} = r_bb(t) \rightarrow \frac{db}{dt} = r_bb(t) (1-\frac{b(t)}{M_b})$.  Note: This isn't the only possibility but we'll use it.
\item This updates our Lotka-Volterra model to something more complicated:
\begin{itemize}
\item $b'(t) = r_bb(t) (1-\frac{b(t)}{M_b})- kb(t)p(t)$
\item $p'(t) = -d_p p(t) + kb(t)p(t)$
\end{itemize}
\item Other than $b=0, p=0$, the meaningful nullclines are solved by setting $b'(t) = 0$ (yielding $r_b(1-\frac{b}{M_b})-kp = 0$) and $p'(t) = 0$ (yielding $b = \frac{d_p}{k}$)
\item Note: We'll clean up through some MAGIC non-dimensionalization (how to derive?) to simplify:
\begin{itemize}
\item $x(t) = \frac{1}{M_b}b(\frac{t}{r_b}), y(t) = \frac{k}{r_b}(\frac{t}{r_b}), \alpha = \frac{d_p}{r_b}, \beta = \frac{kM_b}{r_b}$
\item Gives us new equations: $\frac{dx}{dt} = x(t)[1-x(t)] - x(t)y(t), \frac{dy}{dt} = -\alpha y(t) + \beta x(t)y(t)$
\item And new nullclines: $x+y=1, x=\frac{a}{b}$
\item So  there's an equilibrium point in the positive xy quadrant if: $y = 1-x = 1-\frac{\alpha}{\beta}$ and $y > 0$ implies $1-\frac{\alpha}{\beta} > 0 \Rightarrow \frac{\alpha}{\beta} < 1$
\item Looking at the direction field, it appears solutions swirl around and are attracted \emph{into} the center point $(\frac{\alpha}{\beta}, 1 - \frac{\alpha}{\beta})$, making it a \textbf{stable equilibrum}
\end{itemize}
\end{itemize}

This is similar to the block-spring example, if a damping term $-\frac{\gamma}{m}v$ is added.
\begin{itemize}
\item $\frac{dx}{dt} = v, \frac{dv}{dt} = -\frac{k_s}{m}x -\frac{\gamma}{m}v$
\item This can be thought of in matrix terms: $\frac{d}{dt} \begin{pmatrix} x(t) \\ v(t) \end{pmatrix} =  \begin{pmatrix} 0 & 1 \\ -\frac{k_s}{m} & -\frac{\gamma}{m} \end{pmatrix}  \begin{pmatrix}x(t) \\ v(t)\end{pmatrix}$ Call the matrix $A$.
\item From Diff Eq I, the solution is $exp(tA)$ (matrix exponential), making x(t) a linear combination of $e^{\lambda t}$ or possibly $te^{\lambda t}$ terms, with the eigenvalues as $\lambda$s.
\item The eigenvalues in this case, using the quadratic formula, could be:
\begin{itemize}
\item Two real, distinct, negative roots. So, these $e^{\lambda t}$ terms decay, and x(t) levels off.
\item Two distinct complex roots with real part $-\frac{\gamma}{2m} < 0$.  This ends up being some sines and cosines multiplied by $e^{-\frac{\gamma t}{2m}}$, which decays too.
\item Finally, if we have a repeated negative real eigenvalue, we have solution $x(t) = Ae^{-\frac{\gamma t}{2m}} + Bte^{-\frac{\gamma t}{2m}} $, also decaying.
\item So any disturbance in the spring will oscillate and come to rest at $x(t) = v(t) = 0$ quickly.
\end{itemize}

So with linear systems $\vec{x}'(t) = A\vec{x}(t)$, the eigenvalues determine what happens around the equilibrium point.  However, the \textbf{bacteria-phage model is non-linear}.  Here is \textbf{how we linearize} for nearby solutions in a nonlinear system:
\begin{itemize}
\item Set small disturbance $\delta x(t) << 1, \delta y(t) << $ so $x(t) = \frac{\alpha}{\beta} + \delta x(t), y(t) = 1-\frac{\alpha}{\beta} + \delta y(t)$
\item Since they're small, all powers like $\delta x(t)^2$ and $\delta x(t) \delta y(t)$ are considered basically zero.
\item So substitute $x(t) \rightarrow \frac{\alpha}{\beta} + \delta x(t), y(t) \rightarrow = 1-\frac{\alpha}{\beta} + \delta y(t)$ into our $\frac{dx}{dt}$ and $\frac{dy}{dt}$ equations.
\item This gives us the A solving $\frac{d}{dt} \begin{pmatrix} \delta x(t) \\ \delta y(t) \end{pmatrix} = A \begin{pmatrix}\delta x(t) \\ \delta  y(t)\end{pmatrix}$, which is $A = \begin{pmatrix} -\frac{\alpha}{\beta} & -\frac{\alpha}{\beta}  \\ \beta - \alpha & 0 \end{pmatrix}$ after working through the substitution.
\item Finding the eigenvalues here yields the same situation as the block-spring example: decays in all situations.
\end{itemize}

It turns out through the \textbf{Hartman-Grobman Theorem} that $\vec{x}'(t) = \vec{F}(\vec{x}(t))$, for some continuously differential vector field $F$, if we linearize near equilibrium $x_0$, then what falls out of this $A$ approach works if the eigenbalues \emph{aren't all purely imaginary}.

It turns out the uncapped bacteria system from before looks like $\frac{d}{dt} \begin{pmatrix} \delta x(t) \\ \delta y(t) \end{pmatrix} =  \begin{pmatrix} 0 & -1 \\ \alpha & 0 \end{pmatrix}  \begin{pmatrix} \delta x(t) \\ \delta y(t)\end{pmatrix}$ , with characteristic equation $\lambda ^2 + \alpha = 0, \alpha > 0$.  This means both values are imaginary, and we had to use the conserved quantity appraoach! 

\end{itemize}

\subsection{2.4: Hartman-Grobman Theorem}

\textbf{Extended Example}: Consider a phage that dies off quicky:
\begin{itemize}
\item $\frac{db}{dt} = r_bb(t)-k_bb(t)p(t), \frac{dp}{dt} = -r_pp(t) = 0 \cdot b(t)p(t)$, where $k_p$ is the zero (phages don't increase), and $k_b$ is still the kill factor for the bacteria.
\item In this base, $b(t) = p(t) = 0$ is the only equilibrium.
\item Non-dimensionalize as $x(t) = b(\frac{t}{r_b}), y(t) = \frac{k_b}{r_b}p(\frac{t}{r_b}), \alpha = \frac{r_p}{r_b}$
\item This makes the equations $x'(t) = x(t) - x(t)y(t), y'(t) = -\alpha y(t)$, and the nullclines therefore $x(t) = 0, y(t) = 1, y(t) = 0$
\item Looking at this six-section dierection field, we see that solutions exactly on the y-axis are attracted to equilibrium $(0,0)$, and other are repelled.
\item This makes sense since if the bacteria is 0, the phage die and approach $(0,0)$, otherwise the bacteria multiply and win (so it's a \emph{saddle point})
\item The way to tell: linearize the equations.  $\frac{d}{dt} \begin{pmatrix} x(t) \\ y(t) \end{pmatrix} \approx \begin{pmatrix} 1 & 0 \\ 0 & -\alpha \end{pmatrix}  \begin{pmatrix} x(t) \\ y(t) \end{pmatrix} $ since, if $x(t), y(t) << 1,  x(y)t(t) = 0$.  
\item Then the eigenvalues are $\lambda = 1, -\alpha$ so the solution is $Ae^t$,  $Be^{-\alpha t}$ for $x(t), y(t)$ (TODO respectively?) \textbf{Hartman-Grobman ensures this is the general solution}.
 \end{itemize}

However, let's solve directly and see if we come to the same result. 
\begin{itemize}
\item $y'(t) = -\alpha y(t), y(0) = y_0 \Rightarrow y(t) = y_0 e^{-\alpha t}$
\item With this in hand, $\frac{dx}{dt} = x(t) -= x(y)y(t) = x(t)[1-y_0e^{-\alpha t}]] , x(0) = x_0$ separates out to 
\begin{itemize}
\item $\frac{dx}{x} = [1-y_0e^{-\alpha t}]dt $
\item $\ln(x) = [t + \frac{y_0}{\alpha}e^{-\alpha t}] + C $
\item $x = e^Ce^t \exp(\frac{y_0}{\alpha}e^{-\alpha t})$
\item $x(0) = x_0 \Rightarrow e^C = x_0e^{-\frac{y_0}{\alpha}}$
\item $\Rightarrow x(t) = x_0 e^t \exp(\frac{y_0}{\alpha}(e^{-\alpha t} - 1))$
\end{itemize}

But how do we deform the phase plane so this looks linear?  We need some mapping $\vec{h}(x,y) = \langle u(x,y), v(x,y) \rangle$ that is continuous and invertible (so we don't ``damage'' the phase plane).  This is called a \textbf{homeomorphism}.
\begin{itemize}
\item So near the equilibrium $(0,0)$, the equations  $y'(t) = -\alpha y(t), y(0) = y_0 \Rightarrow y(t) = y_0 e^{-\alpha t}$ linearized for $\delta x, \delta y$ must be similar to those for $u(x(t), y(t), v(x(t), y(t))$
\item This means we need $\frac{du}{dt} = u, \frac{dv}{dt} = -\alpha v$
\item After doing the substitution, we see that $v = v_0e^{-\alpha t}$ exactly mimics $y(t) = y_0 e^{-\alpha t}$ for the phage solution.  So we take $v = y$.
\item Therefore, we know that since $u = u_0e^t$ and $x(ty) = x_0 \exp(t + \frac{y_0}{\alpha}(e^{-\alpha t} - 1))$, that we need $u(x(t), y(t) = u(x_0, y_0)e^t$
\item And this is satisfied if we guess $u(x,y) = xe^{-y}{\alpha}$ and work it out.
\item This function $\vec{h}(x,y) = (u, v) = \langle xe^{-\frac{y}{\alpha}}, y \rangle $ is invertible by $(x,y) = \langle ue^{\frac{v}{\alpha}} , v \rangle$, which is continuous.
\end{itemize}

\end{itemize}

\subsection{2.5: Application - Lasers}

Lasers create excited atoms, which then emit photos while transitioning to an unexcited state.  This system has a close analogue with the previous phages (like photons) and bacteria (like atoms) model.

\begin{itemize}
\item $n(t)$: number of photons in the laser; $r_g$: rate of photons gained (created by excited atoms transitioning to unexcited state); $r_l$: rate of photons lost (emitted)
\item $\Rightarrow \frac{dn}{dt} = r_g - r_l$ by definition.
\item We can assume we're losing a constant $k$ (kill?) portion of photons per unit time, so $ \frac{dn}{dt} = r_g - kn(t)$
\item $e(t)$: number of excited atoms (that will maybe create photons).  Atoms are excited by external energy pump.
\item Excited atoms radiate when meeting a photon (which survives the meeting)
\item So we can use the same setup from the bacteria: with $I$ the constant of meeting (intersect?), $r_g = Ie(t)n(t) \Rightarrow n'(t) = Ie(t)n(t) - kn(t)$
\end{itemize}

\textbf{Mini example: Assume no photons leave} (cap the end of the laser)

\begin{itemize}
\item $k = 0$ in this scenario.
\item So every meeting creates one more photon ($n \rightarrow n + 1$) while enervating one excited atom $(e \rightarrow e - 1)$.  This implies, equivalently:
\begin{itemize}
\item $e+n$ is a conserved quantity,
\item $e(t) + n(t) = e(0) + n(0),$
\item $[e(t) + n(t)]' = 0$
\item Then, if$ k=0,  n'(t) = Ie(t)n(t) - kn(t)$, and coupled with $e'(t) + n'(t) = 0$ above, we have $e'(t) = -Ie(t)n(t)$
\end{itemize}
\end{itemize}

\textbf{Extended example: Atoms spontaneously lose energy}.  This is actually what happens
\begin{itemize}
\item From quantum physics, we have a rate $s$ of atoms just (s)pontaneously losing energty.
\item We also have an energy (p)ump that energizes atoms with quantity $p$.
\item Then, our change in (e)xcited atoms is $e'(t) = p -s - Ie(n)(t)$
\item So our \textbf{final laser equations} are $e'(t) = p -s - Ie(n)(t), n'(t) = Ie(t)n(t)-kn(t)$ 
\item If we want to find the smallest $p$ guaranteeing $n \geq 1$ (there's at least one photo) at equilibrium ($e'(t) = n'(t) = 0$):
\begin{itemize}
\item $n'(t) = 0 \Rightarrow Ien = kn \Rightarrow n(Ie-k) = 0$.  If $n \neq 0, \Rightarrow e = \frac{k}{I}$
\item $e'(t) = 0 \Rightarrow Ien = p - se$
\item Together, $p - se = Ien = kn \Rightarrow kn + se = p \Rightarrow kn + s\frac{k}{I} = p$
\item $n \geq 1 \Rightarrow p \leq  k +  \frac{ks}{I}$
\item \textbf{Another tactic}: We could also assume we \emph{start out at equilibrium}, so $n_0, e_0$ are constant solutions.
\item Solving $n' = 0 = Ie_0n_0 - kn_0, e' = 0 = Ie_0n_0 - se_0 + p$, we find equilibria $n_0 = \frac{p}{k} - \frac{s}{I}, e_0 = \frac{k}{I}$
\item Then,  $n_0 \geq 1 \Rightarrow  \frac{p}{k} - \frac{s}{I} \geq 1 \Rightarrow p \geq k + \frac{ks}{I}$
\end{itemize}
\end{itemize}

\textbf{Non-dimensionalization time}:
\begin{itemize}
\item Scale against $e_0 (= \frac{k}{I}), n_0  (= \frac{p}{k} - \frac{s}{I})$ like this: $x(t) = \frac{n(\alpha t)}{n_0}, y(t) = \frac{e(\alpha t)}{e_0}$
\item NOTE: What does this do?  This makes (1,1) the equilibrium, as $x(t) = \frac{n_0}{n_0} = 1, y(t) = \frac{e_0}{e_0} = 1$ !
\item What $\alpha$ lets us tke $n' = Ien - kn, e'=-Ien - se + p$ and write 
\begin{itemize}
\item $\frac{dx}{dt} = x(t)y(t) - x(t)$
\item $\frac{dy}{dt} = \frac{1}{k}(\frac{pI}{k} -s)[1 - x(t)y(t)] + \frac{s}{k}[1-y(t)]$
\item $x' = \frac{\alpha n'(\alpha t)}{n_0} = xy - x = \frac{Ie(\alpha t)n(\alpha t)}{k n_0} - \frac{n(\alpha t)}{n_0}$
\item $\frac{\alpha Ien - \alpha kn(\alpha t)}{n_0} = \frac{Ie(\alpha t)n(\alpha t)}{k n_0} - \frac{n(\alpha t)}{n_0}$
\item $ \alpha Ie - \alpha k = \frac{Ie(\alpha t)}{k} - 1 \Rightarrow \alpha (Ie - k) = \frac{Ie-k}{k} \Rightarrow \alpha = \frac{1}{k}$
\item This solves the x equation, and I suppose it can be validated in the y equation (tediously).
\item If we chunk up our (somehow positive?) constants as $c = \frac{1}{k}(\frac{pI}{k} - s), d = \frac{s}{k}$, we end up with $y' = c[1-xy] + d[1-y]$
\item We only care about $x,y > 0$, so $x' = 0 = xy-x = x(y-1)$ implies $y=1$ is a nullcline
\item $y'  = 0 = c[1-xy] + d[1-y] = c - cxy + d - dy \Rightarrow c+d = y(d + cx) \Rightarrow y = \frac{c+d}{d + cx}$, a scaled and shifted hyperbola.
\end{itemize}

\textbf{Look at the solutions}:
\begin{itemize}
\item It appears we have a counterclockwise swirl around $(1,1)$, and nearby solutions tend toward this equilibrium.
\item Hartman-Grobman: rewrite our linearized solution in neighborhood of $(1,1)$ as $x(t) = 1 + \delta x(t), y(t) = 1 + \delta y(t)$
\item Using $x' = xy-x, y'=c[1-xy]+d[1-y]$ and $\frac{d}{dt}\begin{pmatrix} \delta x(t) \\ \delta y(t) \end{pmatrix} = A\begin{pmatrix} \delta x(t) \\ \delta y(t) \end{pmatrix}$, we can solve and write $A = \begin{pmatrix} 0 & 1 \\ -c & -c-d  \end{pmatrix} $
\item Eigenvalues: $\lambda = \frac{1}{2}(-c -d \pm \sqrt{(c+d)^2 - 4c})$
\begin{itemize}
\item If square root term is zero, we have repeated eigenvalue, so $\delta x(t), \delta y(t)$ are combos of $e^{-\frac{c+d}{2}}, te^{-\frac{c+d}{2}}$, which decays
\item If square root term is greater than zero, we have two distinct real, negative eigenvalues (since c, d are positive), so this decays.
\item If square root term is less than zero, we have distinct complex eigenvalues, but combos of $e^{-\frac{c+d}{2}} \cos (\frac{1}{2} \sqrt{-(c+d)^2+4c}), e^{-\frac{c+d}{2}} \sin (\frac{1}{2} \sqrt{-(c+d)^2+4c})$ decay too
\item Note : I suppose Hartman-Grobman can't work in purely imaginary eigenvalue scenario, because these kinds of functions don't converge or diverge without a term outside the $\sin$ or $\cos$ 
\item And in any case, since these lambdas aren't strictly imaginary, Hartman-Grobman works.
\end{itemize}

\end{itemize}

\end{itemize}

\subsection{2.6: Liapunov Equations}

We had some intuition that ``nearby'' solutions would fall into an equilibrium, but what does ``nearby'' mean?  \textbf{Liapunov Equations} help us here.  What is the ``basin of attraction''?

\begin{itemize}
\item Suppose we turn the pump off ($p = 0$), and set spontaneous enervation equal to photon leak $s = k$.
\item (TODO?) Somehow we can rescale to $\frac{dx}{dt} = Ie(t)n(t) - kn(t), \frac{dy}{dt} = -Ie(t)n(t) - kn(t)$ which (TODO??) gives us $\frac{dx}{dt} = xy-x, \frac{dy}{dt} = xy-y$
\item This means equilibria ($x' = y' = 0$) exist at $(0,0), (1,1)$
\item If we're turning the pump off, we're looking at equilibrium (0,0).  Linearizing, we get $x' = -\delta x, y'=-\delta y$, so a matrix of $\begin{pmatrix} -1 & 0 \\ 0 & -1 \end{pmatrix}$
\item With repeated non-imaginary (H-G applies!) eigenvalues $-1, -1$, we can see that both $e^{-t}, te^{-t}$ decay, and we get sucked into the origin.
\end{itemize}

But how do we prove this?  Let's find a conserved quantity $U'(x(t), y(t)) =  0$
\begin{itemize}
\item $U'(x(t), y(t)) =  \frac{\delta U}{\delta x}\frac{\delta x}{\delta t}  + \frac{\delta U}{\delta y}\frac{\delta y}{\delta t} = \frac{\delta U}{\delta x}x(y-1) + \frac{\delta U}{\delta y}y(x-1)$
\item Setting $\frac{\delta U}{\delta x}x = -x+1, \frac{\delta U}{\delta y}y = y-1$ makes this zero
\item Solving those two by separating variables and combining, we get $U = -x+y+\ln(|\frac{x}{y}|)$
\item So if we're stabiling $f = (x-y)$ (why?), we see $(x-y)' = x'-y' = (xy-x) - (xy-y) = x-y = f \Rightarrow f = e^{-t}$
\item With $x(0) = x_0, y(0) = y_0 \Rightarrow f(0) = x_0 - y_0, f = x(t) - y(t) =  (x_0 - y_0)e^{-t}$
\item How to express $y(t)$ while eliminating $x(t)$, knowing $x(y) - y(t) = (x_0 - y_0)e^{-t}$ and $U(x,y) = y-x + \ln(|\frac{x}{y}|)$ is conserved?  \textbf{The trick: $U(x_0, y_0) = U(x,y)$} since \emph{it doesn't change}!
\begin{itemize}
\item $y_0 - x_0 +  \ln(|\frac{x}{y}|) =  y - x + \ln(|\frac{x}{y}|) = -(x_0-y_0)e^{-t} + \ln(|\frac{x}{y}|) $
\item $(1-e^{-t})(y_0 - x_0)   = \ln(\frac{x / y}{x_0 / y_0}) $
\item Defining for convenience, $f = \exp((1-e^{-t})(y_0 - x_0) )$, then $f\frac{y}{y_0} = \frac{x}{x_0}$
\item Sub in to $x - y = (x_0 - y_0)e^{-t}: y[\frac{x_0}{y_0}f - 1] = (x_0 - y_0)e^{-t}$
\item Solve for $y: y = \frac{y_0(x_0-y_0)e^{-t}}{x_0f(t) - y_0}$ 
\item Combine with above to get $x = \frac{x_0(x_0-y_0)e^{-t}f(t)}{x_0f(t) - y_0}$
\end{itemize}
\item So with equilibria $(0,0), (1,1)$, the direction field computer plot shows us attracted to $(0,0)$ (no laser action) pretty much anywhere left and down from $(1,1)$ in the $x,y$ phase plane.
\item Apparently the linearized solutions near $0,0$ are $x_{lin} = x_0e^{-t}, y_{lin} = y_0e^{-t}$ (WHY?)
\item Looking above, if $(x_0 - y_0) \approx 0$, then $f(t) \approx 1$, and $x, y \rightarrow x_{lin}, y_{lin}$
\end{itemize}

On to \textbf{Liapunov} functions, which will tell us perhaps the size of the ``basin of convergence'', unlike Hartman-Grobman, which just says there is a neighborhood.

A \textbf{Liapunov} function $U(x,y)$ is 

\begin{itemize}
\item Continuously differentiable
\item With a unique minimum $(x_0, y_0)$, usually aligned to be $U$'s only zero.
\item $U'(x(t), y(t)) \leq 0$.  Everything ``flows downhill''; 
\item Tailor made for the problem, hard to find.
\end{itemize}


\textbf{Back to the rescaled laser example}

\begin{itemize}
\item $x'(t) = x(t)y(t) - x(t)$
\item $y'(t) = c[1-x(t)y(t)] + d[1-y(t)], c, d > 0$
\begin{itemize}
\item \textbf{Analogy: The damped-block spring system} $\frac{d}{dt} \begin{pmatrix} x(t) \\ v(t) \end{pmatrix} =  \begin{pmatrix} 0 & 1 \\ -\frac{k_s}{m} & -\frac{\gamma}{m} \end{pmatrix}  \begin{pmatrix}x(t) \\ v(t)\end{pmatrix}$ 
\item When $\gamma = 0$, we know $E(x,v) = \frac{1}{2}k_sx^2 + \frac{1}{2}mv^2$ is conserved when looking at $E'$
\item $\gamma = 0 \Rightarrow x' = v, v' = -\frac{k_s}{m}x$
\item $\frac{dE}{dt} = ( \frac{1}{2}k_sx^2 + \frac{1}{2}mv^2)' = 0$ since $\frac{1}{2}(k_s xx' + mvv' ) = \frac{1}{2}(kxv + mv\frac{-k}{m}x) = 0$
\item But if $\gamma \neq 0, \frac{d}{dt}E(x(t), v(t)) = \frac{d}{dt}[ \frac{1}{2}k_sx^2 + \frac{1}{2}mv^2] = kxx' + mvv'$
\item $= kx(v) + mv(\frac{-k_s}{m}x - \frac{\gamma}{m}v) = -\gamma v(t)^2 = \frac{dE}{dt}$
\item Total spring energy is then decreasing in the fluid.
\item Brilliant has Cool visualization of spiraling down into the "bowl" of $x,y$ with $E$ as the $z$ dimension, equilibrium $(0,0, 0)$
\item We need to choose a $\gamma$-fied $E-$like function that decreases for pairs $\delta x(t), \delta y(t)$. We can choose, like $E$, some $u(\delta x, \delta y) = \frac{1}{2}C_1[\delta x]^2 + C_2[\delta y]^2$.
\item Choosing $C_1 = c, C_2 = 1$ gives us $\frac{d}{dt}u(\delta x(t), \delta y(t)) = \frac{d}{dt}(\frac{c}{2}[\delta x(t)]^2 + \frac{1}{2}[\delta y(t)]^2)$
\item $= c(dx)(dx)' + dy(dy)' = c (dx)(dy) + dy(-c(dx)-(c+d)(dy)) = -(c+d)[\delta y(t)]^2$
\item So $u =  \frac{d}{dt}(\frac{c}{2}[\delta x(t)]^2 + \frac{1}{2}[\delta y(t)]^2)$ is an energy function that could work for the laser.
\end{itemize}

Finally, we want to construct a function that
\begin{itemize}
\item Doesn't increase (derivative negative) on any pairs $x, y > 0$ (pulls down)
\item Is near equal to  $u = \frac{c}{2}(x-1)^2 + \frac{1}{2}(y-1)^2$ near $(1,1)$. (the energy function for block-spring above)
\item With $x' -= xy-x, y'=c-cxy + d-dy$, plus the identity near $z \approx 1$ of $\ln(z) \approx (z-1)-\frac{1}{2}(z-1)^2$...
\item You can find $U(x,y) = c(x-1)+(y-1) -c\ln(x)-\ln(y)$ that satisfies all of these
\item It therefore shows that pumped laser solutions tend to equilibrium $(1,1)$ in the long term.
\end{itemize}

TODO: So this is enough to establish a convergence to an equilibrium?
\begin{itemize}
\item Find an equilibrium $(x_0, y_0)$
\item Find an energy function $u$ that decreases for all pairs ($\delta x(t) \delta y(t))$ near the minimum.
\item Find a Liapunov function $U$ function that decreases EVERYWHERE along $x(t), y(t)$ (in our domain, like $x,y > 0$)
\item Ensure that $U = u$ in the neighborhood of the equilibrium.
\item Then Liapunov's theorem somehow makes this work (TODO)?
\end{itemize}

\end{itemize}

\subsection{2.7: Dog chasing a duck (Limit Cycles)}

This is a pair of nonlinear equations to determine if a dog in the pond's interior catches a duck who skates along the border.
\begin{itemize}
\item Variables:
\begin{itemize}
\item $r_p$: Radius of pond.
\item  $\vec{r_H}$: Distance of duck to center (always the radius of the pond)
\item $\vec{l}$: Displacement of dog from duck, which is of some length $R$ at any point.
\item $\theta$: Duck's position in the lake (think polar coordinates)
\item $\phi$: Angle between $\vec{r_H}$ and $\vec{l}$.  
\item Duck always swims at  speed $r_p\theta'(t)$, and dog swims at $k > 0$ times this, or $kr_p\theta'(t)$.
\end{itemize}
\item Therefore $\vec{r_H} = \langle r_p \cos(\theta), r_p \sin(\theta) \rangle$. It's just the polar coordinates.
\item Doing some geometry gets you $\vec{l} = R \langle \cos(\theta + \phi, \sin(\theta + \phi) \rangle$
\item We can establish $\vec{T} = \vec{r_H} - \vec{l}$ and dog's speed squared $\|T'(t)\|^2 =( \vec{r_H}' - \vec{l}')\cdot ( \vec{r_H}' - \vec{l}') = \|\vec{r_H}'\|^2 + \|\vec{l}'\|^2 - 2\vec{r_H}'\vec{l}'$
\item Naturally, this  $\|T'\|^2$ is also equal to the constant $(kr_p\theta')^2$.  Our diff equations will fall out of these.
\item $\vec{r_H}^2 = r_p^2[\theta'(t)]^2$ since duck's speed is constant. $\vec{l} = (R')^2 + R^2[\theta'+\phi']^2$ after working it out.
\item Finally, after using identities $\cos(\theta + \phi) = \cos(\theta)\cos(\phi) -  \sin(\theta)\sin(\phi), \sin(\theta + \phi) = \cos(\theta)\sin(\phi) +  \sin(\theta)\cos(\phi)$, we can work out $-2\vec{r_H}'\vec{l}' = -2r_p\theta'[R\cos(\phi)(\theta' + \phi') + \sin(\phi)R']$
\item After rescaling $R$ to $\rho$ such that $\frac{R}{r_p} = \rho$ and diving our speed equation by constant $r_p\theta'$, we end up with speed equation $k^2 = [\rho(1+ \frac{d\phi}{d\theta} - \cos(\phi)]^2 + (\frac{d\rho}{d\theta} - \sin(\phi))^2$
\item 

We propose that there are some solutions here for the \textbf{pursuit equations}.  We'll ignore the generalized form an focus on one set
\begin{itemize}
\item $\rho(1+\frac{d\phi}{d\theta}) - \cos(\phi) = 0, \frac{d\rho}{d\theta} - \sin(\phi) = -k$ do work in the above.  (Doesn't prove others don't work)
\item This leaves our equations as $\frac{d\phi}{d\theta} = \frac{\cos(\phi)}{\rho}-1, \frac{d\rho}{d\theta} = -k + \sin(\phi)$
\item However, there \emph{aren't simple equilibria here}.  In no world with $k \neq 0$ does the dog sit still (or the duck).
\item Supposing $k<1$ and $R, \phi$ are fixed (dog never gets closer and just loops), this means he's going in a circle, since the two legs of a triangle ($\vec{l}, \vec{r_p}$) and the interior angle ($\phi$) are fixed, so this fixes length of the third leg, which is a radius
\item You can also use dog's position vectors $x(t) = r_p\cos(\theta) - R\cos(\theta + \phi),y(t)=  r_p\sin(\theta) - R\sin(\theta + \phi)$ and trig identities to prove $x(t)^2 + y(t)^2 = r_p^2+R^2 - 2r_pR\cos(\phi)$
\item If $k < 1$, then solving $ \frac{d\rho}{d\theta} = 0 = -k + \sin(\phi) \Rightarrow \sin(\phi) = k \Rightarrow \phi = \sin^{-1}(k)$ and $\rho = \cos(\phi) = \cos(\sin^{-1}(k)) = \sqrt{1-k^2}$
\begin{itemize}
\item Quick proof of $ \cos(\sin^{-1}(k)) = \sqrt{1-k^2}$:
\item  $\cos^2(\sin^{-1}(k)) +  \sin^2(\sin^{-1}(k)) = 1 \Rightarrow  \cos(\sin^{-1}(k)) = 1 -  \sin^2(\sin^{-1}(k))$
\item $= 1-k^2 \Rightarrow  \cos(\sin^{-1}(k)) = \sqrt{1-k^2}$
\end{itemize}

\item When $k < 1$, the direction field seems to have attractive equilibriua but \textbf{GOTCHA}: there are $\phi$ angles that differ by $2\pi$ units, so they're the same.  The direction field is a cylinder with circumference $2\pi$, and there are other solutions tracking toward $(\sin^{-1}(k), \sqrt{1-k^2})$
\item Linearizing, assume we are near our equilibirum point and $\phi = \sin^{-1}k + \delta \phi, \rho = \sqrt{1-k^2} + \delta \rho.$
\item We can also remember that $f(x+\delta x) \approx f(x) + f'(x)\delta x$ from calculus.
\item $\frac{d}{d\theta}[\delta \rho] = \frac{d}{d\theta}[\rho - \sqrt{1-k^2}] = \frac{d\rho}{d\theta} - \frac{d}{d\theta}\sqrt{1-k^2} = -k + \sin(\phi)$
\item $= -k + \sin(\sin^{-1}(k) + \delta \phi)$ and by the calculus rule $ \frac{d}{d\theta}[\delta \rho]  = -k + \sin(\sin^{-1}(k)) + \cos(\sin^{-1}(k))\delta \phi = \sqrt{1-k^2}\delta \phi$
\item And for $\frac{d}{d\theta}[\delta \phi] = \frac{d}{d\theta}\phi -  \frac{d}{d\theta} (\sin^{-1}(k)) = \frac{\cos(\phi)}{\rho} - 1$
\item Using multivariable hint $f(x + \delta x, y + \delta y) \approx f(x,y) + \frac{\delta f}{\delta x}\delta x +  \frac{\delta f}{\delta y}\delta y$,
\item $f = \frac{\cos(\sin^{-1}(k) + \delta \phi)}{\sqrt{1-k^2} + \delta \rho} -1  \approx \frac{\sqrt{1-k^2}}{\sqrt{1-k^2}} - 1 + \frac{-\sin(\sin^{-1}(k)}{\sqrt{1-k^2}} \delta \phi - \frac{\cos(\sin^{-1}(k))}{1-k^2} \delta \rho$
\item $=-\frac{k\delta \phi + \delta \rho}{\sqrt{1-k^2}}$
\item So$ \frac{d}{d\theta}\begin{pmatrix} \delta \phi \\ \delta \rho \end{pmatrix} \begin{pmatrix} -\frac{k}{\sqrt{1-k^2}} &-\frac{1}{\sqrt{1-k^2}} \\ \sqrt{1-k^2} & 0  \end{pmatrix} \begin{pmatrix} \delta \phi \\ \delta \rho \end{pmatrix}$, and the eigenvalues aren't purely imeginary, and the real part is negative, so all decay.  Therefore, the equilibrium at $(\sin^{-1}(k), \sqrt{1-k^2})$ attracts\ nearby solutions.
\item There aren't solutions (for $K < 1$?), but numerically solved, the dog catches at $k > 1$, and for $k \leq 1$, swims out to a path approaching a circle.  This is a \textbf{limit cycle}, an isolated trajectory that closes on itself.

\end{itemize}
\end{itemize}

\subsection{Poincare-Bendixson Theorem}

Limit cycles in the real world: a chemical reaction in perpetual osciallation!

Key concept - \textbf{trapping region}: a region in phase plane on some region $D$, with differential solutions touching every point, where the direction field sees every boundary arrow point IN.  This means:
\begin{itemize}
\item The solution has to stay in D.
\item Any solution that self-intersects forms a cycle in the phase plane.
\item The three conceivable ways a solution can ``snake'' around forever (the \textbf{Poincare-Bendixson theorem} says it):
\begin{itemize}
\item Approaches a closed loop in $D$.
\item Approaches a fixed point in $D$ (possibly a special case of the last bullet)
\item Cycle: Snake eats its own tail
\end{itemize}
\item A non-cycling solution is the only other possibility - just a point equilibirum.
\end{itemize}

\textbf{Example}: Chemical oscillatory reaction.
\begin{itemize}
\item $x$ is concentration of $I^-$, $y$ is concentration of $ClO_2^-$ ions in some reaction.
\item $a$ is positive, and clearly $x, y \geq 0$ in the physical world.
\item Otherwise meaningless equations: $\frac{dx}{dt} = 5a - x - \frac{4xy}{1+x^2}, \frac{dy}{dt} = x(\frac{4y}{1+x^2})$
\item Solve for equilibria by setting $\frac{dx}{dt} = \frac{dy}{dt} = 0$
\begin{itemize}
\item Denote $Q = \frac{y}{1+x^2}$
\item First equation implies $x(1+4Q) = 5a$
\item Second euqation, plus knowing $x \neq 0$, $\Rightarrow x(1-Q) = 0 \Rightarrow Q = 1$
\item $Q = 1 \Rightarrow 5x = 5a \Rightarrow x = a$
\item $\Rightarrow 1 = \frac{y}{1+x^2} \Rightarrow y = 1+a^2$
\item Only solution pair is $(a, 1+a^2)$
\end{itemize}
\end{itemize}

Linearizing the solution around $(a, 1+a^2)$
\begin{itemize}
\item $x = a + \delta x, y = 1 + a^2 + \delta y \Rightarrow \frac{dx}{dt} = \frac{d[\delta x]}{dt}, \frac{dy}{dt} = \frac{d[\delta y]}{dt}$
\item Call $f =  \frac{d[\delta x]}{dt} = 5a - x - \frac{4xy}{1+x^2}$, 
\item Approximate $f(x + \delta x, y + \delta y) \approx f(x,y) + \frac{\delta f}{\delta x}\delta x +  \frac{\delta f}{\delta y}\delta y$
\item $f(x,y)(a, 1+a^2)= 5a - x - \frac{4xy}{1+x^2}(a, 1+a^2) = 5a - a - 4(a\frac{1+a^2}{1+a^2}) = 0$
\item $\frac{\delta f}{\delta x}\delta x (a, 1+a^2) = (-1 - \frac{(1+x^2)(4y-2x4xy}{(1+x^2)^2})\delta x (a, 1+a^2)) =(-1 - 4 - \frac{8a^2}{1+a^2})\delta x  = \frac{-5 + 3a^2}{1+a^2}\delta x $
\item $\frac{\delta f}{\delta y}\delta y (a, 1+a^2) = \frac{-4x}{1+x^2}\delta y (a, 1+a^2)) =\frac{-4a}{1+a^2}\delta y $
\item Call $g =  \frac{d[\delta y]}{dt} = x - \frac{xy}{1+x^2}$
\item Approximate $g(x + \delta x, y + \delta y) \approx g(x,y) + \frac{\delta g}{\delta x}\delta x +  \frac{\delta g}{\delta y}\delta y$
\item $g(x,y)(a, 1+a^2) = x - \frac{xy}{1+x^2}(a, 1+a^2) = a - a\frac{1+a^2}{1+a^2} = 0$
\item $\frac{\delta g}{\delta x}\delta x (a, 1+a^2) = (1 -\frac{(1+x^2)y - xy2x}{(1+x^2)^2})\delta x ( = 1 -\frac{(1+a^2)^2 - 2a^2(1+a^2)}{(1+a^2)^2})\delta x = 2a^2\delta x $
\item $\frac{\delta g}{\delta y}\delta y (a, 1+a^2) = \frac{-x}{1+x^2}\delta y = \frac{-a}{1+a^2}\delta y $
\item  $\Rightarrow \frac{d}{dt}\begin{pmatrix} \delta x \\ \delta y \end{pmatrix}  = \frac{1}{1+a^2} \begin{pmatrix}  3a^2 - 5 & -4a \\ 2a^2 & -a \end{pmatrix} \begin{pmatrix} \delta x \\ \delta y \end{pmatrix}$
\item Let's arbitrarily choose $a = 2 \Rightarrow (a, 1+a^2) = (2,5)$.  The coefficient matrix ends up being $\frac{1}{5} \begin{pmatrix} 7 & -8 \\ 8 & -2\end{pmatrix}$, which has eigenvalues with a positive real $\pm$ some $i$ component.  So, Hartman-Grobman applies and we don't decay into our point but push away.
\end{itemize}

We want to \textbf{build the trapping region}.   
\begin{itemize}
\item Remember, $\frac{dx}{dt} = 10 - x - \frac{4xy}{1+x^2}, \frac{dy}{dt} = x(1 - \frac{y}{1+x^2})$ subbing in 2 for $a$)
\item On the left, if $x = 0$ we see $\frac{dx}{dt} = 10, \frac{dy}{dt}  = 0$.  So we're pointing right (into the first quadrant region)
\item On the bottom, if $y = 0$, we're pointing at $\langle 10 -x , x \rangle$ (into the region).
\item On the right, for some $x = b$, $10 - b - \frac{4b}{1+b^2}y$ will make sure we point left.
\item On the top, for some $y = c$, $x(1-\frac{c}{1+x^2} < 0$ makes sure we point down.
\item Assume, since we're encircling $(2,5)$, that $b \geq 3, c \geq 6$ for comfort.
\item To satisfy all of these, note $x(1-\frac{c}{1+x^2}) < 0 \Rightarrow 1 - \frac{c}{1+x^2} < 0 \Rightarrow c > 1+x^2, 0 < x < b \Rightarrow c > 1+b^2 \Rightarrow  \sqrt{c - 1 } > b $ 
\item And for $0 < y < c$, note that $10 - x - \frac{4xy}{1+x^2} < 10 - b < 0. $ 
\item Pick $b = 11$, say, implying $11 < \sqrt{c-1}$, so then $123 < c$.  So $(b,c) = (11, 124)$ ensures oscillation around $(2,5)$ without leaving that region.
\end{itemize}

Tricky: How to reduce this region?  No real way except simulation or some tricks.   If we PRESUME a cycle, we can prove the cycle extens to the left of $x=3$ or $ x_{min} < 3$
\begin{itemize}
\item \textbf{META trick}: Don't worry if you have unsolvable integrals - maybe you can cancel them out.  \textbf{Run with what you have.}
\item Trick: Assume $x(t + T) = x(t), y(t + T) = y(t)$ for some $T>0$, or that there's a PERIOD T.
\item $\int_0^T \frac{dx}{dt}dt = x(T) - x(0) = 0, \int_0^T \frac{dy}{dt}dt = y(T) - y(0) = 0$ by fundamental theorem.
\item Our equations again:  $\frac{dx}{dt} = 10 - x - \frac{4xy}{1+x^2}, \frac{dy}{dt} = x(1 - \frac{y}{1+x^2})$
\item So $0 = \int_0^T [10 - \int x(t) - 4\int \frac{x(t)y(t)} {1+x(t)^2}]dt$ by the first equation
\item $0 = \int x(t) - \int \frac{x(t)y(t)} {1+x(t)^2}]dt$ by the second.
\item Subtract four times the second from the first to get $0 = 10T - 5 \int_0^T x(t) \Rightarrow 2T = \int_0^T x(t)dt \geq int_0^T x_{min}dt = Tx_{min}$
\item So $2 \geq x_{min}$
\end{itemize}


\subsection{Chaos and the Lorenz Equation}

What enabled mathematical \textbf{chaos} (unpredictability in nonlinear differential equations) was really computers and seeing simulated solutions.

The (simplified) \textbf{Lorenz system} are these equations
\begin{itemize}
\item $\frac{dx}{dt} = \sigma (y-x)$
\item $\frac{dy}{dt} = x(\rho - z) - y$
\item $\frac{dx}{dt} = xy-bz$
\item All with $\sigma, \rho, b > 0$
\end{itemize}

Solving the equations, we see equilibria for these are:
\begin{itemize}
\item $(0,0)$ always
\item The two solutions $(\pm \sqrt{b(\rho - 1)}, \pm \sqrt{b(\rho - 1)}, \rho - 1)$ when $\rho > 1$.
\end{itemize}

Looking at $0 < \rho < 1$ specifically:
\begin{itemize}
\item Linearaizing is simple, with $x(t)= \delta x(t), y(t)= \delta y(t), z(t)= \delta z(t)$ and linearized system:
\item $\frac{d[\delta x]}{dt} = \sigma(\delta y - \delta x)$
\item $\frac{d[\delta y]}{dt} = \rho \delta x - \delta y$
\item $\frac{d[\delta z]}{dt} = -b \delta z$
\item
 $\frac{d}{dt} \begin{pmatrix} \delta x \\ \delta y \\ \delta z  \end{pmatrix} \approx  \begin{pmatrix} -\sigma & \sigma & 0 \\
 \rho & -1 & 0 \\ 0 & 0 & -b \end{pmatrix}  \begin{pmatrix} \delta x \\ \delta y \\ \delta z  \end{pmatrix} $
 \item Characteristic equation is $(-b - \lambda)[(1+\lambda)(\sigma + \lambda) - \sigma \rho]  = 0$
 \item Eigenvalues are $-b < 0$ and $\lambda = \frac{1}{2}[-(\sigma + 1) \pm \sqrt{(\sigma + 1)^2 - 4\sigma (1-\rho)}]$
 \item If $\rho < 1$, we have distinct, real, negative eigenvalues, and a locally attractive equilibrium by Hartman-Grobman.
\end{itemize}

But if $\rho < 1$ globally attactive?  Find a Liapunov function.

\begin{itemize}
\item Requirement is that the function $U(x(t), y(t), z(t))$ is minimized at the equilibrium, and that as time progresses, $U$ decreases (so we're sucked into the bowl)
\item We suppose that $U(x,y,z) = ax^2 + y^2 + z^2$ and using $xy \leq \frac{1}{2}x^2+ \frac{1}{2}y^2$:
\begin{itemize}
\item Identity derivation: $0 \leq (x-y)^2 \Rightarrow 0 \leq x^2 -2xy + y^2 \Rightarrow xy \leq \frac{1}{2}(x^2 + y^2)$
\end{itemize}
\item $\frac{\delta U}{\delta x} x'(t)+ \frac{\delta U}{\delta y} y'(t)+ \frac{\delta U}{\delta z}z'(t) = 2ax\sigma (y -x) + 2yx(\rho - z)-2y^2x+2zxy-2bz^2$
\item $=2(a\sigma + \rho)xy - 2a\sigma x^2 -2y^2-2bz^2$
\item \textbf{GOTHCA}: We can't choose $a = -\frac{\rho}{\sigma}$ since then $U = -\frac{\rho}{\sigma}x^2+y^2+z^2$ isn't minimized at $(0,0,0)$!  So $a$ needs to be positive.
\item Choosing $a = \frac{1}{\sigma} \Rightarrow a\sigma = 1$, with $\rho < 1 \Rightarrow
2(a\sigma + \rho)xy - 2a\sigma x^2 -2y^2-2bz^2 < 2a\sigma(2xy - x^2-y^2) - 2bz^2 \leq -2bz^2$ by the identity above.
\item Then $U = \frac{1}{\sigma}x^2+y^2+z^2$ decreases as $t \rightarrow \infty$ and is minimized at the globally attractive $(0, 0, 0)$
\end{itemize}

If $\rho > 1$ things get chaotic.  Instead of one equilibrium, we have two new ones at $(\pm \sqrt{b(\rho - 1)}, \pm \sqrt{b(\rho - 1)}, \rho - 1)$. Everything \textbf{bifurcates}, or qualitatively shifts when inching past $\rho = 1$:
\begin{itemize}
\item We have three equilibria.
\item The origin turns into a saddle equilibrium.
\item Linearizing around   $(\alpha, \alpha , \rho - 1)$ with $\alpha$ denoting  $\sqrt{b(\rho - 1)}$ (pretty straightforward), we get characteristic equation for $A$ of $-\lambda^3 - (\sigma + b + 1)\lambda^2 - b(\sigma + \rho)\lambda - 2\sigma b(\rho - 1) = 0$
\item Problem is, setting $\rho = 1$ drops the $(\rho - 1)$ term and we have $-\lambda(\lambda^2+(\sigma +1 + b)\lambda + b(\sigma + 1)) = 0$, with solutions $\lambda = 0, -b, -\sigma -1$.
\item The last two solutions are attractive, but the zero doesn't work for Hartman-Grobman.
\item If we set $\lambda = (\rho - 1)\Delta r$ when nudging $\rho$ just over 1, we ignore all $\lambda^2, \lambda^3$... as negligible and get
 $-b(\sigma + \rho) (\rho - 1)\Delta r - 2\rho b(\rho - 1) \approx 0$
 \item This means $\Delta r \approx -\frac{2\rho}{\rho + \sigma}$, or that this nudged root has to be negative when $\rho$ is near $1$.
 \item More rigorously, we could have proven the roots of the equation are negative for small $\rho - 1 > 0$
 \item In any case, this means that the near-zero root is negative, so   $(\alpha, \alpha , \rho - 1)$ attracts locally.
 \item We can show that this applies the same for   $(-\alpha, -\alpha , \rho - 1)$ 
\end{itemize}

How do equilibria change as we change $\rho$?
\begin{itemize}
\item We saw the What about as we dial past $\rho = 1$, our origin equilibrium changes from globally attractive to saddle point.
\item In going from a stable equilibrium with negative real-part eigenvalues (attractors) to $(0,0,0)$ as a saddle (mix of negative and positive real parts), we necessarily have a point where the eigenvalues' real parts are zero.
\item In other words, $\lambda = ia$ for some real $a$.
\item Subbing $ia$ into our $-\lambda^3 - (\sigma + b + 1)\lambda^2 b(\sigma + \rho)\lambda - 2\sigma b(\rho - 1) = 0$, we end up getting 
 $[(\sigma + b + 1)a^2 - 2\sigma b(\rho - 1)] + i[a^3-(b(\sigma + \rho)a]= 0$
\item Then we need $a^3 - b(\sigma + \rho)a = 0 \Rightarrow a = 0, a = \pm, \sqrt{b(\sigma + \rho)}$
\item If $a = 0$. the real part isn't zero.  But subbing $a = \pm \sqrt{b(\sigma + \rho)}$ gives us solutions for a set of $\rho = \frac{\sigma(\sigma + b + 3)}{\sigma - b - 1}$
\item So, when moving past this value, our two new equilibria change from locally attractive to saddles too.
\end{itemize}

Can we create a trapping region?
\begin{itemize}
\item The hint: The solutions have to pass through every ellipsoid of form $R^2 = \rho x^2 + ]sigma y^2 + \sigma (z-2p)^2$
\item What we need to prove: At every point on the boundary, the direction field points ``in'', or more specificlally, \emph{the angle between inward normal and direction field is acute}.
\item This also means that the gradient $\nabla g$ of the level set $R^2 = \rho x^2 + \sigma y^2 + \sigma (z-2r\rho)^2$ is the normal. This is $\langle 2\rho, 2\sigma, 2(z-2\rho)\rangle$
\item So $-\nabla g \cdot \langle \frac{dx}{dt},\frac{dy}{dt},\frac{dz}{dt} \rangle > 0 \Rightarrow ... \Rightarrow 2 \rho\sigma x^2+2\sigma y^2 + 2\sigma z^2-4\rho\sigma z > 0$
\item Use $R^2 - \rho x^2 -\sigma (z-2p)^2  =  \sigma y^2 \Rightarrow \rho\sigma x^2+2\sigma y^2 + 2\sigma z^2-4\rho\sigma z > 0$
\item This simplifies the dot product to $2R^2 - 8\rho^2\sigma + 4\sigma \rho z + 2 \rho (\sigma-1)x^2 > 0$
\item Since the $x^2$ term is always positive, we just need to set $R$ to clear zero when $z$ is its most negative (x = 0, y = 0, z = $2\rho - \frac{R}{\sqrt{\sigma}}$).  If we churn a little more we can see that setting $R > 2\sqrt{\sigma}\rho$ will provide a trapping region.

\end{itemize}

Question: Do the confined solutions fill up the whole (ellipsoid) container?  
\begin{itemize}
\item Looking at the divergence (volume change of a cube over time) of the solution will tell us.
\item For unspecified reasons, $\frac{1}{v(t)}v'(t) = \nabla \cdot \langle \sigma(y-x), x(\rho - z) - y, xy-bz \rangle = -\sigma -1 - b$
\item Solving, $v(t) = v(0)e^{-t(\sigma + 1 + b)}$
\item This means the volume decays to 0, so therefore, our line is confined to a smaller and smaller space (but not just a point, I guess?)
\end{itemize}


\begin{itemize}
\item TODO
\end{itemize}


\end{document}


