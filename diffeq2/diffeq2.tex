\documentclass[11pt, oneside]{article}   	% use ``amsart'' instead of ``article'' for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --\rangle pdf in pdflatex		
\usepackage{amssymb}
\usepackage{amsmath}

\usepackage{verbatim}
\usepackage{tikz} 

\usepackage{syntonly}
% \syntaxonly \langle -- use this for checking syntax only
% \mbox {text} - keep together
% \footnote{¥}box {text} - keep together and draw around

%\pagestyle{plain|headings|empty} % header and footer p.27
%SetFonts
%\include{filename}, \includeonly{filename1, filename2} , \input[fiename}

%SetFonts% 

\title{Brilliant: Differential Equations II}
\author{Dave Fetterman}
\date{9/2/22}							% Activate to display a given date or no date

\begin{document}
\maketitle
Note: Latex reference: http://tug.ctan.org/info/undergradmath/undergradmath.pdf

\section{Chapter 1: Basics}
\subsection{Chapter 1: Nonlinear Equations}



The two types of problems in this course are: 


\begin{itemize}
\item Nonlinear equations (several equations on one independent variable)
\item Partial differential equations (single equation with several independent variables)
\end{itemize}

\textbf{Linear} equations have solutions like $y_1,y_2$ that can be combined using any $c \in \mathbb{R}$ like $y_1 + cy_2$.

\textbf{Example}: Bacteria in a dish with a lot of food, no deaths
\begin{itemize}
\item $b'(t) = r_bb(t), r_b > 0. r_b$ would be the rate of growth.
\item This is linear. Reason 1: $\frac{d}{dt}(y_1 +cy_2) = y_1' +cy_2' = r_b(y_1 +c_y2)$ since $y' = r_by(t)$, and same for y2.
\item Also, this works because the solution is $b(t) = b(0)e^{r_b t}$, so $b_1(t) + cb_2(t) = b_1(0)e^{r_b t} +cb_2(0)e^{r_b t} = (b_1(0) + cb_2(0))e^{r_bt}$
\end{itemize}

\textbf{Example}: \textbf{Logistic} equation: Bacteria in a dish with a lot of food, limited by carrying capacity $M$.
\begin{itemize}
\item $b'(t) = r_bb(t)[M-b(t)]$.
\item This is nonlinear. Reason: $\frac{d}{dt}(y_1' +cy_2') = y_1'+cy_2' = r_b[y_1 + cy_2][M-y_1-cy_2] = My_1+Mcy_2-y_1^2-2cy_1y_2 -cy_1^2y_2^2$
\item $\neq My_1 - y_1^2 + Mcy_2-c^2y_2^2$ because of the extra $-2cy_1y_2$ term.
\end{itemize}

Sidebar: Note that this equation $b' = r_bb[M-b]$ is \emph{separable}, so it can be solved.
\begin{itemize}
\item $\frac{db}{dt} = rb[M-b]$
\item $\frac{db}{b(M-b)} = r dt$
\item $\frac{1}{M }(\frac{1}{b} + \frac{1}{M-b}) db = r dt$ after partial fractions work
\item $(\ln(b)-\ln(M-b)) = Mrt + C \Rightarrow \ln(\frac{b}{M-b}) = Mrt + C$
\item $\frac{b}{M-b} = e^{Mrt}e^{C}$
\item Initial conditions $b=b(0), t=0 \Rightarrow \frac{b}{M-b} = \frac{b(0)}{M-b(0)} e^{Mrt}$
\item  $b(1+\frac{b(0)}{M-b(0)} e^{Mrt}) = M \frac{b(0)}{M-b(0)} e^{Mrt}$
\item  $b(M-b(0)+b(0)e^{Mrt}) = M b(0) e^{Mrt}$
\item $b = \frac{Mb(0)e^{Mrt}}{M+b(0)[e^{Mrt} -1]}$
\end{itemize}


This logistic solution will taper off to M at some point. Note that $\lim_{t \rightarrow \infty} b(t) = M$ since the non-exponential terms stop mattering.  Also $b(t) = M$ sticks as a constant solution or \textbf{equilibrium} immediately.  \emph{These equilibria tell us what matters - the long-term behavior of solutions!}

Another \textbf{Example}: Lotka-Volterra equation pairs: Bacteria (b) and bacteria-killing phages (p), with kill rate $k$.
\begin{itemize}
\item The ``product'' $k b(t)p(t)$ measures the interactions and kills resulting from this.
\item $b'(t) = r_b b(t) - k p(t) b(t)$, or the normal growht rate minus kill rate
\item $p'(t) = kp(t)b(t)$ since its population grows as it kills bacteria.
\item Equilibria include $b = 0, p = 0$ and $b = 0, p > 0$, since these are \emph{constant} solutions, or places where $b'(t) = 0, p'(t) = 0$.
\end{itemize}

\textbf{Direction fields}, with vector pointing towards $\langle b'(t), p'(t) \rangle$ (TODO - I think) let us follow the arrows to determine the curve over time.  In this case, the bacteria will always go extinct.

\ 
However, if we add a new death rate term $-d_pp(t)$ so $p'(t) = -d_pp(t) + kp(t)b(t)$:
\begin{itemize}
\item We get an equilibrium at $b = \frac{d_p}{k}, p = \frac{r_b}{k}$. (Since $0 = b'(t) = rb - kpb, (\Rightarrow pk = r), 0 = p'(t)  -dp + kpb,(\Rightarrow bk = d)$)
\item But otherwise the solutions swirl around this point.  This is called a \textbf{cycle}.  TODO What is a \textbf{limit cycle}?
\end{itemize}

Note that there are systems where the ``solution particle'' neither reaches an equilibrium or cycles around one point.  The \textbf{Lorenz system} famously has this owl-eye shaped double attractor (an example of \textbf{strange sets}) where initially close particles diverge unpredictably if the constants $\rho, \sigma, b$ are chosen right:

\begin{itemize}
\item $x'(t) = \sigma(y-x)$
\item $y'(t) = x(\rho - z) - y$
\item $z'(t) = xy-bz$
\end{itemize}

\begin{itemize}
\item TODO
\end{itemize}

\subsection{Chapter 1.2: PDEs}

Many methods of attack for PDEs

\begin{itemize}
\item Separation of variables
\item Power series (Note: did we actually touch on this?)
\item Fourier Transform
\end{itemize}

\textbf{Example}: Standing wave, where one end of a rope is fixed.  
\begin{itemize}
\item Vertical displacement from a line of rope: $u(x,t)$ depends on where ($x$) and when ($t$t).
\item Rope's \textbf{wave equation} is $u_{tt} = v^2u_{xx}$, where $v$ is the ``constant wave speed'', and the others are the space, time partials.
\item Note that $u = \cos(vt)\sin(x)$ and $u = \sin(vt)cos(x)$ both work.
\item If you guess the solution has split variables like $u = X(x)Y(y)T(t)$, then, upon substitution and division by $X(x)Y(y)T(t)$, $\frac{\delta^2 u}{\delta t^2} = v^2 [\frac{\delta^2 u}{\delta x^2} + \frac{\delta^2 u}{\delta y^2}]$ yields $\frac{T''(t)}{T(t)} = v^2 [\frac{X''(x)}{X(x)} + \frac{Y''(y)}{Y(y)} ]$
\item This method may or may not work.  But if it does, it means that since $x, y, $ and $t$ are independent variables, each individual piece must be constant.
\item So, for example, if we know $\frac{X''(x)}{X(x)} = -4\pi^2$, we can get to $X(x) = \sin(2\pi x)$
\item The wave equation is similar in 3D: $u_{tt} = v^2[u_{xx}+u_{yy}+u_{zz}]$, or using the Laplacian, $u_{tt} = v^2\nabla^2 u$.  Here, $u$ measures not displacement but expansion/compression of air at $(x,y,z)$, time $t$.
\end{itemize}


Using Fourier transforms helps turn difficult PDEs into an easier problem like an ODE.  \emph{Fourier transforms work best} when 
\begin{itemize}
\item The domain is all of $\mathbb{R}^n$
\item The function $u$ vanishes at infinity.
\end{itemize}
 
The Fourier transform changes the domain of $x$ to that of $\omega$.  It comes with the (highly simplified) rule (see Vector Calculus course): $F[\frac{\delta f}{\delta x}] = i \omega F[f]$.  
\textbf{Example}: Drunkard's walk.
\begin{itemize}
\item One dimensional: moves left or right in a random way.  Starts at $x=0, t=0$.
\item $u(x,t)$ is probability of being at point $x$ at time $t$.  Naturally, $\int_{x=-\infty}^{x=\infty} u(x,t)dx = 1$.
\item Also, it obeys the 1-dD diffusion equation $\frac{\delta u}{\delta t} = \frac{\delta ^2 u}{\delta x^2}$
\item The Fourier transform doesn't affect $t$ at all.
\item So by taking Fourier transform of both sides of diffusion equation we get 
\begin{itemize}
\item $F(u_t)=  \frac{\delta}{\delta t} F(u) $ since $F$ doesn't care about $t$.
\item $\frac{\delta ^2 u}{\delta x^2} = i\omega F(\frac{\delta u}{\delta x}) = -\omega^2 F(u)$
\item So $  \frac{\delta}{\delta t} F(u)  = -\omega^2 F(u)$
\item This is solvable as $F(u) = ce^{-\omega^2 t}$.  Take it on faith that $c = \frac{1}{2\pi}$ for now. TODO
\item Known fact: $F[Ae^-{\frac{ax^2}{2}}] = \sqrt{\frac{1}{2 \pi a}} Ae^{\frac{-\omega^2}{2a}}$
\item This means $t = \frac{1}{2a}$ and $a = \frac{1}{2t}$
\item $F(u)  = 	\frac{1}{2\pi}e^{-\omega ^2 t}, F[Ae^-{\frac{ax^2}{2}}] = \sqrt{\frac{1}{2 \pi a}} Ae^{\frac{-\omega^2}{2a}}$ so $u  = Ae^{\frac{-ax^2}{2}}$
\item Solving, you get $A = \sqrt{\frac{1}{4\pi t}}, a=\frac{1}{2t}$, so $u(x,t) = \sqrt{\frac{1}{4\pi t}} e^{-\frac{x^2}{4t}}$
\end{itemize}

\end{itemize}


\section{Chapter 2: Nonlinear Equations}
\subsection{2.1: Lotka-Volterra I}


Major ideas:
\begin{itemize}
\item \textbf{phase plane}: TODO
\item \textbf{nullcline}: TODO
\item \textbf{direction field}: TODO
\item \textbf{equilibria}: TODO
\end{itemize}
 
\textbf{Example}: Bacteria vs. phages (again)
\begin{itemize}
\item Bacteria unrestrained grow in proportion to their population, so $\frac{db}{dt} = r_b b(t)$ (solved: $b(t) = b(0)e^{r_b t}$)
\item Phages unfed decrease in proportion to current size, so $\frac{dp}{dt} = -d_p p(t)$  (solved: $p(t) = p(0)e^{-d_p t}$)
\item Bacteria die with likelihood of meeting a phage, and phages increase with likelihood of meeting a bacterium.  So the set of equations, for constant $k$, becomes:
\begin{itemize}
\item $b'(t) = r_b b(t) - kb(t)p(t)$
\item $p'(t) = -d_p p(t) + kb(t)p(t)$
\item \emph{The product of p and b makes our equations nonlinear} (WHY?)
\item I guess, very generally, $b_1p_1 = k, b_2p_2 = k,$ but $(b_1+b_2)(p_1+p_2) = b_1p_1 + b_2p_2 + b_1p_2+b_2p_1 = 2k + b_1p_2+b_2p_1 \neq 2k$, so the last two ``mixed'' terms mean you can't just add solutions $(b_1, p_1)$ and  $(b_2, p_2)$.
\end{itemize}
\end{itemize}

General thoughts on this solution:
\begin{itemize}
\item So a solution $(b(t), p(t))$, traces out a curve on the bp-phase plane (b is x-axis, p is y-axis) as time (unrepresented in the plane) continues.
\item If we add a unit tangent vector at every point $(B, P)$ aligned with $(b'(t), p'(t)) = ( r_bB - kBP, -d_p P + kBP)$, we can follow the arrows to see the solution over time.
\item The above is called a \textbf{direction field}
\item This is sometimes hard to sketch analytically, so we can look to the \textbf{nullclines}: places where one of the components of the direction field is zero.
\item In this case, $r_bB - kBP = (r_b-kP)B = 0$ when $P = 0$ or $P = \frac{r_b}{k}$, and $-d_p P + kBP = (kB-d_p)P = 0$ when $P=0$ or $B = \frac{d_p}{k}$.
\item The \textbf{upshot of nullclines} (since we don't care about $P, B \leq 0$): The lines $B = \frac{d_p}{k}, P = \frac{r_b}{k}$ \emph{divide the plane into pieces where the components of this (continuous) function pair can't change sign}.  
\item For instance, $B > \frac{d_p}{k}, P <  \frac{r_b}{k}$  means $r_b b - kbp > 0, -d_p p+ kdp > 0$, so both populations are growing here.  This helps to sketch the curve.
\item The curve looks like a counterclockwise whirlpool around the $(B, P) = ( \frac{d_p}{k}, \frac{r_b}{k})$.  (bacteria grow with low but growing phages; bacteria decrease as phages overwhelm; both decrease as phages starve; bacteria start coming back)
\item The center point is a (constant \textbf{equilibrium}) solution, and other solutions swirl around it but don't get attracted or repelled.
\end{itemize}

There are a few types of equilibria:
\begin{itemize}
\item This one is a \textbf{center} around which solutions circle.
\item A \textbf{stable equilibrum} would see small upsets come back to an unchanging state.
\item An \textbf{unstable equilibrum} would see small upsets create wildly divergent paths.


\end{itemize}

\subsection{2.2: Lotka-Volterra II}

In the Bacteria-Phage system, we can't yet prove everything rotates around the \textbf{center}.  Let's do that.

Developing a \textbf{conserved quantity} will help to do that.  \textbf{Example}: Block on a horizontal spring with mass $m$, spring constant $k_s$:
\begin{itemize}
\item $x(t)$: Displacement from rest position.
\item $v(t) = \frac{dx}{dt}$: Horizontal velocity
\item $\frac{dv}{dt} = -\frac{k_s}{m}x(t)$ by Hooke's law, I think.
\item Suppose there's some Energy function $E(x,v)$.  By chain rule $\frac{d}{dt}E(x(t), v(t)) = \frac{dE}{dx}\frac{dx}{dt} +  \frac{dE}{dv}\frac{dv}{dt}$
\item $=  \frac{dE}{dx}v - \frac{k_s}{m} \frac{dE}{dv}x$.  If we set E as conserved, as in $E'(t) = 0$, then $\frac{dE}{dx}v = \frac{k_s}{m} \frac{dE}{dv}x$
\item We can eyeball and see that $E = \frac{1}{2}k_sx^2 +  \frac{1}{2}mv^2$ solves this equation, or we can assume $E(x,v) = F(x) + G(v) \Rightarrow 0 = E'(t) = F'(x)v - \frac{k_s}{m}G'(v)x = 0$ from the above equations and guess from there.
\item This means in the xv phase space, that there's a fixed E such that the particle follows the ellipse $E = \frac{1}{2}k_sx^2 +  \frac{1}{2}mv^2$ in phase space around the solution point (0,0).
\end{itemize}

\textbf{Extended Example}: Continuing on finding a conserved quantity for Bacteria / Phage:
\begin{itemize}
\item We need to find $U(b(t), p(t))$ such that $U'(t) = 0$, or by chain rule $\frac{\delta U}{\delta b}\frac{\delta b}{\delta t}  + \frac{\delta U}{\delta p}\frac{\delta p}{\delta t} = 0$
\item Subbing in, $\frac{\delta U}{\delta b}[r_bb-kbp]  + \frac{\delta U}{\delta p}[-d_pp+kbp] = 0$
\item A hint suggests finding $U$ such that $\frac{\delta U}{\delta b} = -\frac{d_p}{b} + k, \frac{\delta U}{\delta p} = -\frac{r_b}{p} + k$ to make terms cancel.
\item Integrating these gives us $U$ as both $-d_p\ln(b)+kb+Q(p)$ and $-r_b\ln(p)+kp+R(b)$ so $U = -d_p\ln(b)-r_b\ln(p)+kb+kp$.  This weird curve consistutes a level set in pb-space upon which a solution sits.
\item The spring example has an elliptic paraboloid solution.  There's an absolute minimum ($E = 0$ at $(0,0)$) but level sets become closed loops away from it.
\item For the Lotka example, there is a critical point ($\nabla U = \vec{0}$) when $\nabla U(b,p) = (\frac{\delta U}{\delta b}, \frac{\delta U}{\delta p}) = (k-\frac{d_p}{b}, k - \frac{r_b}{p})$, which is $(0, 0)$ at our known center $(\frac{d_p}{k}, \frac{r_b}{k})$
\item Showing that we always increase gong away from the point $(\frac{d_p}{k}, \frac{r_b}{k})$ should guarantee us closed level sets.
\item One method: Assume we're picking a unit vector $\vec{v} = \langle \hat{v_b}, \hat{v_p} \rangle$ so that our line from our center is $\vec{v} = \langle \frac{d_p}{k}+t{v_b}, \frac{r_b}{k}+t{v_b} \rangle$.  $U=F(b) + G(p)$ in this case, so sub the $b$ part into $F$ to get $F (\frac{d_p}{k} +t\hat{v_b}) = d_p[1-\ln(\frac{d_p}{k} + t\hat{v_b})]+kt\vec{v_b}$.  Taking derivative of that w.r.t $t$ shows it is always positive.  Same goes for the G(p) portion of U.
\item Another (DF) method: Note that $\nabla U = (k - \frac{d_p}{b}, k - \frac{r_b}{p})$'s grad (second derivative) is always positive.  So derivative always has positive curvature (maybe using that term wrong), and we'll always increase around this point.
\item Also, we know that the particle travels around the level set (loop) and doesn't reverse course, because then, $b'(t) = p'(t) = 0$, and we only have that at the center point (nullcline intersection).
\end{itemize}


\subsection{2.3: Linearization}

\textbf{Extended Example}: Suppose there's a limit to bacterial growth, so we cap our population at $M_b$.
\begin{itemize}
\item If $b(t) << M_b$, things should be similar.  If $b(t)$ is nearly $M_b$, then growth should approach 0.  So, this implies $\frac{db}{dt} = r_bb(t) \rightarrow \frac{db}{dt} = r_bb(t) (1-\frac{b(t)}{M_b})$.  Note: This isn't the only possibility but we'll use it.
\item This updates our Lotka-Volterra model to something more complicated:
\begin{itemize}
\item $b'(t) = r_bb(t) (1-\frac{b(t)}{M_b})- kb(t)p(t)$
\item $p'(t) = -d_p p(t) + kb(t)p(t)$
\end{itemize}
\item Other than $b=0, p=0$, the meaningful nullclines are solved by setting $b'(t) = 0$ (yielding $r_b(1-\frac{b}{M_b})-kp = 0$) and $p'(t) = 0$ (yielding $b = \frac{d_p}{k}$)
\item Note: We'll clean up through some MAGIC non-dimensionalization (how to derive?) to simplify:
\begin{itemize}
\item $x(t) = \frac{1}{M_b}b(\frac{t}{r_b}), y(t) = \frac{k}{r_b}(\frac{t}{r_b}), \alpha = \frac{d_p}{r_b}, \beta = \frac{kM_b}{r_b}$
\item Gives us new equations: $\frac{dx}{dt} = x(t)[1-x(t)] - x(t)y(t), \frac{dy}{dt} = -\alpha y(t) + \beta x(t)y(t)$
\item And new nullclines: $x+y=1, x=\frac{a}{b}$
\item So  there's an equilibrium point in the positive xy quadrant if: $y = 1-x = 1-\frac{\alpha}{\beta}$ and $y > 0$ implies $1-\frac{\alpha}{\beta} > 0 \Rightarrow \frac{\alpha}{\beta} < 1$
\item Looking at the direction field, it appears solutions swirl around and are attracted \emph{into} the center point $(\frac{\alpha}{\beta}, 1 - \frac{\alpha}{\beta})$, making it a \textbf{stable equilibrum}
\end{itemize}
\end{itemize}

This is similar to the block-spring example, if a damping term $-\frac{\gamma}{m}v$ is added.
\begin{itemize}
\item $\frac{dx}{dt} = v, \frac{dv}{dt} = -\frac{k_s}{m}x -\frac{\gamma}{m}v$
\item This can be thought of in matrix terms: $\frac{d}{dt} \begin{pmatrix} x(t) \\ v(t) \end{pmatrix} =  \begin{pmatrix} 0 & 1 \\ -\frac{k_s}{m} & -\frac{\gamma}{m} \end{pmatrix}  \begin{pmatrix}x(t) \\ v(t)\end{pmatrix}$ Call the matrix $A$.
\item From Diff Eq I, the solution is $exp(tA)$ (matrix exponential), making x(t) a linear combination of $e^{\lambda t}$ or possibly $te^{\lambda t}$ terms, with the eigenvalues as $\lambda$s.
\item The eigenvalues in this case, using the quadratic formula, could be:
\begin{itemize}
\item Two real, distinct, negative roots. So, these $e^{\lambda t}$ terms decay, and x(t) levels off.
\item Two distinct complex roots with real part $-\frac{\gamma}{2m} < 0$.  This ends up being some sines and cosines multiplied by $e^{-\frac{\gamma t}{2m}}$, which decays too.
\item Finally, if we have a repeated negative real eigenvalue, we have solution $x(t) = Ae^{-\frac{\gamma t}{2m}} + Bte^{-\frac{\gamma t}{2m}} $, also decaying.
\item So any disturbance in the spring will oscillate and come to rest at $x(t) = v(t) = 0$ quickly.
\end{itemize}

So with linear systems $\vec{x}'(t) = A\vec{x}(t)$, the eigenvalues determine what happens around the equilibrium point.  However, the \textbf{bacteria-phage model is non-linear}.  Here is \textbf{how we linearize} for nearby solutions in a nonlinear system:
\begin{itemize}
\item Set small disturbance $\delta x(t) << 1, \delta y(t) << $ so $x(t) = \frac{\alpha}{\beta} + \delta x(t), y(t) = 1-\frac{\alpha}{\beta} + \delta y(t)$
\item Since they're small, all powers like $\delta x(t)^2$ and $\delta x(t) \delta y(t)$ are considered basically zero.
\item So substitute $x(t) \rightarrow \frac{\alpha}{\beta} + \delta x(t), y(t) \rightarrow = 1-\frac{\alpha}{\beta} + \delta y(t)$ into our $\frac{dx}{dt}$ and $\frac{dy}{dt}$ equations.
\item This gives us the A solving $\frac{d}{dt} \begin{pmatrix} \delta x(t) \\ \delta y(t) \end{pmatrix} = A \begin{pmatrix}\delta x(t) \\ \delta  y(t)\end{pmatrix}$, which is $A = \begin{pmatrix} -\frac{\alpha}{\beta} & -\frac{\alpha}{\beta}  \\ \beta - \alpha & 0 \end{pmatrix}$ after working through the substitution.
\item Finding the eigenvalues here yields the same situation as the block-spring example: decays in all situations.
\end{itemize}

It turns out through the \textbf{Hartman-Grobman Theorem} that $\vec{x}'(t) = \vec{F}(\vec{x}(t))$, for some continuously differential vector field $F$, if we linearize near equilibrium $x_0$, then what falls out of this $A$ approach works if the eigenbalues \emph{aren't all purely imaginary}.

It turns out the uncapped bacteria system from before looks like $\frac{d}{dt} \begin{pmatrix} \delta x(t) \\ \delta y(t) \end{pmatrix} =  \begin{pmatrix} 0 & -1 \\ \alpha & 0 \end{pmatrix}  \begin{pmatrix} \delta x(t) \\ \delta y(t)\end{pmatrix}$ , with characteristic equation $\lambda ^2 + \alpha = 0, \alpha > 0$.  This means both values are imaginary, and we had to use the conserved quantity appraoach! 

\end{itemize}

\subsection{2.4: Hartman-Grobman Theorem}

\textbf{Extended Example}: Consider a phage that dies off quicky:
\begin{itemize}
\item $\frac{db}{dt} = r_bb(t)-k_bb(t)p(t), \frac{dp}{dt} = -r_pp(t) = 0 \cdot b(t)p(t)$, where $k_p$ is the zero (phages don't increase), and $k_b$ is still the kill factor for the bacteria.
\item In this base, $b(t) = p(t) = 0$ is the only equilibrium.
\item Non-dimensionalize as $x(t) = b(\frac{t}{r_b}), y(t) = \frac{k_b}{r_b}p(\frac{t}{r_b}), \alpha = \frac{r_p}{r_b}$
\item This makes the equations $x'(t) = x(t) - x(t)y(t), y'(t) = -\alpha y(t)$, and the nullclines therefore $x(t) = 0, y(t) = 1, y(t) = 0$
\item Looking at this six-section dierection field, we see that solutions exactly on the y-axis are attracted to equilibrium $(0,0)$, and other are repelled.
\item This makes sense since if the bacteria is 0, the phage die and approach $(0,0)$, otherwise the bacteria multiply and win (so it's a \emph{saddle point})
\item The way to tell: linearize the equations.  $\frac{d}{dt} \begin{pmatrix} x(t) \\ y(t) \end{pmatrix} \approx \begin{pmatrix} 1 & 0 \\ 0 & -\alpha \end{pmatrix}  \begin{pmatrix} x(t) \\ y(t) \end{pmatrix} $ since, if $x(t), y(t) << 1,  x(y)t(t) = 0$.  
\item Then the eigenvalues are $\lambda = 1, -\alpha$ so the solution is $Ae^t$,  $Be^{-\alpha t}$ for $x(t), y(t)$ (TODO respectively?) \textbf{Hartman-Grobman ensures this is the general solution}.
 \end{itemize}

However, let's solve directly and see if we come to the same result. 
\begin{itemize}
\item $y'(t) = -\alpha y(t), y(0) = y_0 \Rightarrow y(t) = y_0 e^{-\alpha t}$
\item With this in hand, $\frac{dx}{dt} = x(t) -= x(y)y(t) = x(t)[1-y_0e^{-\alpha t}]] , x(0) = x_0$ separates out to 
\begin{itemize}
\item $\frac{dx}{x} = [1-y_0e^{-\alpha t}]dt $
\item $\ln(x) = [t + \frac{y_0}{\alpha}e^{-\alpha t}] + C $
\item $x = e^Ce^t \exp(\frac{y_0}{\alpha}e^{-\alpha t})$
\item $x(0) = x_0 \Rightarrow e^C = x_0e^{-\frac{y_0}{\alpha}}$
\item $\Rightarrow x(t) = x_0 e^t \exp(\frac{y_0}{\alpha}(e^{-\alpha t} - 1))$
\end{itemize}

But how do we deform the phase plane so this looks linear?  We need some mapping $\vec{h}(x,y) = \langle u(x,y), v(x,y) \rangle$ that is continuous and invertible (so we don't ``damage'' the phase plane).  This is called a \textbf{homeomorphism}.
\begin{itemize}
\item So near the equilibrium $(0,0)$, the equations  $y'(t) = -\alpha y(t), y(0) = y_0 \Rightarrow y(t) = y_0 e^{-\alpha t}$ linearized for $\delta x, \delta y$ must be similar to those for $u(x(t), y(t), v(x(t), y(t))$
\item This means we need $\frac{du}{dt} = u, \frac{dv}{dt} = -\alpha v$
\item After doing the substitution, we see that $v = v_0e^{-\alpha t}$ exactly mimics $y(t) = y_0 e^{-\alpha t}$ for the phage solution.  So we take $v = y$.
\item Therefore, we know that since $u = u_0e^t$ and $x(ty) = x_0 \exp(t + \frac{y_0}{\alpha}(e^{-\alpha t} - 1))$, that we need $u(x(t), y(t) = u(x_0, y_0)e^t$
\item And this is satisfied if we guess $u(x,y) = xe^{-y}{\alpha}$ and work it out.
\item This function $\vec{h}(x,y) = (u, v) = \langle xe^{-\frac{y}{\alpha}}, y \rangle $ is invertible by $(x,y) = \langle ue^{\frac{v}{\alpha}} , v \rangle$, which is continuous.
\end{itemize}

\end{itemize}

\subsection{2.5: Application - Lasers}

Lasers create excited atoms, which then emit photos while transitioning to an unexcited state.  This system has a close analogue with the previous phages (like photons) and bacteria (like atoms) model.

\begin{itemize}
\item $n(t)$: number of photons in the laser; $r_g$: rate of photons gained (created by excited atoms transitioning to unexcited state); $r_l$: rate of photons lost (emitted)
\item $\Rightarrow \frac{dn}{dt} = r_g - r_l$ by definition.
\item We can assume we're losing a constant $k$ (kill?) portion of photons per unit time, so $ \frac{dn}{dt} = r_g - kn(t)$
\item $e(t)$: number of excited atoms (that will maybe create photons).  Atoms are excited by external energy pump.
\item Excited atoms radiate when meeting a photon (which survives the meeting)
\item So we can use the same setup from the bacteria: with $I$ the constant of meeting (intersect?), $r_g = Ie(t)n(t) \Rightarrow n'(t) = Ie(t)n(t) - kn(t)$
\end{itemize}

\textbf{Mini example: Assume no photons leave} (cap the end of the laser)

\begin{itemize}
\item $k = 0$ in this scenario.
\item So every meeting creates one more photon ($n \rightarrow n + 1$) while enervating one excited atom $(e \rightarrow e - 1)$.  This implies, equivalently:
\begin{itemize}
\item $e+n$ is a conserved quantity,
\item $e(t) + n(t) = e(0) + n(0),$
\item $[e(t) + n(t)]' = 0$
\item Then, if$ k=0,  n'(t) = Ie(t)n(t) - kn(t)$, and coupled with $e'(t) + n'(t) = 0$ above, we have $e'(t) = -Ie(t)n(t)$
\end{itemize}
\end{itemize}

\textbf{Extended example: Atoms spontaneously lose energy}.  This is actually what happens
\begin{itemize}
\item From quantum physics, we have a rate $s$ of atoms just (s)pontaneously losing energty.
\item We also have an energy (p)ump that energizes atoms with quantity $p$.
\item Then, our change in (e)xcited atoms is $e'(t) = p -s - Ie(n)(t)$
\item So our \textbf{final laser equations} are $e'(t) = p -s - Ie(n)(t), n'(t) = Ie(t)n(t)-kn(t)$ 
\item If we want to find the smallest $p$ guaranteeing $n \geq 1$ (there's at least one photo) at equilibrium ($e'(t) = n'(t) = 0$):
\begin{itemize}
\item $n'(t) = 0 \Rightarrow Ien = kn \Rightarrow n(Ie-k) = 0$.  If $n \neq 0, \Rightarrow e = \frac{k}{I}$
\item $e'(t) = 0 \Rightarrow Ien = p - se$
\item Together, $p - se = Ien = kn \Rightarrow kn + se = p \Rightarrow kn + s\frac{k}{I} = p$
\item $n \geq 1 \Rightarrow p \leq  k +  \frac{ks}{I}$
\item \textbf{Another tactic}: We could also assume we \emph{start out at equilibrium}, so $n_0, e_0$ are constant solutions.
\item Solving $n' = 0 = Ie_0n_0 - kn_0, e' = 0 = Ie_0n_0 - se_0 + p$, we find equilibria $n_0 = \frac{p}{k} - \frac{s}{I}, e_0 = \frac{k}{I}$
\item Then,  $n_0 \geq 1 \Rightarrow  \frac{p}{k} - \frac{s}{I} \geq 1 \Rightarrow p \geq k + \frac{ks}{I}$
\end{itemize}
\end{itemize}

\textbf{Non-dimensionalization time}:
\begin{itemize}
\item Scale against $e_0 (= \frac{k}{I}), n_0  (= \frac{p}{k} - \frac{s}{I})$ like this: $x(t) = \frac{n(\alpha t)}{n_0}, y(t) = \frac{e(\alpha t)}{e_0}$
\item NOTE: What does this do?  This makes (1,1) the equilibrium, as $x(t) = \frac{n_0}{n_0} = 1, y(t) = \frac{e_0}{e_0} = 1$ !
\item What $\alpha$ lets us tke $n' = Ien - kn, e'=-Ien - se + p$ and write 
\begin{itemize}
\item $\frac{dx}{dt} = x(t)y(t) - x(t)$
\item $\frac{dy}{dt} = \frac{1}{k}(\frac{pI}{k} -s)[1 - x(t)y(t)] + \frac{s}{k}[1-y(t)]$
\item $x' = \frac{\alpha n'(\alpha t)}{n_0} = xy - x = \frac{Ie(\alpha t)n(\alpha t)}{k n_0} - \frac{n(\alpha t)}{n_0}$
\item $\frac{\alpha Ien - \alpha kn(\alpha t)}{n_0} = \frac{Ie(\alpha t)n(\alpha t)}{k n_0} - \frac{n(\alpha t)}{n_0}$
\item $ \alpha Ie - \alpha k = \frac{Ie(\alpha t)}{k} - 1 \Rightarrow \alpha (Ie - k) = \frac{Ie-k}{k} \Rightarrow \alpha = \frac{1}{k}$
\item This solves the x equation, and I suppose it can be validated in the y equation (tediously).
\item If we chunk up our (somehow positive?) constants as $c = \frac{1}{k}(\frac{pI}{k} - s), d = \frac{s}{k}$, we end up with $y' = c[1-xy] + d[1-y]$
\item We only care about $x,y > 0$, so $x' = 0 = xy-x = x(y-1)$ implies $y=1$ is a nullcline
\item $y'  = 0 = c[1-xy] + d[1-y] = c - cxy + d - dy \Rightarrow c+d = y(d + cx) \Rightarrow y = \frac{c+d}{d + cx}$, a scaled and shifted hyperbola.
\end{itemize}

\textbf{Look at the solutions}:
\begin{itemize}
\item It appears we have a counterclockwise swirl around $(1,1)$, and nearby solutions tend toward this equilibrium.
\item Hartman-Grobman: rewrite our linearized solution in neighborhood of $(1,1)$ as $x(t) = 1 + \delta x(t), y(t) = 1 + \delta y(t)$
\item Using $x' = xy-x, y'=c[1-xy]+d[1-y]$ and $\frac{d}{dt}\begin{pmatrix} \delta x(t) \\ \delta y(t) \end{pmatrix} = A\begin{pmatrix} \delta x(t) \\ \delta y(t) \end{pmatrix}$, we can solve and write $A = \begin{pmatrix} 0 & 1 \\ -c & -c-d  \end{pmatrix} $
\item Eigenvalues: $\lambda = \frac{1}{2}(-c -d \pm \sqrt{(c+d)^2 - 4c})$
\begin{itemize}
\item If square root term is zero, we have repeated eigenvalue, so $\delta x(t), \delta y(t)$ are combos of $e^{-\frac{c+d}{2}}, te^{-\frac{c+d}{2}}$, which decays
\item If square root term is greater than zero, we have two distinct real, negative eigenvalues (since c, d are positive), so this decays.
\item If square root term is less than zero, we have distinct complex eigenvalues, but combos of $e^{-\frac{c+d}{2}} \cos (\frac{1}{2} \sqrt{-(c+d)^2+4c}), e^{-\frac{c+d}{2}} \sin (\frac{1}{2} \sqrt{-(c+d)^2+4c})$ decay too
\item Note : I suppose Hartman-Grobman can't work in purely imaginary eigenvalue scenario, because these kinds of functions don't converge or diverge without a term outside the $\sin$ or $\cos$ 
\item And in any case, since these lambdas aren't strictly imaginary, Hartman-Grobman works.
\end{itemize}

\end{itemize}

\end{itemize}

\subsection{2.6: Liapunov Equations}

We had some intuition that ``nearby'' solutions would fall into an equilibrium, but what does ``nearby'' mean?  \textbf{Liapunov Equations} help us here.  What is the ``basin of attraction''?

\begin{itemize}
\item Suppose we turn the pump off ($p = 0$), and set spontaneous enervation equal to photon leak $s = k$.
\item (TODO?) Somehow we can rescale to $\frac{dx}{dt} = Ie(t)n(t) - kn(t), \frac{dy}{dt} = -Ie(t)n(t) - kn(t)$ which (TODO??) gives us $\frac{dx}{dt} = xy-x, \frac{dy}{dt} = xy-y$
\item This means equilibria ($x' = y' = 0$) exist at $(0,0), (1,1)$
\item If we're turning the pump off, we're looking at equilibrium (0,0).  Linearizing, we get $x' = -\delta x, y'=-\delta y$, so a matrix of $\begin{pmatrix} -1 & 0 \\ 0 & -1 \end{pmatrix}$
\item With repeated non-imaginary (H-G applies!) eigenvalues $-1, -1$, we can see that both $e^{-t}, te^{-t}$ decay, and we get sucked into the origin.
\end{itemize}

But how do we prove this?  Let's find a conserved quantity $U'(x(t), y(t)) =  0$
\begin{itemize}
\item $U'(x(t), y(t)) =  \frac{\delta U}{\delta x}\frac{\delta x}{\delta t}  + \frac{\delta U}{\delta y}\frac{\delta y}{\delta t} = \frac{\delta U}{\delta x}x(y-1) + \frac{\delta U}{\delta y}y(x-1)$
\item Setting $\frac{\delta U}{\delta x}x = -x+1, \frac{\delta U}{\delta y}y = y-1$ makes this zero
\item Solving those two by separating variables and combining, we get $U = -x+y+\ln(|\frac{x}{y}|)$
\item So if we're stabiling $f = (x-y)$ (why?), we see $(x-y)' = x'-y' = (xy-x) - (xy-y) = x-y = f \Rightarrow f = e^{-t}$
\item With $x(0) = x_0, y(0) = y_0 \Rightarrow f(0) = x_0 - y_0, f = x(t) - y(t) =  (x_0 - y_0)e^{-t}$
\item How to express $y(t)$ while eliminating $x(t)$, knowing $x(y) - y(t) = (x_0 - y_0)e^{-t}$ and $U(x,y) = y-x + \ln(|\frac{x}{y}|)$ is conserved?  \textbf{The trick: $U(x_0, y_0) = U(x,y)$} since \emph{it doesn't change}!
\begin{itemize}
\item $y_0 - x_0 +  \ln(|\frac{x}{y}|) =  y - x + \ln(|\frac{x}{y}|) = -(x_0-y_0)e^{-t} + \ln(|\frac{x}{y}|) $
\item $(1-e^{-t})(y_0 - x_0)   = \ln(\frac{x / y}{x_0 / y_0}) $
\item Defining for convenience, $f = \exp((1-e^{-t})(y_0 - x_0) )$, then $f\frac{y}{y_0} = \frac{x}{x_0}$
\item Sub in to $x - y = (x_0 - y_0)e^{-t}: y[\frac{x_0}{y_0}f - 1] = (x_0 - y_0)e^{-t}$
\item Solve for $y: y = \frac{y_0(x_0-y_0)e^{-t}}{x_0f(t) - y_0}$ 
\item Combine with above to get $x = \frac{x_0(x_0-y_0)e^{-t}f(t)}{x_0f(t) - y_0}$
\end{itemize}
\item So with equilibria $(0,0), (1,1)$, the direction field computer plot shows us attracted to $(0,0)$ (no laser action) pretty much anywhere left and down from $(1,1)$ in the $x,y$ phase plane.
\item Apparently the linearized solutions near $0,0$ are $x_{lin} = x_0e^{-t}, y_{lin} = y_0e^{-t}$ (WHY?)
\item Looking above, if $(x_0 - y_0) \approx 0$, then $f(t) \approx 1$, and $x, y \rightarrow x_{lin}, y_{lin}$
\end{itemize}

On to \textbf{Liapunov} functions, which will tell us perhaps the size of the ``basin of convergence'', unlike Hartman-Grobman, which just says there is a neighborhood.

A \textbf{Liapunov} function $U(x,y)$ is 

\begin{itemize}
\item Continuously differentiable
\item With a unique minimum $(x_0, y_0)$, usually aligned to be $U$'s only zero.
\item $U'(x(t), y(t)) \leq 0$.  Everything ``flows downhill''; 
\item Tailor made for the problem, hard to find.
\end{itemize}


\textbf{Back to the rescaled laser example}

\begin{itemize}
\item $x'(t) = x(t)y(t) - x(t)$
\item $y'(t) = c[1-x(t)y(t)] + d[1-y(t)], c, d > 0$
\begin{itemize}
\item \textbf{Analogy: The damped-block spring system} $\frac{d}{dt} \begin{pmatrix} x(t) \\ v(t) \end{pmatrix} =  \begin{pmatrix} 0 & 1 \\ -\frac{k_s}{m} & -\frac{\gamma}{m} \end{pmatrix}  \begin{pmatrix}x(t) \\ v(t)\end{pmatrix}$ 
\item When $\gamma = 0$, we know $E(x,v) = \frac{1}{2}k_sx^2 + \frac{1}{2}mv^2$ is conserved when looking at $E'$
\item $\gamma = 0 \Rightarrow x' = v, v' = -\frac{k_s}{m}x$
\item $\frac{dE}{dt} = ( \frac{1}{2}k_sx^2 + \frac{1}{2}mv^2)' = 0$ since $\frac{1}{2}(k_s xx' + mvv' ) = \frac{1}{2}(kxv + mv\frac{-k}{m}x) = 0$
\item But if $\gamma \neq 0, \frac{d}{dt}E(x(t), v(t)) = \frac{d}{dt}[ \frac{1}{2}k_sx^2 + \frac{1}{2}mv^2] = kxx' + mvv'$
\item $= kx(v) + mv(\frac{-k_s}{m}x - \frac{\gamma}{m}v) = -\gamma v(t)^2 = \frac{dE}{dt}$
\item Total spring energy is then decreasing in the fluid.
\item Brilliant has Cool visualization of spiraling down into the "bowl" of $x,y$ with $E$ as the $z$ dimension, equilibrium $(0,0, 0)$
\item We need to choose a $\gamma$-fied $E-$like function that decreases for pairs $\delta x(t), \delta y(t)$. We can choose, like $E$, some $u(\delta x, \delta y) = \frac{1}{2}C_1[\delta x]^2 + C_2[\delta y]^2$.
\item Choosing $C_1 = c, C_2 = 1$ gives us $\frac{d}{dt}u(\delta x(t), \delta y(t)) = \frac{d}{dt}(\frac{c}{2}[\delta x(t)]^2 + \frac{1}{2}[\delta y(t)]^2)$
\item $= c(dx)(dx)' + dy(dy)' = c (dx)(dy) + dy(-c(dx)-(c+d)(dy)) = -(c+d)[\delta y(t)]^2$
\item So $u =  \frac{d}{dt}(\frac{c}{2}[\delta x(t)]^2 + \frac{1}{2}[\delta y(t)]^2)$ is an energy function that could work for the laser.
\end{itemize}

Finally, we want to construct a function that
\begin{itemize}
\item Doesn't increase (derivative negative) on any pairs $x, y > 0$ (pulls down)
\item Is near equal to  $u = \frac{c}{2}(x-1)^2 + \frac{1}{2}(y-1)^2$ near $(1,1)$. (the energy function for block-spring above)
\item With $x' -= xy-x, y'=c-cxy + d-dy$, plus the identity near $z \approx 1$ of $\ln(z) \approx (z-1)-\frac{1}{2}(z-1)^2$...
\item You can find $U(x,y) = c(x-1)+(y-1) -c\ln(x)-\ln(y)$ that satisfies all of these
\item It therefore shows that pumped laser solutions tend to equilibrium $(1,1)$ in the long term.
\end{itemize}

TODO: So this is enough to establish a convergence to an equilibrium?
\begin{itemize}
\item Find an equilibrium $(x_0, y_0)$
\item Find an energy function $u$ that decreases for all pairs ($\delta x(t) \delta y(t))$ near the minimum.
\item Find a Liapunov function $U$ function that decreases EVERYWHERE along $x(t), y(t)$ (in our domain, like $x,y > 0$)
\item Ensure that $U = u$ in the neighborhood of the equilibrium.
\item Then Liapunov's theorem somehow makes this work (TODO)?
\end{itemize}

\end{itemize}

\subsection{2.7: Dog chasing a duck (Limit Cycles)}

This is a pair of nonlinear equations to determine if a dog in the pond's interior catches a duck who skates along the border.
\begin{itemize}
\item Variables:
\begin{itemize}
\item $r_p$: Radius of pond.
\item  $\vec{r_H}$: Distance of duck to center (always the radius of the pond)
\item $\vec{l}$: Displacement of dog from duck, which is of some length $R$ at any point.
\item $\theta$: Duck's position in the lake (think polar coordinates)
\item $\phi$: Angle between $\vec{r_H}$ and $\vec{l}$.  
\item Duck always swims at  speed $r_p\theta'(t)$, and dog swims at $k > 0$ times this, or $kr_p\theta'(t)$.
\end{itemize}
\item Therefore $\vec{r_H} = \langle r_p \cos(\theta), r_p \sin(\theta) \rangle$. It's just the polar coordinates.
\item Doing some geometry gets you $\vec{l} = R \langle \cos(\theta + \phi, \sin(\theta + \phi) \rangle$
\item We can establish $\vec{T} = \vec{r_H} - \vec{l}$ and dog's speed squared $\|T'(t)\|^2 =( \vec{r_H}' - \vec{l}')\cdot ( \vec{r_H}' - \vec{l}') = \|\vec{r_H}'\|^2 + \|\vec{l}'\|^2 - 2\vec{r_H}'\vec{l}'$
\item Naturally, this  $\|T'\|^2$ is also equal to the constant $(kr_p\theta')^2$.  Our diff equations will fall out of these.
\item $\vec{r_H}^2 = r_p^2[\theta'(t)]^2$ since duck's speed is constant. $\vec{l} = (R')^2 + R^2[\theta'+\phi']^2$ after working it out.
\item Finally, after using identities $\cos(\theta + \phi) = \cos(\theta)\cos(\phi) -  \sin(\theta)\sin(\phi), \sin(\theta + \phi) = \cos(\theta)\sin(\phi) +  \sin(\theta)\cos(\phi)$, we can work out $-2\vec{r_H}'\vec{l}' = -2r_p\theta'[R\cos(\phi)(\theta' + \phi') + \sin(\phi)R']$
\item After rescaling $R$ to $\rho$ such that $\frac{R}{r_p} = \rho$ and diving our speed equation by constant $r_p\theta'$, we end up with speed equation $k^2 = [\rho(1+ \frac{d\phi}{d\theta} - \cos(\phi)]^2 + (\frac{d\rho}{d\theta} - \sin(\phi))^2$
\item 

We propose that there are some solutions here for the \textbf{pursuit equations}.  We'll ignore the generalized form an focus on one set
\begin{itemize}
\item $\rho(1+\frac{d\phi}{d\theta}) - \cos(\phi) = 0, \frac{d\rho}{d\theta} - \sin(\phi) = -k$ do work in the above.  (Doesn't prove others don't work)
\item This leaves our equations as $\frac{d\phi}{d\theta} = \frac{\cos(\phi)}{\rho}-1, \frac{d\rho}{d\theta} = -k + \sin(\phi)$
\item However, there \emph{aren't simple equilibria here}.  In no world with $k \neq 0$ does the dog sit still (or the duck).
\item Supposing $k<1$ and $R, \phi$ are fixed (dog never gets closer and just loops), this means he's going in a circle, since the two legs of a triangle ($\vec{l}, \vec{r_p}$) and the interior angle ($\phi$) are fixed, so this fixes length of the third leg, which is a radius
\item You can also use dog's position vectors $x(t) = r_p\cos(\theta) - R\cos(\theta + \phi),y(t)=  r_p\sin(\theta) - R\sin(\theta + \phi)$ and trig identities to prove $x(t)^2 + y(t)^2 = r_p^2+R^2 - 2r_pR\cos(\phi)$
\item If $k < 1$, then solving $ \frac{d\rho}{d\theta} = 0 = -k + \sin(\phi) \Rightarrow \sin(\phi) = k \Rightarrow \phi = \sin^{-1}(k)$ and $\rho = \cos(\phi) = \cos(\sin^{-1}(k)) = \sqrt{1-k^2}$
\begin{itemize}
\item Quick proof of $ \cos(\sin^{-1}(k)) = \sqrt{1-k^2}$:
\item  $\cos^2(\sin^{-1}(k)) +  \sin^2(\sin^{-1}(k)) = 1 \Rightarrow  \cos(\sin^{-1}(k)) = 1 -  \sin^2(\sin^{-1}(k))$
\item $= 1-k^2 \Rightarrow  \cos(\sin^{-1}(k)) = \sqrt{1-k^2}$
\end{itemize}

\item When $k < 1$, the direction field seems to have attractive equilibriua but \textbf{GOTCHA}: there are $\phi$ angles that differ by $2\pi$ units, so they're the same.  The direction field is a cylinder with circumference $2\pi$, and there are other solutions tracking toward $(\sin^{-1}(k), \sqrt{1-k^2})$
\item Linearizing, assume we are near our equilibirum point and $\phi = \sin^{-1}k + \delta \phi, \rho = \sqrt{1-k^2} + \delta \rho.$
\item We can also remember that $f(x+\delta x) \approx f(x) + f'(x)\delta x$ from calculus.
\item $\frac{d}{d\theta}[\delta \rho] = \frac{d}{d\theta}[\rho - \sqrt{1-k^2}] = \frac{d\rho}{d\theta} - \frac{d}{d\theta}\sqrt{1-k^2} = -k + \sin(\phi)$
\item $= -k + \sin(\sin^{-1}(k) + \delta \phi)$ and by the calculus rule $ \frac{d}{d\theta}[\delta \rho]  = -k + \sin(\sin^{-1}(k)) + \cos(\sin^{-1}(k))\delta \phi = \sqrt{1-k^2}\delta \phi$
\item And for $\frac{d}{d\theta}[\delta \phi] = \frac{d}{d\theta}\phi -  \frac{d}{d\theta} (\sin^{-1}(k)) = \frac{\cos(\phi)}{\rho} - 1$
\item Using multivariable hint $f(x + \delta x, y + \delta y) \approx f(x,y) + \frac{\delta f}{\delta x}\delta x +  \frac{\delta f}{\delta y}\delta y$,
\item $f = \frac{\cos(\sin^{-1}(k) + \delta \phi)}{\sqrt{1-k^2} + \delta \rho} -1  \approx \frac{\sqrt{1-k^2}}{\sqrt{1-k^2}} - 1 + \frac{-\sin(\sin^{-1}(k)}{\sqrt{1-k^2}} \delta \phi - \frac{\cos(\sin^{-1}(k))}{1-k^2} \delta \rho$
\item $=-\frac{k\delta \phi + \delta \rho}{\sqrt{1-k^2}}$
\item So$ \frac{d}{d\theta}\begin{pmatrix} \delta \phi \\ \delta \rho \end{pmatrix} \begin{pmatrix} -\frac{k}{\sqrt{1-k^2}} &-\frac{1}{\sqrt{1-k^2}} \\ \sqrt{1-k^2} & 0  \end{pmatrix} \begin{pmatrix} \delta \phi \\ \delta \rho \end{pmatrix}$, and the eigenvalues aren't purely imeginary, and the real part is negative, so all decay.  Therefore, the equilibrium at $(\sin^{-1}(k), \sqrt{1-k^2})$ attracts\ nearby solutions.
\item There aren't solutions (for $K < 1$?), but numerically solved, the dog catches at $k > 1$, and for $k \leq 1$, swims out to a path approaching a circle.  This is a \textbf{limit cycle}, an isolated trajectory that closes on itself.

\end{itemize}
\end{itemize}

\subsection{Poincare-Bendixson Theorem}

Limit cycles in the real world: a chemical reaction in perpetual osciallation!

Key concept - \textbf{trapping region}: a region in phase plane on some region $D$, with differential solutions touching every point, where the direction field sees every boundary arrow point IN.  This means:
\begin{itemize}
\item The solution has to stay in D.
\item Any solution that self-intersects forms a cycle in the phase plane.
\item The three conceivable ways a solution can ``snake'' around forever (the \textbf{Poincare-Bendixson theorem} says it):
\begin{itemize}
\item Approaches a closed loop in $D$.
\item Approaches a fixed point in $D$ (possibly a special case of the last bullet)
\item Cycle: Snake eats its own tail
\end{itemize}
\item A non-cycling solution is the only other possibility - just a point equilibirum.
\end{itemize}

\textbf{Example}: Chemical oscillatory reaction.
\begin{itemize}
\item $x$ is concentration of $I^-$, $y$ is concentration of $ClO_2^-$ ions in some reaction.
\item $a$ is positive, and clearly $x, y \geq 0$ in the physical world.
\item Otherwise meaningless equations: $\frac{dx}{dt} = 5a - x - \frac{4xy}{1+x^2}, \frac{dy}{dt} = x(\frac{4y}{1+x^2})$
\item Solve for equilibria by setting $\frac{dx}{dt} = \frac{dy}{dt} = 0$
\begin{itemize}
\item Denote $Q = \frac{y}{1+x^2}$
\item First equation implies $x(1+4Q) = 5a$
\item Second euqation, plus knowing $x \neq 0$, $\Rightarrow x(1-Q) = 0 \Rightarrow Q = 1$
\item $Q = 1 \Rightarrow 5x = 5a \Rightarrow x = a$
\item $\Rightarrow 1 = \frac{y}{1+x^2} \Rightarrow y = 1+a^2$
\item Only solution pair is $(a, 1+a^2)$
\end{itemize}
\end{itemize}

Linearizing the solution around $(a, 1+a^2)$
\begin{itemize}
\item $x = a + \delta x, y = 1 + a^2 + \delta y \Rightarrow \frac{dx}{dt} = \frac{d[\delta x]}{dt}, \frac{dy}{dt} = \frac{d[\delta y]}{dt}$
\item Call $f =  \frac{d[\delta x]}{dt} = 5a - x - \frac{4xy}{1+x^2}$, 
\item Approximate $f(x + \delta x, y + \delta y) \approx f(x,y) + \frac{\delta f}{\delta x}\delta x +  \frac{\delta f}{\delta y}\delta y$
\item $f(x,y)(a, 1+a^2)= 5a - x - \frac{4xy}{1+x^2}(a, 1+a^2) = 5a - a - 4(a\frac{1+a^2}{1+a^2}) = 0$
\item $\frac{\delta f}{\delta x}\delta x (a, 1+a^2) = (-1 - \frac{(1+x^2)(4y-2x4xy}{(1+x^2)^2})\delta x (a, 1+a^2)) =(-1 - 4 - \frac{8a^2}{1+a^2})\delta x  = \frac{-5 + 3a^2}{1+a^2}\delta x $
\item $\frac{\delta f}{\delta y}\delta y (a, 1+a^2) = \frac{-4x}{1+x^2}\delta y (a, 1+a^2)) =\frac{-4a}{1+a^2}\delta y $
\item Call $g =  \frac{d[\delta y]}{dt} = x - \frac{xy}{1+x^2}$
\item Approximate $g(x + \delta x, y + \delta y) \approx g(x,y) + \frac{\delta g}{\delta x}\delta x +  \frac{\delta g}{\delta y}\delta y$
\item $g(x,y)(a, 1+a^2) = x - \frac{xy}{1+x^2}(a, 1+a^2) = a - a\frac{1+a^2}{1+a^2} = 0$
\item $\frac{\delta g}{\delta x}\delta x (a, 1+a^2) = (1 -\frac{(1+x^2)y - xy2x}{(1+x^2)^2})\delta x ( = 1 -\frac{(1+a^2)^2 - 2a^2(1+a^2)}{(1+a^2)^2})\delta x = 2a^2\delta x $
\item $\frac{\delta g}{\delta y}\delta y (a, 1+a^2) = \frac{-x}{1+x^2}\delta y = \frac{-a}{1+a^2}\delta y $
\item  $\Rightarrow \frac{d}{dt}\begin{pmatrix} \delta x \\ \delta y \end{pmatrix}  = \frac{1}{1+a^2} \begin{pmatrix}  3a^2 - 5 & -4a \\ 2a^2 & -a \end{pmatrix} \begin{pmatrix} \delta x \\ \delta y \end{pmatrix}$
\item Let's arbitrarily choose $a = 2 \Rightarrow (a, 1+a^2) = (2,5)$.  The coefficient matrix ends up being $\frac{1}{5} \begin{pmatrix} 7 & -8 \\ 8 & -2\end{pmatrix}$, which has eigenvalues with a positive real $\pm$ some $i$ component.  So, Hartman-Grobman applies and we don't decay into our point but push away.
\end{itemize}

We want to \textbf{build the trapping region}.   
\begin{itemize}
\item Remember, $\frac{dx}{dt} = 10 - x - \frac{4xy}{1+x^2}, \frac{dy}{dt} = x(1 - \frac{y}{1+x^2})$ subbing in 2 for $a$)
\item On the left, if $x = 0$ we see $\frac{dx}{dt} = 10, \frac{dy}{dt}  = 0$.  So we're pointing right (into the first quadrant region)
\item On the bottom, if $y = 0$, we're pointing at $\langle 10 -x , x \rangle$ (into the region).
\item On the right, for some $x = b$, $10 - b - \frac{4b}{1+b^2}y$ will make sure we point left.
\item On the top, for some $y = c$, $x(1-\frac{c}{1+x^2} < 0$ makes sure we point down.
\item Assume, since we're encircling $(2,5)$, that $b \geq 3, c \geq 6$ for comfort.
\item To satisfy all of these, note $x(1-\frac{c}{1+x^2}) < 0 \Rightarrow 1 - \frac{c}{1+x^2} < 0 \Rightarrow c > 1+x^2, 0 < x < b \Rightarrow c > 1+b^2 \Rightarrow  \sqrt{c - 1 } > b $ 
\item And for $0 < y < c$, note that $10 - x - \frac{4xy}{1+x^2} < 10 - b < 0. $ 
\item Pick $b = 11$, say, implying $11 < \sqrt{c-1}$, so then $123 < c$.  So $(b,c) = (11, 124)$ ensures oscillation around $(2,5)$ without leaving that region.
\end{itemize}

Tricky: How to reduce this region?  No real way except simulation or some tricks.   If we PRESUME a cycle, we can prove the cycle extens to the left of $x=3$ or $ x_{min} < 3$
\begin{itemize}
\item \textbf{META trick}: Don't worry if you have unsolvable integrals - maybe you can cancel them out.  \textbf{Run with what you have.}
\item Trick: Assume $x(t + T) = x(t), y(t + T) = y(t)$ for some $T>0$, or that there's a PERIOD T.
\item $\int_0^T \frac{dx}{dt}dt = x(T) - x(0) = 0, \int_0^T \frac{dy}{dt}dt = y(T) - y(0) = 0$ by fundamental theorem.
\item Our equations again:  $\frac{dx}{dt} = 10 - x - \frac{4xy}{1+x^2}, \frac{dy}{dt} = x(1 - \frac{y}{1+x^2})$
\item So $0 = \int_0^T [10 - \int x(t) - 4\int \frac{x(t)y(t)} {1+x(t)^2}]dt$ by the first equation
\item $0 = \int x(t) - \int \frac{x(t)y(t)} {1+x(t)^2}]dt$ by the second.
\item Subtract four times the second from the first to get $0 = 10T - 5 \int_0^T x(t) \Rightarrow 2T = \int_0^T x(t)dt \geq int_0^T x_{min}dt = Tx_{min}$
\item So $2 \geq x_{min}$
\end{itemize}


\subsection{Chaos and the Lorenz Equation}

What enabled mathematical \textbf{chaos} (unpredictability in nonlinear differential equations) was really computers and seeing simulated solutions.

The (simplified) \textbf{Lorenz system} are these equations
\begin{itemize}
\item $\frac{dx}{dt} = \sigma (y-x)$
\item $\frac{dy}{dt} = x(\rho - z) - y$
\item $\frac{dx}{dt} = xy-bz$
\item All with $\sigma, \rho, b > 0$
\end{itemize}

Solving the equations, we see equilibria for these are:
\begin{itemize}
\item $(0,0)$ always
\item The two solutions $(\pm \sqrt{b(\rho - 1)}, \pm \sqrt{b(\rho - 1)}, \rho - 1)$ when $\rho > 1$.
\end{itemize}

Looking at $0 < \rho < 1$ specifically:
\begin{itemize}
\item Linearaizing is simple, with $x(t)= \delta x(t), y(t)= \delta y(t), z(t)= \delta z(t)$ and linearized system:
\item $\frac{d[\delta x]}{dt} = \sigma(\delta y - \delta x)$
\item $\frac{d[\delta y]}{dt} = \rho \delta x - \delta y$
\item $\frac{d[\delta z]}{dt} = -b \delta z$
\item
 $\frac{d}{dt} \begin{pmatrix} \delta x \\ \delta y \\ \delta z  \end{pmatrix} \approx  \begin{pmatrix} -\sigma & \sigma & 0 \\
 \rho & -1 & 0 \\ 0 & 0 & -b \end{pmatrix}  \begin{pmatrix} \delta x \\ \delta y \\ \delta z  \end{pmatrix} $
 \item Characteristic equation is $(-b - \lambda)[(1+\lambda)(\sigma + \lambda) - \sigma \rho]  = 0$
 \item Eigenvalues are $-b < 0$ and $\lambda = \frac{1}{2}[-(\sigma + 1) \pm \sqrt{(\sigma + 1)^2 - 4\sigma (1-\rho)}]$
 \item If $\rho < 1$, we have distinct, real, negative eigenvalues, and a locally attractive equilibrium by Hartman-Grobman.
\end{itemize}

But if $\rho < 1$ globally attactive?  Find a Liapunov function.

\begin{itemize}
\item Requirement is that the function $U(x(t), y(t), z(t))$ is minimized at the equilibrium, and that as time progresses, $U$ decreases (so we're sucked into the bowl)
\item We suppose that $U(x,y,z) = ax^2 + y^2 + z^2$ and using $xy \leq \frac{1}{2}x^2+ \frac{1}{2}y^2$:
\begin{itemize}
\item Identity derivation: $0 \leq (x-y)^2 \Rightarrow 0 \leq x^2 -2xy + y^2 \Rightarrow xy \leq \frac{1}{2}(x^2 + y^2)$
\end{itemize}
\item $\frac{\delta U}{\delta x} x'(t)+ \frac{\delta U}{\delta y} y'(t)+ \frac{\delta U}{\delta z}z'(t) = 2ax\sigma (y -x) + 2yx(\rho - z)-2y^2x+2zxy-2bz^2$
\item $=2(a\sigma + \rho)xy - 2a\sigma x^2 -2y^2-2bz^2$
\item \textbf{GOTHCA}: We can't choose $a = -\frac{\rho}{\sigma}$ since then $U = -\frac{\rho}{\sigma}x^2+y^2+z^2$ isn't minimized at $(0,0,0)$!  So $a$ needs to be positive.
\item Choosing $a = \frac{1}{\sigma} \Rightarrow a\sigma = 1$, with $\rho < 1 \Rightarrow
2(a\sigma + \rho)xy - 2a\sigma x^2 -2y^2-2bz^2 < 2a\sigma(2xy - x^2-y^2) - 2bz^2 \leq -2bz^2$ by the identity above.
\item Then $U = \frac{1}{\sigma}x^2+y^2+z^2$ decreases as $t \rightarrow \infty$ and is minimized at the globally attractive $(0, 0, 0)$
\end{itemize}

If $\rho > 1$ things get chaotic.  Instead of one equilibrium, we have two new ones at $(\pm \sqrt{b(\rho - 1)}, \pm \sqrt{b(\rho - 1)}, \rho - 1)$. Everything \textbf{bifurcates}, or qualitatively shifts when inching past $\rho = 1$:
\begin{itemize}
\item We have three equilibria.
\item The origin turns into a saddle equilibrium.
\item Linearizing around   $(\alpha, \alpha , \rho - 1)$ with $\alpha$ denoting  $\sqrt{b(\rho - 1)}$ (pretty straightforward), we get characteristic equation for $A$ of $-\lambda^3 - (\sigma + b + 1)\lambda^2 - b(\sigma + \rho)\lambda - 2\sigma b(\rho - 1) = 0$
\item Problem is, setting $\rho = 1$ drops the $(\rho - 1)$ term and we have $-\lambda(\lambda^2+(\sigma +1 + b)\lambda + b(\sigma + 1)) = 0$, with solutions $\lambda = 0, -b, -\sigma -1$.
\item The last two solutions are attractive, but the zero doesn't work for Hartman-Grobman.
\item If we set $\lambda = (\rho - 1)\Delta r$ when nudging $\rho$ just over 1, we ignore all $\lambda^2, \lambda^3$... as negligible and get
 $-b(\sigma + \rho) (\rho - 1)\Delta r - 2\rho b(\rho - 1) \approx 0$
 \item This means $\Delta r \approx -\frac{2\rho}{\rho + \sigma}$, or that this nudged root has to be negative when $\rho$ is near $1$.
 \item More rigorously, we could have proven the roots of the equation are negative for small $\rho - 1 > 0$
 \item In any case, this means that the near-zero root is negative, so   $(\alpha, \alpha , \rho - 1)$ attracts locally.
 \item We can show that this applies the same for   $(-\alpha, -\alpha , \rho - 1)$ 
\end{itemize}

How do equilibria change as we change $\rho$?
\begin{itemize}
\item We saw the What about as we dial past $\rho = 1$, our origin equilibrium changes from globally attractive to saddle point.
\item In going from a stable equilibrium with negative real-part eigenvalues (attractors) to $(0,0,0)$ as a saddle (mix of negative and positive real parts), we necessarily have a point where the eigenvalues' real parts are zero.
\item In other words, $\lambda = ia$ for some real $a$.
\item Subbing $ia$ into our $-\lambda^3 - (\sigma + b + 1)\lambda^2 b(\sigma + \rho)\lambda - 2\sigma b(\rho - 1) = 0$, we end up getting 
 $[(\sigma + b + 1)a^2 - 2\sigma b(\rho - 1)] + i[a^3-(b(\sigma + \rho)a]= 0$
\item Then we need $a^3 - b(\sigma + \rho)a = 0 \Rightarrow a = 0, a = \pm, \sqrt{b(\sigma + \rho)}$
\item If $a = 0$. the real part isn't zero.  But subbing $a = \pm \sqrt{b(\sigma + \rho)}$ gives us solutions for a set of $\rho = \frac{\sigma(\sigma + b + 3)}{\sigma - b - 1}$
\item So, when moving past this value, our two new equilibria change from locally attractive to saddles too.
\end{itemize}

Can we create a trapping region?
\begin{itemize}
\item The hint: The solutions have to pass through every ellipsoid of form $R^2 = \rho x^2 + ]sigma y^2 + \sigma (z-2p)^2$
\item What we need to prove: At every point on the boundary, the direction field points ``in'', or more specificlally, \emph{the angle between inward normal and direction field is acute}.
\item This also means that the gradient $\nabla g$ of the level set $R^2 = \rho x^2 + \sigma y^2 + \sigma (z-2r\rho)^2$ is the normal. This is $\langle 2\rho, 2\sigma, 2(z-2\rho)\rangle$
\item So $-\nabla g \cdot \langle \frac{dx}{dt},\frac{dy}{dt},\frac{dz}{dt} \rangle > 0 \Rightarrow ... \Rightarrow 2 \rho\sigma x^2+2\sigma y^2 + 2\sigma z^2-4\rho\sigma z > 0$
\item Use $R^2 - \rho x^2 -\sigma (z-2p)^2  =  \sigma y^2 \Rightarrow \rho\sigma x^2+2\sigma y^2 + 2\sigma z^2-4\rho\sigma z > 0$
\item This simplifies the dot product to $2R^2 - 8\rho^2\sigma + 4\sigma \rho z + 2 \rho (\sigma-1)x^2 > 0$
\item Since the $x^2$ term is always positive, we just need to set $R$ to clear zero when $z$ is its most negative (x = 0, y = 0, z = $2\rho - \frac{R}{\sqrt{\sigma}}$).  If we churn a little more we can see that setting $R > 2\sqrt{\sigma}\rho$ will provide a trapping region.

\end{itemize}

Question: Do the confined solutions fill up the whole (ellipsoid) container?  
\begin{itemize}
\item Looking at the divergence (volume change of a cube over time) of the solution will tell us.
\item For unspecified reasons, $\frac{1}{v(t)}v'(t) = \nabla \cdot \langle \sigma(y-x), x(\rho - z) - y, xy-bz \rangle = -\sigma -1 - b$
\item Solving, $v(t) = v(0)e^{-t(\sigma + 1 + b)}$
\item This means the volume decays to 0, so therefore, our line is confined to a smaller and smaller space (but not just a point, I guess?)
\end{itemize}

\section{Partial Differential Equations}
\subsection{1D  Wave Equation and D'Lambert's Formula}

General set up: A rope with a fixed right end  (boundary condition and a moving left end), moving up and down.

Start out with special case: no boundary condition  (infinite rope, pulse in the middle)

\begin{itemize}
\item $u(x,t)$ measures the vertical displacement form the x-axis of the rope at point $x$, time $t$
\item Physical observation gives us the PDE rule $\frac{\delta^2 u}{\delta x^2} = \frac{\delta^2 u}{\delta t^2}$ (or $u_{xx} = u_{tt}$)
\item $g(x) = u(x, 0)$ is the initial shape of the rope.
\item It's assumed that the rope is not moving initally, so $u_t(x,0) = 0$
\end{itemize}

Beginning to solve this:
\begin{itemize}
\item $u_{tt} = u_{xx} \Rightarrow u_{tt} - u_{xx} = 0$
\item Sort of like $a^2 - b^2 = 0 \Rightarrow (a+b)(a-b) = 0$, we have $0 = (\frac{\delta}{\delta t} \pm \frac{\delta}{\delta x})(u_t \mp u_x) 
= u_{tt} - u_{xt} + u_{tx} - u_{xx} = u_{tt} - u_{xx}$
\item This means the solution is either $u_+ = u_{t} + u_{x}$ or $u_- = u_{t} - u_{x}$.  Note - we don't solve these simultaneously, since that just gives us $u(x,t) = 0$.
\item These can be written as, e.g. $0 = u_t + u_x = \langle 1, 1 \rangle \cdot \langle u_x, u_t  \rangle =  \langle 1, 1 \rangle \cdot \nabla u$
\item TRICK: This is a directional derviative along $\langle 1, 1 \rangle$.  Introducing a variable like $s$ (accelerant along $\langle 1,1 \rangle $?) below does nothing interesting:
\begin{itemize}
\item $\frac{d}{ds}[u(x+sb, t+sc)] = \frac{\delta u}{\delta x}(x+sb,t+sc)b +  \frac{\delta u}{\delta t}(x+sb,t+sc)c = \langle b, c\rangle \cdot \nabla u(x+sb, t+sc)$
\item So if we set $b = c =1$, we see that $\frac{d}{ds}[u(x+s, t+s)] = \langle 1, 1\rangle \cdot \nabla u(x+s, t+s)$  
\item However, since in our world, $u_x + u_t = 0$, then this dot product is zero, and $\frac{d}{ds} u = 0$.  This is then \emph{constant in s}.
\item So then $u(x,t) = u(x+s, t+s)$, and shifting $x$ forward by $s$ (seconds?) and $t$ by the same changes nothing.  \emph{Interpretation: $u(x,t) = u(x+s, t+s)$ means that $s$ (``shift'') seconds later, the point $x+s$ will see the same displacement as $x$.  The wave goes ``right'' down the line}.
\item From this, we see that $u_+(x, t) = u_+(x-t, 0)$ as well.  So, our function at $t$ is what happened $t$ seconds ago at the origin.  
\end{itemize}
\item Note: We can't have one solution satisfy both conditions $u_+ = g(x), (u_+)_t = 0$, since then $g'(x) = 0$ which only works if $g$ is a constant.
\item Also, $u_{tt} - u_{xx} = 0$ is a linear PDE, in that solutons $u_1(x,t), u_2(x,t)$ see that $\frac{\delta^2}{\delta t^2}[c_1u_1(x,t) + c_2u_2(x,t)] - \frac{\delta^2}{\delta x^2}[c_1u_1(x,t) + c_2u_2(x,t)] = 0$.  Multiply by a constant or add solutions together and it's still zero.
\item  If we set $t=0$, we get  $u(x,t) = c_+g(x+t)+c_-g(x-t) \Rightarrow u(x,0) = c_+g(x+0) + c_-(x-0) = (c_+ + c_-)g(x) = u(x,0)$ ,so $(c_+ + c_-) = 1$
\item Differentiating by $t$, $u_t(x,t) = c_+g'(x+t) - c_-g'(x-t)$ so $u_t(x,0) = (c_+ - c_-)g'(x) \Rightarrow (c_+ - c_-) = 0$.  So $c_+ = c_- = \frac{1}{2}$, and \textbf{our solution with initial shape $g(x)$ with $g'(x) = u_t(x, 0) = 0$} is $u = \frac{1}{2}g(x+t) - \frac{1}{2}g(x-t)$
\item \textbf{This no-initial-velocity wave function translates} into ``my displacement at time 3, say, is the average of the initial displacements 3 to my left and 3 to my right'' (as those urges meet at ``me'' 3 seconds from the start).  Conceptually, along the fixed initial curve $g(x)$, each point  sends out two sensors, one left, one right, and averages the initial values at those points to find itself at time $t$. So the top of a hill will start dipping down, becoming two hills pushing out, for example.
\end{itemize}

With inverted conditions $u(x, 0) = 0, u_t(x,0) = f(x)$, we can use the fact that $u(x,t)$ solving the wave equation implies $u_t$ solves it as well!

\begin{itemize}
\item  $\frac{\delta^2u}{\delta t^2} - \frac{\delta^2u}{\delta x^2} = 0 \Rightarrow \frac{\delta}{\delta t}[\frac{\delta^2u}{\delta t^2} - \frac{\delta^2u}{\delta x^2}] = 0 \Rightarrow  \frac{\delta^2u_t}{\delta t^2} - \frac{\delta^2u_t}{\delta x^2}  = 0$.
\item Therefore, $u_t(x,0) = f(x)$ admits the same solution $u_t(x,t) = \frac{1}{2} [f(x+t) - f(x-t)]$
\item Since $u(x,t) - u(x,0)  = \int  \frac{1}{2} [f(x+t) - f(x-t)]dt$, and $u(x,0) =  0$ by assumption in this setup, 
$u(x,t) = \int_{s=0}^{s=t} [f(x+s) - f(x-s)]ds$, which is $\frac{1}{2} \int f(s)$ from $x-t$ to $x+t$  = $\frac{1}{2}\int_{s=x-t}^{s=x+t} f(s)ds$
\end{itemize}

And because the region of the intergral for a point $x$ gets wider as $t \rightarrow \infty$, on a flat rope with a pulse in the middle at $x=0$, we see $u(x,t)$ sitting at $0$ until the wave meets it, at which point it rises and then stays at the peak (integral of the whole thing).

So \textbf{d'Alembert's formula} is the superposition of the initially flat wave with the initially still wave, which accomodates \emph{all} solutions:

\fbox{$u(x,t) = \frac{1}{2}[g(x+t) + g(x-t)] + \frac{1}{2}\int_{s=x-t}^{s=x+t} f(s)ds$}

\emph{For the case of no boundary conditions}, this solves $u_{tt} = u_{xx}$ for $u(x,0) = g(x), u_t(x,0) = f(t)$.  In this instance, the propagation speed is clearly finite.

\emph{Note: This complete of a PDE solution is unusual.}

\subsection{Sources and Boundary conditions}

\textbf{Scenario 1: Here, we fix the infinite rope at the origin}, with the wave coming in from the negative x-axis.

Looking at \textbf{boundary conditions}, or constraints on spatial edges of a PDE problem:
\begin{itemize}
\item A free boundary (a loop that can shift up and down a pole) will cause a reflected wave to travel backwards.
\item A fixed boundary (setting $u(0, t) = 0, t \geq 0$) will cause an inverted pulse backwards.  
\end{itemize}

We set up a function $\tilde{u}(x,t) = \{ u(x,t), x \leq 0; = -u(-x, t), x \geq 0\}$ using \textbf{extension by odd reflection}.  So an inverted ghost rope exists to the right of the origin.

\emph{Note}: This seems to be more about cleverly encoding a boundary behavior (we will invert our wave) with this ghost rope than proving we'll have that behavior with math.

\begin{itemize}
\item And if $u_t(x,0) = 0, g(0) = 0, u(x,0) = g(x)$ extended to $x > 0$ as $\tilde{g}(x)$, then d'Alembert's applies: $\tilde{u}(x,t) = \frac{1}{2}[\tilde{g}(x+t) + \tilde{g}(x-t)]$
\item So when $x \leq 0, x \leq t \Rightarrow -t \leq x \leq 0$: (meaning, negative $x$, close enough to the origin to be affected by time $t$)
\begin{itemize}
\item $\tilde{u}(x,t) = u(x,t)$ here, since there's no inversion on the left side.
\item $(x+t)$ is positive , so $\tilde{g}(x+t) = -g(-(x+t))$ by definition of $\tilde{u}$.
\item $(x-t)$ is negative , so $\tilde{g}(x-t) = g(x-t)$ by definition of $\tilde{u}$.
\item So d'Alembert's reduces to $u(x,t) = \frac{1}{2} [-g(-(x+t)) + g(x-t)]$.  This means \emph{I'm the average of the starting position to my left $t$ seconds ago, and the inverted right-of-origin ghost position to my right $t$ seconds ago}
\end{itemize}
\end{itemize}

This means that for the part of the rope we care about, $x \leq 0$:
\begin{itemize}
\item For $x \leq -t$ (parts of the line unaffected by the reflection so far),  $u(x,t)= \frac{1}{2}[g(x+t) + g(x-t)]$
\item For $x \geq -t$ (parts affected by the reflection)  $u(x,t) = \frac{1}{2} [-g(-(x+t)) + g(x-t)]$
\item The intuition, still hard to visualize: if I'm zero at point -10, and wave crests at -11, then
\begin{itemize}
\item First my left sensor will eat the left wave and I'll go up and over.  
\item Then much later my right sensor will eat the right shadow wave and I'll do the inverted behavior.
\item These in total mean I'll get a reflection.
\item For the intuition, keep moving my point closer to the origin - nothing changes.
\end{itemize}
\end{itemize}

\textbf{Scenario 2: Here, we let the rope slide up and down at the origin}, but bound the total energy:

\begin{itemize}
\item One hand on the rope at $x=-L$, very far away:
\item Our energy is the sum of kinetic (change in u based on time?) and elastic (change in u based on x?) energies.
\item $E = \int_{x=-L}^{x=0} [(\frac{\delta u}{\delta t})^2 + (\frac{\delta u}{\delta x})^2]dx$
\item We can't gain or lose energy.  This means $\frac{dE}{dt} = 0$. Solving that:
\begin{itemize}
\item $0 = \frac{d}{dt} ( \frac{1}{2} \int_{x=-L}^{x=0} [(\frac{\delta u}{\delta t})^2 + (\frac{\delta u}{\delta x})^2]dx)  = \frac{1}{2} \int_{x=-L}^{x=0} [ \frac{\delta}{\delta t} (\frac{\delta u}{\delta t})^2 + \frac{\delta}{\delta t} (\frac{\delta u}{\delta x})^2]dx)$
\item $= \int_{x=-L}^{x=0} [ u_t u_{tt} + u_x \frac{\delta u_t}{\delta x}]dx)$.  
\item (Do integration by parts on the second term with $U = u_x, dV= \frac{\delta u_t}{\delta x})$: $0 = \int_{x=-L}^{x=0} [ u_t u_{tt}]dx + u_xu_t - \int_{x=-L}^{x=0} [u_tu_{xx}]dx =  \int_{x=-L}^{x=0} u_t[u_{tt} - u_{xx}]dx + u_xu_t$
\item Since $u_{tt} - u_{xx}$ = 0 (REMEMBER YOUR PROBLEM-SPECIFIC IDENTITIES!), $u_x(0,t)u_t(0,t) = 0$
\item Saying the displacement can't change with respect to $t$ there gives us the fixed rope case above, so that's uninteresting.
\item Therefore, if there's no energy change as the rope vibrates, we know $u_x(0,t) = 0$
\end{itemize}

Note: Dirichlet conditions are constraints on the value of the function at the boundary (like $u(0, t) = 0$).  Neumann constraints are on the derivatives at the boundary.

So redoing d'Alembert with the energy conservation, and therefore the ``Neumann'' condition $u_x(0,t) = 0$:
\begin{itemize} 
\item We know if $u$ solves $u_{tt} - u_{xx} = 0$, then $u_x$ does too, since $0 = u_{tt} - u_{xx} = \Rightarrow  0 = \frac{d}{dx}[u_{tt} - u_{xx}] = [[u_x]_{tt} - [u_x]_{xx}] = 0$.
\item We know $u_x(0,t) = 0$ by given constaints, so then we enforce this through odd reflection on $u_x$ as well: $\tilde{u}_x = \{ u_x(x,t), x\leq 0; -u_x(-x,t), x \geq 0\}$
\item By D'Alembert, this solves the wave equation with $u_x(x,0) = g'(x)$, so $\tilde{u}_x = \frac{1}{2}[\tilde{g}'(x+t) + \tilde{g}'(x-t)]$
\item Therefore at $-t \leq x \leq 0=, \tilde{u}_x(x,t) = \frac{1}{2}[\tilde{g}'(x+t) + \tilde{g}'(x-t)] = \frac{1}{2}[- g'(-x-t) + g'(x-t)]$
\item Then integrating, we drop the minus sign in the first term! $u(x,t) =  \frac{1}{2}[g(-x-t) + g(x-t)] + C$.  Note that $u(x,0) =  \frac{1}{2}[g(x) + g'(x))] \Rightarrow C = 0$!
\end{itemize}

(Note: a nonzero initial velocity profile $u_t(x,0) = f(x)$ can be handled as well.  We skip it).
\end{itemize}

Remember the 1D springs Wave Equation, where springs are initially $l$ apart, have displacement from this measured by $u(x, t)$,  have Hooke's coefficient $k$?
\begin{itemize}
\item Force pushing from the left on ball $x$: $F_L = k[u(x-l,t) - u(x,t)]$
\item Force pushing from the right on ball $x$: $F_R = k[u(x+l,t) - u(x,t)]$
\item Additional ``source'' force $F(x,t)$ means total force $F_{tot} = F_L(x,t) + F_R(x,t) + F(x,t)$
\item $F_{tot} = ma = mu_{tt}$
\item The Taylor-ish formula $f(x + \delta x) \approx f(x) + f'(x)(\delta x) + f''(x)(\delta x)^2$ means $F_L + F_R \approx kl^2f''(x) = kl^2u_{xx}$
\item Therefore, $mu_{tt} = kl^2u_{xx} + F(x,t) \Rightarrow F(x,t) = u_{tt} - \frac{kl^2}{m}u_{xx}$.  Set $1 = v = \frac{kl^2}{m}, f(x,t) = 
\frac{F(x,t)}{m}$ to get a simplified all-purpose wave equation. $f(x,t) = u_{tt} - u_{xx}$, with $f$ as the source force-per-unit-mass.
\end{itemize}

\textbf{New setup: Source force} $f(x,t)$, ignore boundary conditions, and set $u(x,0) = 0, u_t(x,0) = 0$ (still, flat (infinite) rope).
\begin{itemize}
\item Part 1: We can relate $f(x,t)$ to a made-up intermediate function $I(x,t)$ which has properties motivated by $u_{tt} - u_{xx} = (\frac{\delta}{\delta t} - \frac{\delta}{\delta x})(\frac{\delta u}{\delta t} + \frac{\delta u}{\delta x})$
\item $I(x,t) = (\frac{\delta u}{\delta t} + \frac{\delta u}{\delta x})$
\item $u_{tt} - u_{xx} = f(x,t) \Rightarrow (\frac{\delta I}{\delta t} - \frac{\delta I }{\delta x}) = f(x,t)$
\item We can derive that $u(x,0) = 0, u_t(x,0) = 0$ means that at $x=0$, $I(x,0) =  u_t(x,0) +  u_x(x,0) =  u_x(x,0)$
\item Since $u(x,0) = 0$ and $I(x,0) = u_x(x,0)$, $I(x, 0 ) = 0$.
\end{itemize}

We can relate $f(x,t)$ and $I(x,t)$:
\begin{itemize}
\item Use the dummy variable trick, and look at $f(x-s, t+s)$.  We know also that $\frac{\delta I}{\delta t} - \frac{\delta I }{\delta x} = f(x,t)$
\item $f(x-s, t+s) = \frac{\delta I}{\delta t}(x-s,t+s) - \frac{\delta I }{\delta x}(x-s,t+s) = \frac{d}{ds}[I(x-s, t+s)]$ by chain rule.
\item Integrating both sides: $\int_{s=-t}^{s=0} f(x-s, t+s)ds = I(x,t) - I(x+t, 0) = I(x,t)$
\item We can rewrite, using $k=-s$, as $I(x,t) = \int_{k=t}^{k=0} f(x+k, t-k)d(-k) = \int_{s=0}^{t}f(x+s, t-s)ds$ 
\end{itemize}

Using the same technique, we can relate 	$I(x,t)$ and $u(x,t)$ since $I(x,t) = (\frac{\delta u}{\delta t} + \frac{\delta u}{\delta x})$
\begin{itemize}
\item Use the dummy variable trick with variable $s'$ and look at $f(x-s', t-s')$.  We know also that $\frac{\delta u}{\delta t} + \frac{\delta u }{\delta x} = I(x,t)$
\item $I(x-s', t-s') = \frac{\delta u}{\delta t}(x-s,t-s) - \frac{\delta u}{\delta x}(x-s',t-s') = \frac{d}{ds'}[u(x-s', t-s)']$ by chain rule.
\item Integrating both sides: $\int_{s'=-t}^{s'=0} f(x-s', t-s')ds = u(x,t) - I(x-t, 0) = u(x,t)$
\item We can rewrite, using $j=-s'$, as $u(x,t) = \int_{j=t}^{j=0} f(x-j, t-j)d(-j) = \int_{j=0}^{t}I(x-j, t-j)dj$
\item So, $u(x,t) = \int_{s'=t}^{s'=0} f(x-s', t-s')ds' = \int_{s'=0}^{t}I(x-s', t-s')ds'$
\end{itemize}

Combining these, $u(x,t) =  \int_{s'=0}^{t}I(x-s', t-s')ds'$, and $I(x,t) =  \int_{s=0}^{t}f(x+s, t-s)ds$:
\begin{itemize}
\item  $I(x-s', t-s') =  \int_{s=0}^{t}f(x+s-s', t-s-s')ds$:
\item So $u(x,t) =  \int_{s'=0}^{t}  \int_{s=0}^{t}f(x+s-s', t-s-s')dsds'$
\item Change of variables, \fbox{ $y= x+s-s', w = s' + s \Rightarrow u(x,t) = \frac{1}{2} \int_{w=0}^t \int_{y=x-w}^{y=x+w} f(y, t-w)dydw$}
\begin{itemize}
\item TODO: The $\frac{1}{2}$ term apparently comes from the Jacobian (TODO) $\| \frac{\delta(s', s)}{\delta (w,y)}\|$
\end{itemize}
\item This together means that the \emph{points that can influence $u(x,t)$} in the $xt-$plane are a triangle with $(x,t)$ as the top, reaching down to $t=0$, slope 1.  So the ``wave speed'' in this setup is 1.
\end{itemize}


\subsection{2D and 3D (Compression) Waves}

(Note: The 2D equation will fall out of the 3D one).

Major setup for 3D compression waves:
\begin{itemize}
\item Air molecules compress together from sound, so $u(x,y,z,y)$ measures the density of air at that point.
\item Let's assume $g(x \pm t)$ plays the same role as last time: the initial wave state.  (Note: The setup implies we're looking at waves that propagate at ``one x per t''.)
\item Sound can come from multiple directions, so the expanded version should look like $u(\vec{x}, t) = g(\hat{n} \cdot \vec{x} \pm t)$, with $\hat{n}$ some fixed direction in $\mathbb{R}^3$.
\item The equation is \fbox{$u_{tt} = u_{xx} + u_{yy} + u_{zz}$} since:
\begin{itemize}
\item Setting $\hat{n} = \hat{i}$ or any other basis vector, $g(\hat{i} \cdot \vec{x} \pm) = g(x \pm t)$, which means if the other dims are zero, then $u_{tt} = u_{xx}$ (reduce to 1D case).  That checks out (necessary, not sufficient)
\item $\frac{\delta}{\delta t}[g(\hat{n} \cdot \vec{x} \pm t)] = \pm g'(\hat{n} \cdot \vec{x} \pm t)$, same for $g'$' and $\frac{\delta^2}{\delta t^2}$
\item $\frac{\delta^2}{\delta x^2}[g(\hat{n} \cdot \vec{x} \pm t)] = \hat{n}_x^2g''(\hat{n} \cdot \vec{x} \pm t)$, same for $y, z$
\item Since $[\hat{n}_x^2 + hat{n}_y^2 + hat{n}_z^2 = 1], u_{tt} = u_{xx} + u_{yy} + u_{zz}$ works out.
\item This can also be written $u_{tt} - \nabla^2u$
\end{itemize}
\end{itemize}


Setup with a forcing function:
\begin{itemize}
\item $u_{tt} - \nabla^2u = f(\vec{x}, t), u(\vec{x}, 0) = 0, u_t(\vec{x}, 0) = 0$.
\item So with a still, blank initial state $f$ is going to be a POP at the origin for a brief time.
\item Taking our experience from actual sound, we expect it to decrease away from the origin, and for there to be a finite propagation speed.
\item It should also be spherically symmetric.
\end{itemize}

Switching to spherical coordinates ($u$ depends on $r, \theta, \phi$), and using the multivariable chain rule from vecctor calculus, we get
\begin{itemize}
\item $\nabla^2u = \frac{1}{r^2}\frac{\delta}{\delta r} [r^2 \frac{\delta u}{\delta r}] + \frac{1}{r^2\sin^2(\phi)} \frac{\delta^2u}{\delta \theta ^2} + \frac{1}{r^2\sin(\phi)} \frac{\delta}{\delta \phi} [\sin(\phi)\frac{\delta u}{\delta \phi}]$
\item If we're taking this to be spherically symmetric, then we can zero out $\phi, \theta$ terms:  $\nabla^2u = \frac{1}{r^2}\frac{\delta}{\delta r} [r^2 \frac{\delta u}{\delta r}] $
\item Expanding this out, this means that $u_{tt} -\frac{1}{r^2} \frac{\delta}{\delta r}[r^2\frac{\delta u }{\delta r}] = u_{tt} - \frac{2}{r} u_r - u_{rr} = f(r,t)$
\item If we set $U = ru$, and churn through with e.g. $u_r  = \frac{\delta}{\delta r} [\frac{U}{r}] = -\frac{U}{r^2} + \frac{U_r}{r}$, etc., we end up with $U_{tt} - U_{rr} = rf(r,t)$.  Note that $\frac{U}{r} = u$ means that the solution diminishes $u$ with distance.
\item So now we're solving with $U(r,0) = U_t(r,0) = 0$ since $U = ru$.
\item Though only $r \geq 0$ matters, we need to keep $U(0,t)$ at zero through odd reflection.  Note that if $f$ is even, $rf(r,t)$ is odd.
\item The result from last quiz implies: $U(x,t) = \frac{1}{2} \int_{s=0}^t \int_{\rho=r-s}^{\rho=r+s} \rho f(\rho, t-s)d\rho ds$ , with $s$ subbing for dummy $w$ and $\rho$ being the distance instead of $y$. Note that our function is really $\rho f$ now instead of $f$.
\item Building a ``Dirac delta snap''  for a symmetric pop at the origin, set $f(r,t) = \frac{1}{\epsilon} e^{-\pi^2 \frac{r^2}{\epsilon^2}}\chi(t)$ for tiny $\epsilon$
\item Define $\delta(\rho) = \frac{\exp(-\frac{\pi^2\rho^2}{\epsilon^2})} {\epsilon^2}$, change $s' = t-s$
\item Eventually the math reduces to a delta pop at $r - t + s'$ (in range) and $r + t - s'$ (outside the interval)
\item The math reduces to $\int_{s'=0}^{s'=t} \chi(s')\delta(r -t + s')ds'$, or just $\chi(t-r)$ (due to the integral of $\delta$ being one exactly at $t-r$.
\item Therefore, \fbox{$ U(r,t) = \{ \frac{\epsilon^3}{(2\pi)^2}\chi(t-r), t - r > 0; 0, t -r \leq 0 \}$}
\item Looking at this, we confirm that disturbance diminishes with distance, and has a finite propagation speed.
\item \emph{TODO: So I guess $\chi$ is the actual initial wave function of time at the origin?}  The delta was I suppose there to ``center'' it?
\end{itemize}

What if we don't have spherical symmetry?  
\begin{itemize}
\item in the general case, all points $\vec{x}$ influence fixed point $\vec{P}$ through
\begin{itemize}
\item Distance separating points $r = \| \vec{x} - \vec{P} \|$
\item Normalized direction $\frac { \vec{x} - \vec{P}}{\| \vec{x} - \vec{P} \|}$
\end{itemize}
\item However, we can \emph{average} $u$ over all points $r$ away: $U(r,t;P) = \frac{1}{4\pi r^2} \iint_{S(\vec{P}, r} u(\vec{x}, t) d\sigma(\vec{x}$, with $S$ being the r-sphere around P
\item Getting the r-partials requires writing each $\vec{x}$ as some $\vec{P} + r\hat{n}$ over all directions, and using the divergence theorem.
\item LOTS OF ALGEBGRA IN HERE to get $U_{tt} - U_{rr} - \frac{2}{r}U_r = F(r,t)$ with a dependence on $r,t. \vec{P}$.  This $F$ is an even function, and it approaches $u(x,t)$ as $r$ approaches zero.
\item Our equation ens up being $u(\vec{P}, t) = \frac{1}{4\pi} \iiint_{B(\vec{P}, t)} \frac{f(\vec{y}, t - \| \vec{y} - \vec{P} \|)}{ \| \vec{y} - \vec{P} \|} d\vec{y}$
\item Like the other case, we see that we points in space affecting $U$ are a ``4D cone'' with vertex at $(\vec{P}, t)$
\item Also, if we consider that $f$ doesn't depend on $z$, we can flatten this spherical integral to a 2D one by looking at columns of $z$ over the disc of radius $t$
\item This ends up being $u(\vec{P}, t) = \frac{1}{2\pi} \iint_{B_2(\vec{P}, t)} f(\vec{y}) \ln(\sqrt { (\frac{t}{\| \vec{y} - \vec{P} \|})^2 - 1}   + \frac{t}{\| \vec{y} - \vec{P} \|}  )d\vec{y} $
\item This \textbf{method of descent} is really just ``reducing'' our 2D case from a 3D one.
\item Also, the \textbf{spherical averages} let us reduce a 3D problem to a 1D problem, given the assumptions of the problem.
\end{itemize}


\subsection{2D waves (boundary constrained): Separation of Variables} 

Main idea: ``Guess'' that a function like $u(x,t)$ can be factored into $u(x,t) = X(x)T(t)$ and work from there.  You can do this recursively as well like  $u(x,y,t) = S(x,y)T(t), S(x,y) = X(x)Y(y)$.

Main Setup: 
\begin{itemize}
\item Rectangular drumhead from $[0,0]$ to $[w, l]$
\item Vertical (z) displacement is $u(x,y,t)$, with Dirichlet condition $u(x,y,t) = 0$ enforced on the boundary.
\item Known that $u_{tt} = u_{xx}+ u_{yy}$.
\end{itemize}

Solving for $u$ by guessing that there's a split solution $u(x,y,t) = S(x,y)T(t)$.
\begin{itemize}
\item  $u_{tt} = u_{xx}+ u_{yy}$.
\item So $S(x,y)T''(t) = \frac{d^2S(x,y)}{dx^2}T(t) + \frac{d^2S(x,y)}{dy^2}T(t)$
\item $\Rightarrow \frac{T''(t)}{T(t)} = \frac{\nabla^2 S(x,y)}{S(x,y)}$ 
\item \textbf{Big A-ha}: left hand side is a function of \emph{t}, and right hand of \emph{x,y}.  If they are to be equal, they must both be consta
\end{itemize}


To solve for $T$:
\begin{itemize}
\item We know $\frac{T''(t)}{T(t)}$ is a constant, so equate it to $-k^2$ for some constant $k$.
\item Supposing the solution $T(t) = e^{rt} \Rightarrow T'' = r^2e^{rt}$, we know $r^2 = -k^2 \Rightarrow r = \pm ik$.
\item The solution $T(t) = Me^{ikt} + Ne^{-ikt}$ is equivalent to $T(t) = A\cos(kt) + B\sin(kt)$ since you can express $\sin, \cos$ as linear combos of $e^{ikt} = \cos(kt) + i \sin(kt)$ and $e^{-ikt} = \cos(kt) - i \sin(kt)$ 
\end{itemize}

To solve for $X, Y$:
\begin{itemize}
\item $\frac{\nabla^2 S(x,y)}{S(x,y)} = -k^2$ by its equality with $\frac{T''(t)}{T(t)}$.
\item Suppose we can formulate a solution so that $S(x,y) = X(x)Y(y) \Rightarrow -k^2X(x)Y(y)
 = Y(y)X''(x) + Y''(y)X(x) \Rightarrow -k^2 
 = \frac{X''(x)}{X(x)} + \frac{Y''(y)}{Y(y)}$.
 \item So, for some $j$, $\frac{X''(x)}{X(x)} = -j^2 \Rightarrow X(x) = C \cos(jx) + D \sin(jx)$ by the $T$ solution above.
 \item However, $X(x) = 0$  and $X \neq 0$ (not a constant function) forces us to conclude $C = 0 \Rightarrow X(x) = D\sin(jx)$.
 \item Also, the boundary condition $X(w) = 0$ also means for every $j = \frac{n \pi}{w}$, $n \in \mathbb{N}$, $X(x) = D \sin(\frac{\pi n}{w} x)$
 \item Following this identical logic for $Y$ over length $l$, y $q = \frac{m  \pi}{l}$,$Y(y) = F \sin(\frac{\pi m}{l} x)$
 \item Considering that $-k^2 = -j^2 - q^2$, this means $k = \sqrt{(\frac{\pi n}{w} x)^2+ (\frac{\pi m}{l} x)^2}$
 \item Then, $u =T(t)X(x)Y(y)$
 \item $ \Rightarrow u = (A_{mn} \cos (\sqrt{(\frac{\pi n}{w} x)^2+ (\frac{\pi m}{l} x)^2}) + B_{mn} \sin (\sqrt{(\frac{\pi n}{w} x)^2+ (\frac{\pi m}{l} x)^2}))(\sin(\frac{\pi n}{w} x))(\sin(\frac{\pi m}{l} x))$
 \item This runs over all $m,n \in \mathbb{N}$
\end{itemize}

Turning to a \textbf{circular membrane} with radius $r_0$, displacement described by $z = u(r, \theta, t)$:
\begin{itemize}
\item Boundary condition is then Dirichlet condition $u(r_0, \theta, t) = 0$ 
\item $u_{tt} = \nabla^2 u = \Rightarrow u_{tt} = u_rr + \frac{1}{r} u_r  +\frac{1}{r^2}u_{\theta \theta}$
\item (Note: I suppose this magic (forgotten from vector calculus) is because $u_{tt} = u_{xx} + u_{yy}$, and we're converting between $x,y$ and $r, \theta$)
\item $T(t) = A \sin(kt) + B \cos(kt)$ by identical logic to the rectangular drum, where $\frac{T''(t)}{T(t)}$ also was a constant.
\item Assuming similarly that  $S(r,\theta) = R(r)\Theta(\theta)$, you end up with $R(r)\Theta(\theta) [\frac{R''}{R} + \frac{R'}{rR} + \frac{1}{r^2}\frac{\Theta''}{\Theta} + k^2] = 0$
\item Finally, if $\frac{\Theta''}{\Theta} = \kappa$ for $\kappa \in \mathbb{R}$, then $\Theta(\theta) = Ce^{\sqrt{\kappa} \theta} + De^{-\sqrt{\kappa} \theta}$.  H
\item However, we have an additional condition that we have to be able to rotate the whole scene by $\theta = m2\pi$ radians and have it remain the same, or $\Theta(\theta + 2\pi) = \Theta(\theta)$.  This means $\kappa < 0$ since we're in ``imaginary exponents yielding sign an cosine'' territory.
\item This implies $\kappa = -m^2$ for an integer $m$, and therefore, \fbox{$\Theta(\theta) = C\cos(m\theta)+ D\sin(m\theta)$}.
\item We know also that  \fbox{$T(t) = A \cos(kt) + B\sin(kt)$} 
\item Setting $\frac{\Theta''}{\Theta} = \kappa = -m^2$, 
and with $R(r)\Theta(\theta) [\frac{R''}{R} + \frac{R'}{rR} + \frac{1}{r^2}\frac{\Theta''}{\Theta} + k^2] = 0 \Rightarrow$ 
\fbox{$r^2R'' + rR' + [k^2r^2 - m^2]R$} (a solution we'll look for later), we have the circular drum solution.
\end{itemize}


Notes to self:
\begin{itemize}
\item Need a clear intuition for a lot of things.  What do the variables and their derivatives physically mean?
\item Need more symbolic comfort with how integration works.
\end{itemize}

\subsection{Fundamental Solutions}

Main idea: The fundamental solution for the problem seems to:
\begin{itemize}
\item  Solve the differential equation.  Here, it is $u_t = \nabla^2 u$ or, specifically, $ u_t = u_{xx} + u_{yy}$
\item Solve the initial conditions
\item More specifically, as $t \rightarrow 0_+, u(x,y,t) \rightarrow g(x,y)$, or we can ``rewind back'' to the initial displacement setup.
\item Somehow , this is the function from which all heat equations (presumably specified as $g(x,y)$?) are built. 
\end{itemize}



Motivation for heat equation (random 1D walk)
\begin{itemize}
\item Drunkard starts at lamppost (position 0) and walks left or right every $\Delta t$, each with probability $\frac{1}{2}$
\item Probability of being $i$ steps away from lamppost at time $n\Delta t$ is $p(i, n\Delta t)$.
\item This depends only on $p(i-1, n\Delta t), p(i+1, n\Delta t)$ as $p(i, (n+1)\Delta t) = \frac{1}{2}p(i-1, n\Delta t) +  \frac{1}{2}p(i+1, n\Delta t)$
\item We can do the same thing we did the the 1D spring equation and approximate $p_x, p_{xx}$ by means of its relation to small perturbances.  Then we can relate this to $p_t$.
\begin{itemize}
\item Assume we're going to shrink these moves to $\Delta X$ instead of 1. 
\item We know $p(x, (n+1)\Delta t) = \frac{1}{2}p(x + \Delta x, n\Delta t) +  \frac{1}{2}p(x - \Delta x, n\Delta t)$.
\item We know the calculus rule for smooth $f$, small $\Delta x$: $f(x+\Delta x) \approx f(x) + f'(x)\Delta x + \frac{1}{2}f''(x)(\Delta x)^2$
\item Applying this, this means $\frac{1}{2}p(x, (n+1)\Delta t) + \frac{1}{2}p(x - \Delta x, n\Delta t) = \frac{1}{2}[p(x,n\Delta t) + p_x(x, n\Delta t)\Delta x + \frac{1}{2}p_{xx}(x, n\Delta t)(\Delta x)^2] +  \frac{1}{2}[p(x,n\Delta t) + p_x(x, n\Delta t)\Delta -x + \frac{1}{2}p_{xx}(x, n\Delta t)(\Delta -x)^2] = p(x,n\Delta t) + \frac{1}{2}[\Delta x]^2 p_{xx}(x, n\Delta t)$
\item So for small steps, $p(x, (n+1)\Delta t) \approx p(x, n\Delta t) + \frac{1}{2}[\Delta x]^2 p_{xx}(x, n\Delta t)$].  \emph{So we have this rule for $p$ relating to derivatives along $x$.}
\item Now let's relate to derivatives along $t$! For small $\Delta t$, the definition of $\frac{\delta p}{\delta t} = \frac{p(x, (n+1)\Delta t) - p(x, n\Delta t)}{\Delta t} \approx \frac{[\Delta x]^2}{2 \Delta t}\frac{\delta ^2 p}{\delta x^2}(x, n \Delta t)$
\item Special case: If we set $t = n\Delta t$, then in this case, we can say $\frac{\delta p}{\delta t} = D \nabla^2 p, D > 0$ if $D = \frac{[\delta x]^2}{2 \Delta t}$, the \textbf{diffusion equation}.  Note: Since we're pushing $\Delta x, \Delta t \rightarrow 0$, is this, uh, $D \approx  \frac{[\Delta x]^2}{2 \Delta t}$?
\end{itemize}
\end{itemize}

We consider that $p$ should be peaked (sharply) around 0, since it's just as likely to go left as right.  We hypothesize that we should use a (sharp) bell curve to make that happen.  (Note: Is this sort of like the Dirac Delta function?)
\begin{itemize}
\item Looking at $D = \frac{[\Delta x]^2}{2 \Delta t}$, \emph{we hypothesize that the operative units must be} $\frac{x^2}{t}$
\item Therefor rour bell curve looks like $p(x,t) = C(t)\exp\{-\frac{x^2}{\sigma^2 t}\}$ 
\item Since it's a probability measure along the real line, $\int_{x=-\infty}^{x=\infty} p(x,t) dx = 1$.  At time $t$, the particle is at \emph{some} $x$.
\item Using identity $\int_{u=-\infty}^{u=\infty} e^{-au^2}du = \sqrt{\frac{\pi}{a}}, a > 0 \Rightarrow a = \frac{1}{\sigma^2 t} \Rightarrow C(t) = \frac{1}{\sigma \sqrt{\pi t}}$
\item Even though our guess was unsubstantiated, we see $p(x,t) = \frac{1}{\sigma \sqrt{\pi t}} \exp\{-\frac{x^2}{\sigma^2 t}\}$ solves $p_t = Dp_{xx}$.  To solve for D:
\item To solve for D, find $p_t = \frac{\delta }{\delta t} [\frac{1}{\sigma \sqrt{\pi t}} \exp\{-\frac{x^2}{\sigma^2 t}\}]$, same for $p_{xx}$.  Through lots of chain rule churning, we see $p_t = \frac{\sigma^2}{4}p_{xx} \Rightarrow D = \frac{\sigma^2}{4} \Rightarrow \sigma = 2\sqrt{D}$.  \emph{Note that $p$ can't be negative, so ignore -$2\sqrt{D}$.}
\item This implies our solution is $p(x,t) = [\frac{1}{\sqrt{4 \pi D t}} \exp\{-\frac{1}{4D} \frac{x^2}{t}\}], t > 0$
\end{itemize}

To say our solution is a  \textbf{fundamental diffusion solution} means, further, that we can start with any initial conditions $p(x, 0)= g(x)$, watch the equation unfold over $t$, and we'll still have $p_t = Dp_{xx}$.  

\begin{itemize}
\item A \textbf{convolution} $(\Phi \star g)(x, t)$ of (probability function) $\Phi$ and initial state $g$ is the function that combines the two at point $x$, or $(f \star g)(x) \int_{y=-\infty}^{y=\infty} f(x-y)g(y)dy =  \int_{y=-\infty}^{y=\infty} f(y)g(x-y)dy = ... $ (TODO)
\item In this case, ($\Phi \star g) = p(x,t) = \int_{y=-\infty}^{y=\infty} \frac{1}{\sqrt{4\pi D t}} \exp\{-\frac{1}{4D}\frac{(x-y)^2}{t}\}g(y)dy, t >0$
\item It has the properties: 
\begin{itemize}
\item $\lim_{t \rightarrow 0^+} p(x, t) = g(x)$ for every reasonable choice of $p(\vec{x}, 0) = g(\vec{x})$.
\item $p_t = Dp_{xx}$
\end{itemize}
\item Note we can find a fundamental solution for the heat equation $u_t = \nabla^2 u$ as well
\begin{itemize}
\item For some $v$, we can hypothesize a distribution function $\Phi(\vec{x}, t) = t^{-a}v(\frac{\|x\|^2}{t})$
\item Taking derivates $\frac{\delta}{\delta t}, \frac{\delta^2}{\delta x_i^2}$ lets us use $u_t = u{x_0x_0} + u_{x_1x_1} + ...$
\item Calling the argument $z = \frac{\|x\|^2}{t}$, we take derivatives and find $0 = av(z) + (z+2n)v'(z) + 4zv''(z)$.  Note that $n$ is from $\mathbb{R}^n$, since $\nabla (\|\vec{x}\|^2) = 2\vec{x}$ and $\nabla (2\vec{x}) = 2n$.
\item How do we choose $a$ so that the equation can be solved as $[\frac{1}{4}v(z) + v'(z)$?  
\item Note that if $[\frac{1}{4}v(z) + v'(z)] = 0$, then its derivative $\frac{d}{dz} [\frac{1}{4}v(z) + v'(z)]=[\frac{1}{4}v'(z) + v''(z)] =0$
\item Setting $a = \frac{n}{2}$, we get $0  = 2n[\frac{1}{4}v(z) + v'(z)] + \frac{z}{4}\frac{d}{dz}[\frac{1}{4}v'(z) + v''(z)]$
\item We can rearrange to see $[av(z) = 2nv'(z)] + z[v'(z) + 4v''(z)] = 0$.
\item Therefore the solution is $\Phi(\vec{x},t) = t^{-\frac{n}{2}}v(\frac{\|x\|^2}{t})$.  To find $v$, solve $[\frac{1}{4}v(z) + v'(z)] =0$
\item This is obviously $v(z) = Ce^{-frac{1}{4}z}$, or  $\Phi(\vec{x},t) = Ct^{-\frac{n}{2}}v(\frac{\|x\|^2}{t})$, but we must find $C$ so that $\int \Phi = 1$. 
\item The identity $\int_{\mathbb{R}^n} e^{-a\|x\|^2} = 
(\frac{\pi}{a})^\frac{n}{2}$ helps us find 
$\lim_{t \rightarrow 0^+} \frac{C}{t^{-\frac{n}{2}}} \int e^{-\frac{\|x\|^2}{4t}}d\vec{x} 
= \lim_{t \rightarrow 0^+}  \frac{C}{t^{-\frac{n}{2}}} (4\pi t)^{\frac{n}{2}} = C(4\pi)^{\frac{n}{2}}$
\item So the fundamental heat solution is $\Phi(\vec{x},t) = (4\pi t)^{-\frac{n}{2}}e^{-\frac{\|x\|^2}{4t}}$
\item As an example, take $g(x,y) = u(x,y,0) = u_0e^{-x^2+y^2}$, like a candle had heated the origin.
\item Solving $u(x,y) = (\Phi \star g)(\vec{x}, t) = \frac{u_0}{4\pi t}\int_{\mathbb{R}^2} e^{\frac{-\|x - y\|^2}{4t}}e^{-\|y\|^2}d\vec{y}$ basically requires completing the square to get rid of the $2\vec{x}\cdot\vec{y}$ components, then changing variables $\vec{v} = \vec{y} - \frac{1}{1+4t}\vec{x}$, which, over the whole plane, is the same integral.
\item You end up with $u(x,y,t) = \frac{u_0}{1+4t}e^{-\frac{x^2+y^2}{1+4t}}$
\item Computing partial derivatives $u_t, u_{xx}, u_{yy}$, checking $u_t = u_{xx}+u_{yy}$ and seeing that $\lim_{t \rightarrow 0^+}u(x,y,t) = g(x,y)$ validates it.
\item And it turns out, using identity $\int_{\mathbb{R}^2} e^{-a(x^2+y^2)}dxdy = \frac{\pi}{a}$
 (the square of the 1d case), that $u_tot = \frac{u_0}{1+4t}\int_{\mathbb{R}^2} e^{-\frac{x^2+y^2}{1+4t}} dx dy = \frac{u_0}{1+4t}(\pi (1+4t)) = \pi u_0$, so the total energy doesn't depend on time. It is conserved.
\end{itemize}


\end{itemize}

\subsection{Fun with Functionals}

Note that the heat solution ($u_t = nabla^2 u$) and the diffusion equation ($p_t = D\nabla^2 p$) are very similar.  The first measures how much heat (density) $u(x,y,z,t)$ exists at a spot $u$ in time; the second measures the probability density of a (particular) particle being in that spot at a time.  This means \emph{we can think about one in terms of the other}.

If, for example, we confined our space $G$  to an ellipsoid (or really, any closed region) in $\mathbb{R}^3$, then saying ``it's insulated to any heat going in our out'' and ``particles are confined to not leave or enter'' is the same thing.  They would have these things in common:

\begin{itemize}
\item The solutions \emph{feel} like they should settle down to a constant, uniform value as $t \rightarrow \infty$
\item The transfer at the ``skin'' $\delta G$ is zero so on the boundary, for normal $\hat{n}, D_{\hat{n}} |_{\delta G} = \nabla u \cdot n |_{\delta G} = 0$.  Otherwise, the heat transfer, say, would be nonzero.   (Note that this is an example of a Neumann condition.)
\item This means that $\frac{d}{dt} \iiint_G u d\vec{x} = 0$.  For one, no heat entering or leaving means the total has to stay the same.  
Also consider that $\frac{d}{dt} \iiint_G u d\vec{x} =  \iiint_G u_t d\vec{x} =  \iiint_G \nabla^2 u d\vec{x} $ (by heat equation definition) $= \iiint_G \nabla \cdot (\nabla u ) d\vec{x} =  \iint_{\delta G} \nabla u \cdot \hat{n} d\sigma(\vec{x})$  (by divergence them on function $\nabla u$)  $= 0$.  
\item We can define a cost function $C = \frac{1}{2}\iiint_G [(\nabla u) \cdot (\nabla u)] d\vec{x} =  \frac{1}{2}\iiint_G [(\nabla u)]^2d\vec{x}$, since this is zero exactly when $\nabla u \cdot \nabla u = 0$ all over the space.
\end{itemize}

The idea of \textbf{functionals} is that these setups for ``energy'' ($E[u] = \iiint_G u d\vec{x}$) and ``cost'' ($C[u] = \frac{1}{2} \iiint_G [\nabla u]^2 d\vec{x}$) take in \emph{functions} to become scalar-producing functions themselves.

We can prove a few things: the cost decreases over time, TODO

Cost decreases over time:
\begin{itemize}
\item $\frac{d}{dt} [\frac{1}{2} \iiint_G [\nabla  u]^2 dx]$
\item =  $\frac{1}{2}  [ \iiint_G \frac{\delta}{\delta t} [\nabla  u]^2 dx]$ by... Fubini's maybe ?
\item =$\iiint_G [\nabla u \cdot \nabla u_t] dx$ by the chain rule (applied to each dimension, basically)
\item Do a multivariable product rule $\nabla \cdot [fV] = \nabla f + V \cdot f\nabla \cdot V \Rightarrow \nabla f \cdot V = \nabla (fV) - f \nabla V$, setting $f = u_t, V = \nabla u$
\item This gives $C[u] = \frac{1}{2} \iiint_G [\nabla u]^2 dx = \iiint_G \{ \nabla \cdot [u_t \cdot \nabla u] - u_t \nabla \cdot \nabla u \} dx$
\item Using divergence on the first term gives  $= \iiint_G \{ \nabla \cdot [u_t \cdot \nabla u] = \iint_{\delta G} [u_t \cdot \nabla u] = 0$ by the boundary Neumann conditions
\item The second term $-\iiint_G ( u_t \nabla \cdot \nabla u )dx = -\iiint_G ( u_t \nabla^2 u )dx = -\iint_G [\nabla^2 u]^2 dx$ by definotion of heat equation.  The integrand must be non-negative, so the whole thing has to decrease or stay constant.
\end{itemize}

Note that if $\iint_G [\nabla^2 u]^2 dx = 0$ (exact equality), then $u$ must be constant.

\begin{itemize}
\item The above equation implies that $\nabla^2 u = 0$ throughout $G$. 
\item Use the product rule in reverse again, with $f = u, V = \nabla u: \nabla \cdot (u \nabla u) = \nabla u \cdot \nabla u + u \nabla^2 u$
\item We proved the second term is zero.  (The first term is equivalent to $\| \nabla u \|^2$, by the way.)
\item The divergence theorem say $\iiint_G \nabla \cdot (u \nabla u) dx = \iint_{\delta G} u\nabla u \cdot \hat{n} d\sigma  = 0$.
\item Therefore the cost $C[u]$ is 0, therefore $u$ is constant.
\end{itemize}

If it's an inequality, $C[u]$ must decrease forever.

Example: Unit cube $[0, 1] \times [0, 1] \times [0, 1]$, intial condition $u(x,y,z,0) = u_1 \cos(\pi x) \cos(2 \pi y) \cos(3 \pi z) + u_0$

\begin{itemize}
\item  Note that the boundary does satisfy $\nabla u \cdot \hat{n} |_{\delta G} = 0$:
\item Notice that $\nabla u = \langle K \sin(\pi x), L \sin(2 \pi y), M \sin(3 \pi z) \rangle$ for some messy constants, and those are all 0 at $x, y, z \in \{0, 1\}$ (the boundaries)
\item Also, we can solve this equation $u_t = \nabla u$: 

\begin{itemize}
\item Note that $u_xx = -\pi^2 (u-u_0), u_yy =  -4\pi^2 (u-u_0), u_yy =  -9\pi^2 (u-u_0)$, so $\nabla ^2 u = -14\pi^2 (u-u_0)$
\item $\frac{du}{dt} = 1-4\pi^2 (u-u_0)$
\item $\frac{du}{ (u-u_0)} = -14\pi^2 dt$
\item $\ln(u-u_0) = -14\pi^2t + C$
\item $u = u_0 + De^{-14\pi^2t}$.  Only $D =  u_1 \cos(\pi x) \cos(2 \pi y) \cos(3 \pi z)$ satisfies initial conditions.
\item So $u =  u_0 + u_1\cos(\pi x) \cos(2 \pi y) \cos(3 \pi z) e^{-14\pi^2t}$
\end{itemize}
\item We see also that energy is conserved, or that $E[u]= \iiint_G u(x,y,z,t) = \iiint_G u_0 + u_1[\int_0^1 \cos(\pi x)] \times [\int_0^1 \cos(2 \pi y)] \times [\int_0^1 \cos(3 \pi z)] = u_0$, since the factors (we can separate into them easily) are zero on this $[0, 1]$ interval. 
\item Finally $C[u] =  \frac{1}{2} \iiint_G [\nabla u]^2 dx $ for  $u =  u_0 + u_1\cos(\pi x) \cos(2 \pi y) \cos(3 \pi z) e^{-14\pi^2t}$ decreases over time approaching (but in general not hitting) 0, since $ [\nabla u]$ is some function $u_1e^{-14\pi^2 t}[-\pi \sin(...)\cos(...)\cos(...) + ..]$, and $ [\nabla u]^2 $ is $u_1^2e^{-28\pi^2t}$ times something less than one.  The integral is therefore less than $u_1^2e^{-28\pi^2t}$, and trends toward zero.
\end{itemize}


\subsection{Laplace Equation}

Motivation: Soap bubble created over an (arbitrary) wire loop.  Physics dictates this is of smallest area (\textbf{minimal area surfaaces}).

Laplaces's equation $\nabla^2 u = 0$

Setup:

\begin{itemize}
\item  Disk $D = \{ (x,y) \in \mathbb{R}^2 | x^2 + y^2 \leq \mathbb{R}^2 \}$
\item Function $h(x,y) > 0$ gives height of the wire at any point on $\delta D$.
\item So what is the function $u(x,y)$ with $u|_{\delta D}$ that describes the soap bubble?
\item Note that area is $A[u] = \iint_D \sqrt{1 + (\frac{\delta u}{\delta x})^2 + (\frac{\delta u}{\delta y})^2 } dx dy$
\begin{itemize}
\item TODO: Re-learn surface integrals again!
\end{itemize}
\item \emph{Lower bound} on $A[u]$ is necessarily $\pi R^2$, since $\sqrt{1 + (\frac{\delta u}{\delta x})^2 + (\frac{\delta u}{\delta y})^2 }  \geq \sqrt{1}$, and $D$ is of area $\pi R^2$.
\item \emph{Upper bound} on $\sqrt{1 + (\frac{\delta u}{\delta x})^2 + (\frac{\delta u}{\delta y})^2 }$, if $u$ is super spiky, I guess:

\begin{itemize}
\item Start with known theorem: \emph{Holder inequality} : $| \iint_D f(x,y)g(x,y)dxdy| \leq \sqrt{\iint_D f(x,y)^2dxdy} \sqrt{\iint_D g(x,y)^2dxdy} $
\item Set $f = \sqrt{1 + (\frac{\delta u}{\delta x})^2 + (\frac{\delta u}{\delta y})^2 } 
\equiv  \sqrt{1 + \langle \frac{\delta u}{\delta x},  \frac{\delta u}{\delta y}  \rangle \cdot   \langle \frac{\delta u}{\delta x},  \frac{\delta u}{\delta y}  \rangle}
\\
\equiv \sqrt{1+ \nabla u \cdot \nabla u}$ (written as $\sqrt{1+ [\nabla u]^2}$)
\item Set $g = 1$
\item Then $A[u] = \iint_Dfgdxdy \leq
 \sqrt{\iint_D (\sqrt{1+ [\nabla u]^2})^2dxdy}\sqrt{\iint_D (1)^2}dxdy 
 \\ 
 = 
  \sqrt{\iint_D (1+ [\nabla u]^2)dxdy} \sqrt{\pi R^2}
  $
  \item Square both sides and divide by $\pi R^2$ to get $\frac{A[u]^2}{\pi R^2} \leq \iint_D [1 + [\nabla u]^2]dx dy = \pi R^2 + \iint_D[\nabla u]^2 dx dy$
 \item So we find our upper bound: $0 \leq \frac{A[u]^2 - \pi R^2}{\pi R^2} \leq \iint_D[\nabla u]^2dxdy$
\end{itemize}
\item The upshot: We have to choose a $u$ bounded by the wire that makes $A[u]$ as close to $\pi R^2$ as possible.
\item We can make last quiz's cost functional $C[u] = \frac{1}{2}\iint_D [\nabla u]^2 dx dy$ (look familiar?) as small as possible.
\end{itemize}

\begin{itemize}
\item The main idea of \emph{nudges}: a 1D function $f(x)$ is at a minimum at $x_0$ if $f(x_0 + \eta) > f(x_0)$ for small $x_0$.
\item Instead of a bump $\eta$, we'll use a function bump (in display, looks almost like a Dirac delta) that distorts our function $u$, but still $\eta = 0$ around $\delta D$ to maintain our conditions.
\item So a ``minimizing'' function (? that minimizes $C[u]$ on this loop? ) $u$, with a minimum at $u_0(x,y)$ will necessarily be a minimum if $M = 0$ for $C[u_0(x,y) + M \eta (x,y)]$.  This means $\frac{d}{dM} |_{M = 0} C[u_0(x,y) + M \eta (x,y)] = 0$
\item Since $C[u] = \frac{1}{2}\iint_D[\nabla u]^2 dx dy$ We can find $\frac{dC}{dM}$ at 0:
\begin{itemize}
\item $\nabla[u_0(x,y) + M \eta (x,y)] = \nabla u_0(x,y) + M \nabla \eta(x,y)$
\item $\nabla[u_0(x,y) + M \eta (x,y)]^2 = \|\nabla u_0(x,y)\|^2 + M^2 \|\nabla \eta(x,y) \|^2 + 2 M \nabla u_0(x,y) \cdot \nabla \eta(x,y)$
\item Integrating the above and dividing by 2 we we  $C[u_0(x,y) + M \eta (x,y)] = C[u] + M^2C[\eta] + M\iint_D  \nabla u_0(x,y) \cdot \nabla \eta(x,y)$
\item At $M=0$, $\frac{dC}{dM} = \iint_D  \nabla u_0(x,y) \cdot\eta(x,y)$
\item Since we know this is 0, then we know $ \iint_D  \nabla u_0 \cdot \nabla \eta = 0$
\item So at $M=0$, the derivative w.r.t. $M$ is 0, and therefore $ \iint_D  \nabla u_0 \cdot \nabla \eta = 0$
\end{itemize}
\item Now we use this to prove that our minimizing function $u_0$ obeys Laplace: $\nabla ^2 u_0 = 0$
\begin{itemize}
\item If we have a mix of various $\nabla$ equations, it's often useful to see what we pieces we can play with by using the product rule for gradients: $\nabla \cdot (\eta \nabla u_0) = \eta \nabla^2 u_0 + \nabla u_0 \cdot \nabla \eta$.  
\item Also, we have the divergence theorem $\iint_D \nabla V = \int_{\delta D} V_r$. (I suppose $V_r$ substitutes for $V \cdot \hat{n}$).
\item So integrate everything over $D$: $\iint_D \nabla \cdot (\eta \nabla u_0) = \iint_D \eta \nabla^2 u_0  +\iint_D  \nabla u_0 \cdot \nabla \eta$.  
\item First term is $\int_{\delta D}  \eta \nabla (u_0)_r dl $ by Divergence, but $\eta \rightarrow 0$ near $\delta D$, so it's 0.  Third is  0 by previous result.
\item So $ \iint_D \eta \nabla^2 u_0 dx dy = 0$ for ANY bump function $\eta$.  Then if $\nabla ^2 u_0 \neq 0$ at some point, we can design an $\eta$, say a Dirac delta, that makes $\eta\nabla^2u_0 \neq 0$ there, and 0 elsewhere. But since  $ \iint_D \eta \nabla^2 u_0 dx dy = 0$ always, this means that $ \nabla^2 u_0  =0$ everywhere!
\end{itemize}
\item Just like making infinitesimal changes to functions in calcuus, this whole idea of ``bumping'' a functional forms the basis of \textbf{calculus of variations}, which is calculus on functionals.  We learned that any function that minimizes $C[u]$ with fixed boundary values $h(x,y)$ on $\delta D$, that \fbox {Laplace's equation $\nabla ^2 u = 0$} holds.  Solutions of Laplace's equation are called \textbf{harmonic} functions.
\end{itemize}

To find a solution for Laplace, use technique of \emph{spherical  averages}: 
\begin{itemize}
\item Around any point $P$, fix an $r$ and the corresponding disk $D = \{(x,y) \in \mathbb{R}^2 | (x-P_x)^2 + (y-P_y)^2 \leq r^2\}$
\item The average of $u$ on that boundary $\delta D$ is then $U(r;P) = \frac{1}{2 \pi r} \int_{\delta D(P, r)} u dl$
\item How do we find how this function changes (derivative) with respect to r?  \textbf{GOTCHA}: Can't crank through using $r$ directly since $r$ is in the limits of integration!  Solution: fix $r$ and integrate over $\theta$.
\begin{itemize}
\item Set $\delta D(P,r) = \{(P_x + r \cos (\theta), P_y + r \sin(\theta)\}$.
\item Then $U(r; P) = \frac{1}{2\pi r} \int_{\theta = 0}^{\theta = 2\pi} u(P_x + r cos(\theta), P_y + r\sin(\theta)) r d\theta$.
\item (Note: $dl = r d\theta$, so the $r$ sneaks in there.
\item Since the integral is over $\theta$, cancel $r$:$U(r; P) = \frac{1}{2\pi} \int_{\theta = 0}^{\theta = 2\pi} u(P_x + r cos(\theta), P_y + r\sin(\theta)) d\theta$.
\item So we want $U'(r;P) =  \frac{1}{2\pi} \int_{\theta = 0}^{\theta = 2\pi} \frac{d}{dr} u(P_x + r cos(\theta), P_y + r\sin(\theta)) d\theta$.
\item Notice this is $\nabla u(P_x + r cos(\theta), P_y + r\sin(\theta)) \cdot \langle \cos(\theta), \sin(\theta) \rangle$
\item That means we can use the divergence theorem as $U'(r;P) = \frac{1}{2\pi r} \int_{\delta D(P, r)} \nabla[u]_r dl = \frac{1}{2\pi r}\iint_{D(P,r)} \nabla^2 u dx dy$
\item The last term is 0 by supposition of the Laplace condition, so $U'(r;P) = 0$
\end{itemize}
\item So the average is constant in $r$, which means by shrinking $\lim_{r\rightarrow 0^+} U(r;P)$, we have to get $u(P)$.  The \textbf{mean value property} of a harmonic ($\nabla ^2 u = 0)$  function $u$ says that the point's value is the average of the values around it.
\item Logically, this means that either the whole function is constant, or that the max (min) has to occur on a boundary.
\item This ``average of the surrounding circle'' has to extend to surrounding disks too: 

\begin{itemize}
\item $\frac{1}{2\pi}_{\delta D(P,r)} \int u dl = ru(P)$ is the integral over a circle of radius $r$.
\item Integrating all of those from $r= 0$ to $r = r_0$: $u(P) \int_{r=0}^{r=r_0} r dr = u(P)\frac{r_0^2}{2}$
\item But also equals $\frac{1}{2\pi} \int_{r=0}^{r=r_0} [\int_{\delta D(P,r)}  u dl] dr = \frac{1}{2\pi} \iint_{D(P, r_0)} u dx dy$
\item So $u(P) = \frac{1}{\pi r_0^2} \iint_{D(P, r_0)} u dx dy$.  So $u(P)$ is the average of the u-values of the disk too.
\end{itemize}
\end{itemize}

Example: Using mean value of $z = \frac{1}{2} \sin(2 \theta) + 2$ around the circle of radius 1:
\begin{itemize}
\item By mean value theorem, $u(0,0) = 2$ (substitute directly) should be the same as $\frac{1}{2\pi}\int_{\delta D} u dl$
\item $\frac{1}{2\pi} \int_{\theta = 0}^{\theta = 2\pi} [\frac{1}{2}\sin(2 \theta) + 2] d\theta =
 \frac{1}{2\pi} [-\frac{1}{4}cos(2\theta) + 2\theta ]|_{\theta = 0}^{\theta = 2\pi} = 2$.
 
\end{itemize}

\subsection{Approximating Laplace}

Main idea: Most PDEs don't have exact solutions.  Even approximate solutions often need to be tailored to the particular PDE.  Two numerical solutions in the toolkit include:
\begin{itemize}
\item \textbf{finite difference method}: Discretize the space and use the spherical (surroundg points) average property of harmonic functions.
\item \textbf{Rayleigh-Ritz variational method}: Guess a solution with unspecified parameters and minimize the cost function over those parameters
\end{itemize}

\textbf{Example for Finite differences}: Imagine we're on a lattice of square size $h$ with $P_1, P_2, P_3, P_4$ in the $N, W, S, E$ positions around point $(x,y)$.  What is $u$?
\begin{itemize}
\item Tool: use Taylor approximation $f(x+h) \approx f(x) + hf'(x) + h^2\frac{f''(x)}{2}, h \neq 0$
\item This means $u(P_1) = u(x_0, y_0 + h) \approx u(x_0, y_0) + hu_y(x_0, y_0 + h) +h^2\frac{u_{yy}(x_0, y_0 + h)}{2}, u(P_3) = u(x_0, y_0 - h)  \approx u(x_0, y_0)  - hu_yf(x_0, y_0 - h)  + h^2\frac{u_{yy}(x_0, y_0 - h) }{2}$, similar for $P_2, P_4$.
\item Adding these together yields $\frac{1}{4} [u(P_1) + u(P_2) + u(P_3) + u(P_4)] =  u(x_0, y_0)  + \frac{2}{4} h^2\nabla^2 u$, and since $\nabla^2 u = 0$ in this setup, $u(x_0, y_0)$ is the average of its neighbors.
\item This should remind us of the mean-value property over circles and disks for these Laplace setups: $u(x,y) = \frac{1}{2 \pi r} \int_{C((x,y), r) \subset D} udl = \frac{1}{\pi r^2} \int_{D((x,y), r) \subset D} ud \sigma$ 
\item But \emph{big idea}: We aren't limited to immediately surrounding points.  We can extend this over the whole grid (reaching, say, known Dirichlet boundary conditions) and solve for the unknowns, if we have enough information! (matrix algebra).  
\end{itemize}

\textbf{Example}:If our grid looks like 

$ \begin{bmatrix} 
 h_4 & h_3 &h_2 &h_1 \\
h_5 & u_2 &u_1 &h_{12} \\
h_6 & u_3 &u_4 &h_{11} \\
h_7 & h_8 &h_9 &h_{10} \\
 \end{bmatrix} $

\begin{itemize}
\item We can, express, e.g. $u_3 = \frac{h_8 + u_2 + u_4 + h_6}{4}$ and similar for other $u_1, u_2, u_4$
\item And multiply to $4u_3 = u_4 + u_2 + h_6+h_8$ and similar
\item Define $\vec{u} = \begin{pmatrix} u_1 \\ u_2 \\ u_3 \\ u_4 \\ \end{pmatrix}$,  $\vec{h} = \begin{pmatrix} h_{12}+ h_2 \\ h_5+h_3 \\ h_6+h_8 \\ h_{11}+h_9 \\ \end{pmatrix}$, write $A\vec{u} = \vec{h}$, with $A = 
\begin{pmatrix} 4 & -1 & 0 & -1 \\
-1 & 4 & -1 & 0 \\
0& -1 & 4 & -1 \\
-1 & 0 & -1 & 4 \\
\end{pmatrix}$
\item With boundary conditions in the unit square $u(x,0) = u(0,y) = 0, u(x,1) =3x, u(1,y) = 3y$ this lets us get the value of all the $h_i$ and plug into $\vec{h}$ to get $= \begin{pmatrix} 4 \\ 1 \\ 0 \\ 1\end{pmatrix}$
\item Find the matrix inverse and solve to get $\vec{u} = \frac{1}{3}\begin{pmatrix} 4 \\ 2 \\ 1 \\ 2\end{pmatrix}$
\item This suggests $u(\frac{2}{3}, \frac{2}{3}) \approx u_1 = \frac{4}{3}, u(\frac{1}{3}, \frac{2}{3}) \approx u_2 = \frac{2}{3}...$
\item From there, we hand wave to get exact solution $u(x,y) = 3xy$.  \emph{(Presumably, a computer interpolates this? )}
\item Without an exact solution, you can shrink $h$ and solve bigger and bigger matrix problems.
\end{itemize}


Finite differences work well for a square $D$, though it can be extended to rectangles and disks.  But for irregular shapes, we use Rayleigh-Ritz technique.

\textbf{Example for Rayleigh-Ritz technique}.  
\begin{itemize}
\item Setup, circle interior: $\nabla^2 u = \frac{\delta ^2 u}{\delta x^2} +  \frac{\delta ^2 u}{\delta y^2}  = 0, x^2 + y^2 < 1$.  ``u is harmonic inside disk D''
\item Setup, circle boundary: $u(x,y) = x^2, x^2 + y^2 = 1$.  ``u equals $x^2$ on $\delta D$''
\item \emph{Main idea}: We want to find the $u$ that minimzes $C[u] = \frac{1}{2} \iint_D [\nabla u]^2 dx dy$ while maintaining $u=x^2$ on $\delta D$.  
\item Restated: Among set of $D$-situated differentiate functions $F$, find $u \in F$ that has the smallest cost.
\item Tactic: Pick a parametrized function that fits, and tweak the parameters to minimize cost.
\item So, guess $u$ looks like $v(x,y) = x^2 + a(x^2 + y^2-1)$.
\item Then $C[v] = C[x^2 + a(x^2 + y^2-1)] = \frac{1}{2} \iint_D[x^2 + a(x^2 + y^2-1)]^2dxdy$
\item Call the integrand $[\nabla v]^2$.  Since $\nabla v = \langle 2x(1+a), 2ya \rangle, [\nabla v]^2 = \nabla v \cdot \nabla v = 4x^2(1+a) + 4y^2a^2$
\item To integrate over the circle $D = \{x^2+y^2 \leq 1\}$, switch to polar $x=r\cos(\theta), y=r\sin(\theta)$, and churn through to get $C[v] = \frac{\pi}{2}((1+a)^2+a^2)$  This is minimized at $a=-\frac{1}{2}$
\item Checking $v(x,y) = x^2 -\frac{1}{2}(x^2 + y^2 - 1)$, we see that:
\begin{itemize}
\item $\nabla^2 v = 2 + -2 = 0$ everywhere.
\item $x^2+y^2 = 1 \Rightarrow v = x^2$
\item Pretend we know $u$ and minimize $\frac{1}{2}\iint_D [\nabla (v-u)]^2dxdy$:
\item $=\frac{1}{2}\iint_D[\nabla (v-u)] \cdot [\nabla (v-u)] dx dy$
\item With product rule $\nabla(f\nabla g)-f\nabla^2g = \nabla f \cdot \nabla g$
\item And divergence rule $\iint_D \nabla (f\nabla g) = \int_{\delta D} f\nabla g \cdot \hat{n} dl$
\item We get $= \frac{1}{2}  \int_{\delta D} (v-u)\nabla (v-u) \cdot \hat{n} dl - \frac{1}{2} \iint_D \nabla (v-u)\nabla ^2(v-u) $
\item Since $v-u = 0$ on $\delta D$, the first term is 0.
\item Since $\nabla ^2 (v-u) = \nabla ^2 v - \nabla ^2 u = 0$ inside the disk, the second term is zero.
\end{itemize}
\item Therefore the cost is zero, which can only be true if $v=u$!
\end{itemize}


\section{Chapter 4 - Transform methods}

\subsection{Fourier Transforms}

Main idea: 
\begin{itemize}
\item We use identity $e^{int} = i \sin(nt) + \cos(nt)$.
\item We have a signal composed of amplitudes $c_n$ at frequencies $n$: $s(t) = \sum_{n = -\infty}^{n = \infty}a_n i \sin(nt) + b_n\cos(nt) = \sum_{n = 0}^{n = \infty}c_n e^{int}$ for a $c_n$ composed of $a_n, b_n$ (the $e$-based format is equivalent but easier to integrate).  This coefficients of $\sin, \cos$ are called a \textbf{Fourier series}
\item \textbf{Fourier's trick}: If you multiply signal $s(t) = \sum_{n=-\infty}^{\infty} c_ne^{int}$ by a particular $e^{-imt}$ and integrate, you extract $c_m$ since $c_n = \frac{1}{2\pi}\int_{t=0}^{t=2\pi} s(t)e^{-imt}dt = \int_{t=0}^{t=2\pi} e^{i(n-m)t}   = 1$ if $m = n$ and $0$ otherwise (since  $e^{int} = i \sin(nt) + \cos(nt)$ and those each integrate to 0 over a full period).  
\item This means we can extract frequencies $c_n$ or (``source''?)  $\hat{s}_n$ as $\hat{s}_n(t)  = \frac{1}{2\pi} \int_{t=0}^{t=2\pi} s(t)e^{-int}$, with $s(t) = \sum_{n=-\infty}^{\infty}\hat{s_n}e^{int}$ as the full  signal.
\item And since $n$ gives us integer frequencies, our \textbf{Fourier Transform} says ``given a signal, integrate over the whole time period to get the source amplitude at a given frequency'': $\hat{s}(\omega) = \frac{1}{2\pi} \int_{t=-\infty}^{t=\infty} s(t)e^{-iwt}dt$.  TODO: Why did we switch from $t \in [0,2\pi]$ to $t \in \mathbb{R}$ here?  \emph{Note: We only use $\mathbb{R}$ as our domain from here on out, not $t \in [0,2\pi]$.}
\item The inverse is just the definition of $s(t)  = \sum_{n=-\infty}^{\infty}c_ne^{int}$ expressed over a continuum of frequencies: $s(t) = \int_{w=-\infty}^{\infty} \hat{s}(\omega)e^{iwt}d\omega$: ``given a source set of amplitudes, integrate over the whole frequency spectrum to get our result signal''
\end{itemize}

Example: Contribution of amplitude, frequency to signal strength
\begin{itemize}
\item If we're looking for the strength of a signal based on $\sin$ or $\cos$, integrating over $2\pi$ doesn't do since that would be zero.
\item Therefore, we use ``root mean square average strength'' measure \fbox{$s_{rms}= \sqrt{\frac{1}{2\pi}\int_{t=0}^{t=2\pi}  [s(t)]^2dt}$}
\begin{itemize}
\item Say there's a signal $s(t) = 2\sin(3t) - \cos(t)$.  Which part contributes more to the strength?
\item Temporarily relabel amplitudes as $w_1, w_2$ so $s(t) = \sqrt{\frac{1}{2\pi}\int_{t=0}^{t=2\pi} [w_1\sin(3t) - w_2\cos(t)]^2dt}$. 
\item Rewrite using identities $\sin(at) = \frac{e^{iat} - e^{-iat}}{2i}, \cos(at) = \frac{e^{iat}+ e^{-iat}}{2}$
\item ...(churn through) Everything ends up being paired up into sines and cosine terms (eliminated over 0 to $2\pi$ integral) except $(w_1\frac{-2}{2i})^2 + (w_2\frac{2}{2})^2$, and we end up with $s_{rms} = \sqrt{\frac{1}{2}(w_1^2+w_2^2)}$
\item Takeaway: \emph{Only amplitude contributes to signal strength, not frequency}
\end{itemize}
\item So signal stregth of $s(t)$ is $ \sqrt{\frac{1}{2\pi}\int_{t=0}^{t=2\pi}  [s(t)]^2dt} = \sqrt{\frac{1}{2\pi}\int_{t=0}^{t=2\pi} (\sum_{m=-\infty}^{\infty} c_m)(\sum_{k=-\infty}^{\infty} c_k)} = \sqrt{\sum_{k=-\infty}^{\infty} c_nc_{-n}}$ due to integral being zero for $e^{i(m+k)t}, m+k \neq 0$ terms
\end{itemize}

Using this to actually in solving PDEs: apparently for one example, we can use the Fourier transform of a function, find out its solution, then transform that back to the original domain.  (Does this work just some of the time?)
\begin{itemize}
\item Setup: The wave equation $u_{tt} = \nabla^2 u$ actually does apply to (I suppose?) an infinitely long string over $x$.
\item Other parameters: Intial shape $u(x,0) = g(x)$ on a still $u_t(x,0)= 0$ wire that tapers to zero on either end: $lim_{x \rightarrow \pm \infty} g(x) = 0 $.
\item (Note that this only looks like Fourier if $x$ is like time.)
\item $\hat{u}(\omega,t) = \frac{1}{2\pi} \int_{\mathbb{R}} u(x,t)e^{-i\omega x}dx$
\item To find out what $\widehat{u_{tt}}$ is, first consider $\frac{\delta ^2 }{\delta t^2} \hat{u} =  \frac{1}{2\pi} \int_{\mathbb{R}} u_{tt}(x,t)e^{-i\omega x}dx$. (Note: This also equals $\hat{u}_{tt}$, the transform of $u_{tt}$, since the variable $t$ isn't used elsewhere.)
\item And (the trick!) $\widehat{u_{tt}} = \widehat{u_{xx}}$  by $u_{tt} = u_{xx}$, so this $= \frac{1}{2\pi} \int_{\mathbb{R}} u_{xx}(x,t)e^{-i\omega x}dx$
\item And by integration by parts, with $U = e^{-i\omega x}, V = u_{xx}$, we have 
	$= \frac{1}{2\pi}[  e^{-i\omega x}u_x|_{-\infty}^{\infty} - (-i\omega)   \int_{\mathbb{R}} u_{x}(x,t)e^{-i\omega x}dx]$.  The first term is zero from the  boundary conditions ( I guess if $u \rightarrow 0$ then $u_x \rightarrow 0$ too?  Did I miss something?)
\item Repeating the process on the second term, we get $\widehat{u}_{tt} = \frac{1}{2\pi}  \omega^2 \int_{\mathbb{R}} (x,t)e^{-i\omega x}dx$, which suggests $\frac{\delta ^2 \hat{u}}{\delta t^2} = -\omega^2 \hat{u}$
\item WIth the boundary conditions $\hat{u}(\omega, 0) = \hat{g}(\omega)$ (the transform of the initial $g(x)$ state on $u$, and similarly $\hat{u}(\omega, t) = 0$ implies a solution of $\hat{u}(\omega, t) = \hat{g}(\omega)\cos(|\omega|t)$.  TODO: Why the absolute value? 
\end{itemize}

So, we reduced a second-order PDE with two variables ($t, x$) to an ordinary one with one variable $\omega$.
%Mostly, we use Fourier transforms to find the coefficients $\hat{s}(\omega)$ from the signal $s(t)$ instead of the other way around.
The wave solution then has a Fourier transform $\hat{u}(\omega, t) = \hat{g}(\omega) \cos(|\omega |t)$.  With the initial condition set as, say, $g(x) = 2\pi u_0e^{-\frac{x^2}{2}}$, we can transform back.
\begin{itemize}
\item With the identity $\int_{\mathbb{R}} e^{-ax^2} = \sqrt{\frac{\pi}{a}}$, and knowing $\hat{g}(\omega)  = \int_{\mathbb{R}} g(x)e^{-i\omega x}dx,$, we can complete the square to $\hat{g}(\omega) = u_0\int_{\mathbb{R}} e^{-\frac{1}{2}(x+i\omega)^2-\frac{1}{2}\omega^2} dx$
\item We can sub $y = x+i\omega$ and use our identity well enough, to get $\hat{g}(\omega) = u_0\sqrt{2\pi}e^{-\frac{\omega^2}{2}}$
\item Then, since $\hat{u}(\omega, t) = u_0\sqrt{2\pi}e^{-\frac{\omega^2}{2}} \cos(|\omega |t)$, we can use $u(x,t) = \int_{\mathbb{R}} \hat{u}(\omega, t)e^{ix\omega}d\omega$, knowing $\cos(|\omega|t) = \frac{e^{i|\omega|t} + e^{-|\omega|t}}{2}$
\item After splitting the integrals over the positive and negative domains for the absolute values and completing the square, we get $\pi u_0[e^{-\frac{(x+t)^2}{2}} + e^{-\frac{(x-t)^2}{2}}]$
\item Apparently this is the same solution as with d'Alembert's formula (the two sensors going out from x at time 0 and querying at t): $u(x,t) = \frac{1}{2}[g(x+t) - g(x-t)]$, since $g(x) = 2\pi u_0e^{-\frac{x^2}{2}}$.
\end{itemize}


\subsection{Fourier and the Heat Equation}
Main ideas:

\begin{itemize}
\item The last section was solving $u_{tt} = u_{xx}$, the 1D wave equation.
\item This section solves $u_{t} = \nabla^2 u$, the heat equation of on a 1D rod, with initial temp distribution $g(x)$.
\item Solution to this in a previous section,  assuming a Dirac heat spike at $x=0$ was $C\exp\{-\frac{x^2}{\sigma^2 t}\}$
\item Main tactic again is to take a PDE, Fourier transform into a simpler ODE, solve, and transform back.
\item Like the radio frequency waves of the previous chapter, these Fourier transforms go over all reals $x \in \mathbb{R}$.  So we can't do it over time ($t \geq 0)$ but can over $x$.  
\end{itemize}

OK, let's get to it.  How do the derivatives of the transformed function relate to the transform of the function's derivatives?
\begin{itemize}
\item Main Fourier transform from signal domain to frequency domain, just like before: $\hat{u}(\omega, t) = \frac{1}{2\pi} \int_{x = -\infty}^{x = \infty} u(x,t)e^{-i\omega x} dx$.  
\item Then $\widehat{u_x}(\omega, t) = \frac{1}{2\pi} \int_{x = -\infty}^{x = \infty} u_x(x,t)e^{-i\omega x} dx = \frac{1}{2\pi} u(x,t) e^{-i \omega x}|_{x = -\infty}^{x = \infty} - (-i\omega) \int_{x = -\infty}^{x = \infty} u(x,y)e^{-i \omega x} dx$ by integration by parts.  Assuming $u \rightarrow 0$ when approaching either infinity, the first term is zero, and the second is just $\hat{u}$, so:
\\
 \fbox{taking a FT of a derivative is just $(i\omega)$ times the FT of the original: $\widehat{u_x} = (i\omega)\hat{u}$}
 \item This can be repeated easily, e.g. $\widehat{u_{xx}} = -\omega^2 \hat{u}$, or $\widehat{\frac{\delta^n u}{\delta x^n}}(\omega, t) = (i\omega)^n\hat{u}(\omega,t)$
\end{itemize}

Solving our Heat Equation

\begin{itemize}
 \item A-HA: And since, in this Laplace setup,  $u_{t} = \nabla^2 u \equiv u_{xx}$, then $\widehat{u_t} = -\omega^2 \hat{u}$ since transforms preserve equality \emph{(TODO: did we prove they were 1-1 before?) I guess if inv(trans(a)) = a, then they must be...}
\item Boundary: $u(x,0) = g(x) \Rightarrow \hat{u}(\omega, 0) = \hat{g}(\omega)$.
\item $\widehat{u_t} = \frac{d \hat{u}}{dt} =  - \omega^2 \hat{u} \Rightarrow \hat{u} = Ce^{-\omega^2 t}$
\item $\hat{u}(\omega,t=0) = \hat{g}(\omega) \Rightarrow \hat{u} = \hat{g}(\omega)e^{-\omega^2 t}$
\item We then need to ``undo'' the transform to get our actual $u$ in the signal domain.  We need to know $g$ to do this.
\item Assume $g$ is a Dirac Delta at a: $g(x) = \delta_a(x)$.
\item Then, by the nature of the delta function, then $\hat{g}(\omega)= \frac{1}{2\pi}\int_{x = -\infty}^{x = \infty}  \delta_a(x)e^{-i\omega x}dx = \frac{1}{2\pi}e^{-i\omega a}$, just picking out the one value where the integral is nonzero, and $dx = 1$ for this infinitesimal slice, basically.
\item So to reverse $\hat{u} = \hat{g}(\omega)e^{-\omega^2 t}$, we transform back $ \int_{x = -\infty}^{x = \infty}  (\frac{1}{2\pi}e^{-i\omega a})e^{-\omega^2 t} e^{i\omega x} d\omega$ 
\item Complete the square so the exponent is $-t(\omega + \frac{[x-a]}{2ti})^2-(\frac{[x-a]^2}{4t})$
\item Use the identity $\int_{k = -\infty}^{k = \infty} e^{-ak^2} = \sqrt{\frac{\pi}{a}}$ with $k =  (\omega + \frac{[x-a]}{2ti})$ to get $u(x,t) = \sqrt{\frac{1}{4\pi t}} e^-\frac{[x-a]^2}{4t}$
\end{itemize}

This $F(x,t;a) =  \sqrt{\frac{1}{4\pi t}}  [e^-\frac{[x-a]^2}{4t}]$ turns out to be a \emph{fundamental solution}, in that if we have any initial ``data'' $g(x) = u(x,0)$, then we can sum over all points $a$ where the unit of heat is found: 
\\
More concretely, $u(x,t) = \int_{a=-\infty}^{a=\infty} F(x,t;a)u(a,0)da = \sqrt{\frac{1}{4\pi t}} \int_{a = -\infty}^{a = \infty}  [e^-\frac{[x-a]^2}{4t}u(a,0)]da$ says

``Set $a$ as the point where all the heat is concentrated, in amount $u(a,0)$.  This is the initial condition for the fundamental solution.  Integrate over all of these''.
\\
This (integral sum) obeys $u_t = u_{xx}$ since F does! 
\begin{itemize}
\item  Note: this requires that $\lim_{t \rightarrow 0^+} \sqrt{\frac{1}{4\pi t}} \int_{a = -\infty}^{a = \infty}  [e^-\frac{[x-a]^2}{4t}u(a,0)]da = u(x,0)$
\end{itemize}

Example:
\begin{itemize}
\item If we set $u(x,0) = e^{-\beta x^2}$, then we have to simply evaluate 
$\sqrt{\frac{1}{4\pi t}} 
\int_{a=-\infty}^{a=\infty}u(a,0)e^{-\frac{[x-a]^2}{4t}}da 
= \sqrt{\frac{1}{4\pi t}}
\int_{a=-\infty}^{a=\infty}e^{-\frac{[x-a]^2}{4t}-\beta a^2} da$
\item This requires completing the square, with steps $-\frac{1}{4t}[(1+4\beta t)a^2-2ax+x^2]$ and eventually $-\frac{1+4\beta t}{4t}[a-\frac{x}{1+4\beta t}]^2 - \frac{\beta x^2}{1+4\beta t}$
\item Applying the shift and the lemma from above, we get $u(x,t) = \sqrt{\frac{1}{1+4\beta t}}e^{-\frac{\beta x^2}{1+4 \beta t}}$
\item APPARENTLY (unverified) this solves $u_t = u_{xx}$ (lots of bad looking derivatives), and $u(x,0) = e^{-\beta x^2}$ (L' Hopital's Rule?
\end{itemize}

\subsection{Practice: Fourier and Laplace}

Usually, doing the FT is not hard.  The inverse is hard.
//
Theme of this section is the Laplace equation: $\nabla^2 u  = u_{xx} +   u_{yy}  = 0$

Setup: Heat equation on the positive $y$ half-plane
\begin{itemize}
\item Some $g(x)$ heat distribution on the x-axis
\item Note: There's not a unique solution to $u_{xx} + u_{yy} = 0$ in general.
\begin{itemize}
\item Suppose there is a solution $f(x,y)$.  Then, since this is linear, if there's a solution $u$ with $u(x, 0) = 0$, then the solution $f + u$ will hold.
\item If $u''(y) = 0$, then $u = ay + b$ has $u_{yy} = 0$ so $u + f$ is a solution.
\item However, \emph{if u is bounded (that is, $u(x,y) < B$ for all $x,y$)}, then there is a unique solution (why?)
\end{itemize}
\item If $u_{xx} + u_{yy} = 0, u(x,0) =g(x)$, and $u, u_x \rightarrow 0$ as $x \rightarrow \pm \infty$, then we can solve the FT $\hat{u}(\omega, dy) = \frac{1}{2\pi} \int_{x=-\infty}^{x=\infty} u(x,y)e^{-i\omega x}dx$
\begin{itemize}
\item $\widehat{{u}_{yy} } = \hat{u}_{yy}  = \frac{1}{2\pi} \int_{x=-\infty}^{x=\infty} u_{yy}e^{-i\omega x} dw$
\item $\hat{u}_{yy} = -\frac{1}{2\pi} \int_{x=-\infty}^{x=\infty} u_{xx}e^{-i\omega x} dw$ by $u_{xx} + u_{yy} = 0$
\item IBP: $dV = u_{xx}, U = e^{-i\omega x}$:  $= -\frac{1}{2\pi}[e^{-i\omega x} u_x|_{-\infty}^{\infty} + (i\omega)\int u_xe^{-i\omega x}]$
\item = $-\frac{1}{2\pi}[0 + -i\omega(e^{-i\omega x} u)|_{-\infty}^{\infty} + (i\omega)^2\int ue^{-i\omega x}]$
\item So $\frac{d^2}{dy^2} \hat{u} = \omega^2 \hat{u}$
\item And if we assume a solution of form $\hat{u}(\omega,y) = e^{ry}$, then $\frac{d\hat{u}}{dy^2} = r^2e^{ry} = \omega^2e^{ry} \Rightarrow r =\pm |\omega|$
\item So, we have combinations of two possible solutions: $A(\omega)e^{|\omega|y} + B(\omega)e^{-|\omega|y} $
\item However, if $u$ is bounded as $y \rightarrow \infty$, then $A(\omega) = 0$ by necessity.
\end{itemize}

To recap for the purpose of solving:
\begin{itemize}
\item $\hat{g}(\omega) = \frac{1}{2\pi} \int_{x=-\infty}^{x=\infty} g(x)e^{-i\omega x}$ by general FT.
\item $\hat{u}(\omega, y) = \hat{g}(\omega)e^{-|\omega|y}$ by the solution above.
\item $u(x,y) = \int_{\omega=-\infty}^{\omega=\infty} \hat{u}(\omega,y) e^{i\omega x} d\omega$ by general inverse FT.
\item So $u(x,y) = \int_{s=-\infty}^{s=\infty} [\hat{g}(\omega)e^{-|\omega|y} ]e^{i\omega x}d\omega$ by substitution of $u(x,y)$
\item And $u(x,y) = \int_{\omega=-\infty}^{\omega=\infty}  (\frac{1}{2\pi} \int_{s=-\infty}^{s=\infty} g(s)e^{-i\omega s}) e^{-|\omega|y}e^{i\omega x}d\omega = \frac{1}{2\pi}\int g(s)[\int e^{-i\omega s - |\omega| y + i\omega x} d\omega]ds$ (So there's no need to find $\hat{g}(\omega)$ explicitly.)
\item The innermost integral evaluates to $\frac{2y}{(x-s)^2 + y^2}$, so we're solving 
\\
$u(x,y) = \frac{1}{\pi} \int_{s=-\infty}^{s=\infty} g(s)\frac{y}{(x-s)^2 + y^2} ds, y > 0$, which obeys $\nabla^2 u = 0$ on the half-plane.
\item We also need $\lim_{y\rightarrow0^+}u(x,y) = g(x)$ for all real $x$, which,since $g(s) \approx g(x) = g(x)(s-x)+\frac{1}{2}g''(x)(s-x)^2...$, reduces to understanding what happens to $\int_{u=-\epsilon}^{u=\epsilon} \frac{u^n}{u^2+y^2}du$ for $u = s-x$.  $n \geq 2$ has no singularities, so this is all about solving $n \in \{0, 1\}$, which ends up evaluating to $g(x)$
\end{itemize}
\item To solve $u(x,y) = \frac{1}{\pi} \int_{s=-\infty}^{s=\infty} g(s) = \frac{y}{(x-s)^2 + y^2} ds$,
 with $g(s) = 1+\cos(x)$, using $cos(A+B) = \cos(A)\cos(B) - \sin(A)\sin(B)$, we end up with $u = 1 + \cos(x)e^{-y}$ after expanding and using some integral identities.
\end{itemize}

Note that for Laplace solutions, the maximum value is achieved on the boundary, at $y= 0$, where $e^{-y}$ is maximized.

\subsection{Challenge: Fourier and 3D Waves}

We generalize from the 1D Fourier wave equations (relating $x$ and some $\omega$ thing) (TODO Read  Fourier Transforms: https://blog.endaq.com/fourier-transform-basics) to $n-dimensional$ in trying to solve 3D (or n-D) Laplace: $u_{tt} = \nabla^2 u$.

An example would be compression waves (density at 3D point $\vec{x}$).

The core equations then become:
\begin{itemize}
\item $s(\vec{x}) = \int_{\vec{\omega} \in \mathbb{R}^n} \hat{s}(\vec{\omega}) \exp \{ i\vec{\omega} \cdot \vec{x}\} d\vec{\omega}$
\item $\hat{s}(\vec{\omega}) = \frac{1}{(2\pi)^n}\int_{\vec{x} \in \mathbb{R}^n} s(\vec{x})\exp\{-i\vec{\omega} \cdot \vec{x} \} d\vec{x}$
\end{itemize}

First, develop the differential equation on the other side of the transform (meaning, of $\hat{u}$):
\begin{itemize}
\item Assume: $u(\vec{x}, 0) = g(\vec{x})$ goes to 0 at infinity,  and $u_t(\vec{x}, 0) = 0$.
\item This means we can use the divergence theorem, I guess since the balls near infinity are all zero flux?  $\int_{\vec{x} \in \mathbb{R}^3} \nabla \cdot \vec{u}\ d\vec{x} = 0 \Leftrightarrow \lim_{\vec{x}\rightarrow \infty} \vec{u}(\vec{x}) = \vec{0}$
\item Also, we have an (actually obvious) identity in hand that helps: $\nabla \cdot [f\nabla h - h\nabla f] = f\nabla^2h - h\nabla^2 f]$
\item And, remember that $u_tt = \nabla^2u$ in our setup.
\item $\frac{d^2 \hat{u}}{dt^2} = \widehat{u_{tt}} =  \frac{1}{(2\pi)^3}\int_{\vec{x} \in \mathbb{R}^n} u_{tt} \exp\{-i\vec{\omega} \cdot \vec{x} \} d\vec{x}$
\item This equals $\frac{1}{(2\pi)^3}\int_{\vec{x} \in \mathbb{R}^n} \nabla^2u \exp\{-i\vec{\omega} \cdot \vec{x} \} d\vec{x}$ by the Laplace setup.
\item Set $f = \exp\{-i\vec{\omega}\vec{x} \}, h = u$, so $\nabla\cdot[e^{-i\vec{\omega}\vec{x}} \nabla u - u\nabla e^{-i\vec{\omega}\vec{x}}] = e^{-i\vec{\omega}\vec{x}} \nabla^2u - u\nabla^2 e^{-i\vec{\omega}\vec{x}} ]$
\item This means $ \frac{1}{(2\pi)^3}\int_{\vec{x} \in \mathbb{R}^n} [ \nabla^2u e^{-i\vec{\omega}\vec{x}} d\vec{x} - u\nabla^2 e^{-i\vec{\omega}\vec{x}}]  + u\nabla^2 e^{-i\vec{\omega}\vec{x}}$, and the bracketed part is $\nabla\cdot[e^{-i\vec{\omega}\vec{x}} \nabla u - u\nabla e^{-i\vec{\omega}\vec{x}}]$ by the last line.  This will be zero as $\vec{x} \rightarrow \infty$ by Divergence theorem.
\item This also means, in this particular setup, that $\int_{\vec{x} \in \mathbb{R}^n} (\nabla^2u) e^{-i\vec{\omega} \cdot \vec{x}}d\vec{x} = \int_{\vec{x} \in \mathbb{R}^n} u \nabla^2 e^{-i\vec{\omega} \cdot \vec{x} } d\vec{x}$
\item Since $\nabla^2  e^{-i\vec{\omega} \cdot \vec{x}} = (-i\vec{\omega} )^2 = -\|\vec{\omega}\|^2$, we get \fbox{$\frac{d^2\hat{u}}{dt^2} = -\|\vec{\omega}\|^2 \hat{u}$ as the diff eq on the omega side.}
\item Then \fbox{$u = \int_{\mathbb{R}^3}\hat{g}\cos(\|\omega\|t)e^{i\vec{\omega}\cdot \vec{x}}d\vec{\omega}$ is the solution} since anything of the form $A(\omega)e^{i\vec{\omega}\cdot \vec{x}} + B(\omega)e^{-i\vec{\omega}\cdot \vec{x}}$ is also expressable as $C(\omega)\cos(\|\omega\|t) + D(\omega)\sin(\|\omega\|t)$ and our inital conditions force $C$ to be $g$ and thus $D$ to be zero.
\end{itemize}

If we have an initial density distribution $g$, we can  transform it to the omega domain. 
Here, \fbox{$g(x,y,z) = u_0 \exp\{-\frac{1}{2}(x^2+y^2+z^2)\}$ is given}
so $\hat{g}(\vec{\omega}) = \frac{1}{(2\pi)^n}\int_{\vec{x} \in \mathbb{R}^n} g(\vec{x})\exp\{-i\vec{\omega} \cdot \vec{x} \} d\vec{x}$

\begin{itemize}
\item We're given identity $\int_{x \in \mathbb{R}^n} \exp\{-a\|x\|^2\} dx = (\frac{\pi}{a})^{\frac{n}{2}}$
\item Recognize $\|\vec{\omega}\|^2$ in the exponent of $g$: $e^{-\frac{1}{2} \|\vec{x}\|^2}$
\item Looks like we're completing squares again.  The exponent of the product becomes $-\frac{1}{2}(x+i\omega)^2 - \frac{1}{2}  \|\vec{\omega}\|^2$
\item Set $q = x+iw, dq = dx$ and with the identity in hand, the integral becomes $\frac{u_0}{(2\pi)^3}e^{-\frac{1}{2}\|\vec{\omega}\|^2} \int e^{-\frac{1}{2}q^2} dq =$ \fbox{$\frac{u_0}{(2\pi)^{\frac{3}{2}}} e^{-\frac{1}{2}\|\vec{\omega}\|^2} = \hat{g}(\omega)$}
\end{itemize}

Putting $\hat{g}(\omega)$ into the inverse transform, we have $u(x,y,z,t) = \int_{\vec{\omega}\in\mathbb{R}^3} \hat{g}(\vec{\omega})\cos(\|\vec{\omega}\|t)e^{i\vec{\omega} \cdot \vec{x}} d\vec{\omega}$  

So our inverse FT ends up with the usual pattern - integral of:  $\hat{g}$ times ``$\hat{u}$ diff eq solution'' times $e^{i\omega x}$

 Combining $\hat{g}$ with $u$ equations gives us 
 \\
 
 $u = \frac{u_0}{(2\pi)^{\frac{3}{2}}}  \int_{\mathbb{R}^3}e^{-\frac{1}{2}\|\vec{\omega}\|^2}  \cos(\|\omega\|t) e^{i\vec{\omega}\cdot \vec{x}}d\vec{\omega}$
 
 
$\equiv  \frac{u_0}{(2\pi)^{\frac{3}{2}}}  \int_{\mathbb{R}^3}  \cos(\|\omega\|t) \exp\{-\frac{1}{2}\|\vec{\omega}\|^2 + i\|\vec{\omega}\| \|\vec{x}\|\cos(\phi)\}d\vec{\omega} $


Solving this is tricky and involves moving to spherical coordinates.

\begin{itemize}
\item Big trick: change $\vec{x} \cdot  \vec{\omega}$ to $\| \vec{\omega} \| \|\vec{x} \|\cos(\phi)$ and use spherical coordinates to define $\omega$.  
\begin{itemize}
\item $\omega$ still integrates over all $\mathbb{R}^3$ but with $\phi$ declination from $\vec{x}$ and $\theta$ ``around'' $\vec{x}$.  (Think of $\vec{x}$ pointing up like $z$).
\item TODO: I think the Jacobian implies $d\vec{\omega} = \| \vec{\omega}\|^2\sin(\phi)d\phi d\theta d(\|\vec{\omega}|)$
\item The whole thing is then $2\pi \int_{|\vec{\omega}\| = 0}^{|\vec{\omega}\| =\infty} \cos(\|\vec{\omega}\|t)\|\vec{\omega}\|^2 e^{-\frac{\|\omega\|^2}{2}} ( \int_{\phi = 0}^{\phi = \pi} \exp\{i \| \vec{x}\|  \| \vec{\omega}\|      \cos(\phi)\} \sin(\phi)d\phi)d(\|\vec{\omega}\|) $
\item Subbing $u = \cos(\phi)$, the inner integral is:
\begin{itemize}
\item $\int -\exp\{i \| \vec{x}\|  \| \vec{\omega}\| u\}du = -\frac{1}{i \| \vec{x}\|  \| \vec{\omega}\|} e^{i \| \vec{x}\|  \| \vec{\omega}\|      \cos(\phi)} |_{\phi = 0}^{\phi = \pi} = -\frac{1}{i \| \vec{x}\|  \| \vec{\omega}\|}[e^{-ixw}-e^{ixw}] = \frac{2\sin(\| \vec{x}\|  \| \vec{\omega}\|)} {\|\vec{x}\|  \| \vec{\omega}\| }$
\item So the whole intergral is now 
\\
$u(\vec{x}, t) = \frac{u_0}{\|\vec{x}\|} \sqrt{\frac{2}{\pi}}  \int_{\|\vec{\omega}\| = 0}^{\|\vec{\omega}\| =\infty}\cos(\|\vec{\omega}\|t)\|\vec{\omega}\|e^{-\frac{\|\vec{\omega}\|^2}{2}} \sin(\|\vec{x}\| \|\vec{\omega}\|)d(\| \vec{\omega}\|)$
\item Replace $\|\vec{\omega}\| = v$.  calculate $\cos(vt)\sin(\|\vec{x}\| v) = \frac{e^{vt}+e^{-vt}}{2}\frac{e^{\|\vec{x}\| v} - e^{-\|\vec{x}\| v}}{2i}$ to get $\frac{1}{2}[\sin(v\|x\|+t) + \sin(v\|x\|-t)]$
\item Now evaluate $\int_{v=0}^{v=\infty} ve^{-\frac{v^2}{2}} \sin(av)dv$
\\
$ = a\int_{v=0}^{v=\infty} e^{-\frac{v^2}{2}} \cos(av)dv$ (IBP) 
\\
$= \frac{a}{2} \int_{v=0}^{v=\infty} e^{-\frac{v^2}{2}} \cos(av)dv$ (cosine is even)
\\
$= \frac{a}{2} \int_{v=0}^{v=\infty} e^{-\frac{v^2}{2}} \frac{1}{2}(e^{av}+e^{-av}) dv$ (Euler cosine)
\\
= ... (Completing squares, $\int e^{-a^2}{2} = \sqrt{\frac{\pi}{a}}$ theorem) ... 
= $\sqrt{\frac{\pi}{2}}ae^{\frac{-a^2}{2}}$
\item Finally, we get \fbox{
$u(x,t) = \frac{u_0}{2\|\vec{x}\|} [(\|\vec{x}\| + t)\exp\{ -\frac{(\|\vec{x}\| + t)^2}{2} \}   +
(\|\vec{x}\|- t)\exp\{ -\frac{(\|\vec{x}\| - t)^2}{2} \}   ] $
}

\end{itemize}
\item We can confirm $u(x,0) = u_0\exp\{-\frac{\|x\|^2}{2}\}$ directly, and $u_t(x,0)=0$ by a TRICK: note that $u$ is symmetric in $t$, so $u_t(x,-t) = u_t(x,t)$, implying $u_t(x,0) = 0$!
\item To confirm $u_{tt} = \nabla^2u$, we define $h(z)=u_0ze^{-\frac{z^2}{2}}$, making $u = \frac{h(\|x\| + t) + h(\|x\| - t)}{2 \|x\|}$
\item Since $u(\vec{x},t)$ depends only on $\| x \| = r$, spherical coordinates' formula can be used: $\nabla^2 u = \frac{1}{r^2} \frac{\delta}{\delta r} [r^2 \frac{\delta u}{\delta r}]$
\item Remember $h$ is a one-dimensional function.
\item $u_t = \frac{h'(r+t) - h'(r-t)}{2r}, u_{tt} = \frac{h''(r+t) + h''(r-t)}{2r}$
\item $u_r = \frac{rh'(r+t) + rh'(r-t)-h(r+t)-h(r-t)}{2r^2}$
\item Don't go for $u_{rr}$!  Instead: $\frac{\delta}{\delta r}[r^2u_r] = \frac{r(h''(r+t)+h''(r-t))}{2}$
\item Multiplying by $\frac{1}{r^2}$ shows they are equal.
\end{itemize}
\end{itemize}

Results:
\begin{itemize} 
\item1D waves ($u(x,t) = \frac{g(x+t)+g(x-t)}{2})$
\item 3D (compression) waves ($u(\vec{x}, t)= \frac{u_0}{2\|x\|}[(\| x\| + t)\exp\{  \frac{(\|x\| + t)^2}{2} \}  + (\| x\| - t)\exp\{  \frac{(\|x\| - t)^2}{2} \}  ]$ 
\end{itemize}

Starting with bump $g(x) = u_0e^{-\frac{x^2}{2}}$ and similar in 3D, there are some similarities:
\begin{itemize}
\item Both are bounded between 0 and 1.  (Use L'Hopital's for the 3D case to be sure)
\item Both have disturbances travel at finite speed $t$
\end{itemize}

Differences:
\begin{itemize}
\item The 1D wave is a translation of the original function.  The 3D wave has $\|x\|$ in the denominator, so it flattens out.
\item The 1D wave with this setup is always positive.  The 3D wave can be negative (rarefied, in compression terms)!
\end{itemize}

\subsection{Schroedinger's Equation}

This uses the Fourier transform to make isomorphic a probability density over position (x) with that over v velocity (p), and shows that the other side of the FT can have intrinsic meaning.

Note: Much of this requires complex conjugation ($(a+bi)^* = a - bi$). 

\subsubsection{Complex Conjugation tips:}
\begin{itemize}
\item $(a+b)^* = a^*+b^*$.  Separate the real and imaginary and it's clear.
\item $(a \times b)^* = a^*\times b^*$.  Mulitply it out and it's clewar.
\item $(e^{it})* = (e^{-it})^*$.  Change $e^{it}$ to $\cos(t) + i\sin(t)$ and it falls right out.
\item $\|a + bi\|^2$, the squared ``length'' of tjhe complex number, is $a^2+b^2$ or $(a+bi)(a-bi) = (a+bi)(a+bi)^*$
\item $(f'(t))^* = ((f^*)'(t))$.  Substitute $f^*$ for $f$ in the limit definition and it's clear.
\item This makes sense since it looks like any linear transform (incl. derivative) of conjugation looks to be the conjugate of the transform.
\item FT $F(\omega)$ of $(f^*)(x)$ ends up being $F^*(-\omega)$.  
\end{itemize}

\subsubsection{Main ideas of Schroedinger's equation}


\begin{itemize}
\item Equation is $u_t = \frac{i}{2}\nabla^2 u$.
\item The ``signal'' ($u$) side sees $\|u(x,t)\|^2$ as probability particle is near $x$ at time $t$.
\item The transformed ``motion'' ($\hat{u}$) sees $\|\hat{u}(p,t)\|^2$ as probability particle's \emph{velocity vector} is near $\vec{pd}$ at time $t$.
\item This works because change in expected position (expected velocity) $\frac{dX}{dt}$ = $\iiint_p \vec{p}(2\pi)^3 |\hat{u}(p,t)|^2 d\vec{p}$ , where $X$ is $\iiint_x x \|u(x,t)\|^2$
\end{itemize}

Initial conditions $u(x,0) = g(x)$ require.
\begin {itemize}
\item $\iiint_{\mathbb{R}^3} |g(\vec{x})|^2 d\vec{x} = 1$, to be a legit probability distribution at time 0.
\item Knowing $u_t = \frac{i}{2}\nabla*2 u$ means $u_t^* = -\frac{i}{2}\nabla^2 u$. Then $\frac{d}{dt}[\iiint_{\mathbb{R}^3} |u|^2 d\vec{x}] = \iiint u_tu^* + uu_t^* = \frac{i}{2} \iiint u^*\nabla^2 u - u\nabla^2 u^* = \nabla \cdot ( u^*\nabla u - u\nabla u^* )$ (Note: This is an identity - take $\nabla(f\nabla g - g\nabla f)$ and see!)
\item So If $u, \nabla u \rightarrow 0$ go to zero near infinity, then we can use divergence theorem (since any ball would be zero on the surface).  Those are the three conditions for using $u(x,0) = g(x)$ as an initial distribution.
\end {itemize}

Step one: Solve the diff eq on the $\hat{u}$ side.
\begin {itemize}
\item $\hat{u}(p,t) = \frac{1}{(2\pi^3} \iiint_{\mathbb{R}^3} u(\vec{x}, t) \exp \{ -i\vec{p} \cdot \vec{x}\} d\vec{x}$ by standard FT.
\item Taking $\frac{d}{dt}$ of both sides and doing the $u_t = \frac{i}{2} \nabla^2 u$ substitution, cranking through $\nabla$s, yields \fbox{$\frac{d\hat{u}}{dt} = -\frac{i}{2} \|\vec{p}\|^2 \hat{u}(\vec{p}, t)$}, since the double derivative of the $\exp$ spits out $(-i \vec{p} \cdot -i \vec{p})$
\item So $\hat{u}(\vec{p}, t) = \hat{g}(\vec{p})e^{-\frac{i}{2}\|\vec{p}\|^2t}$
\end {itemize}

Aside: The Dirac Delta function $\delta$

\begin {itemize}
\item Definition : (Also works in $\mathbb{R}^3$: $\int_{x=-\infty}^ {x=\infty} f(x)\delta(x-a) = f(a)$
\item Fourier transform is $\hat{\delta}(\vec{p}) = \frac{1}{(2\pi)^3} \iiint_{\vec{x} \in \mathbb{R}^3} \delta(\vec{x} - \vec{a})\exp \{ -i\vec{p}\cdot\vec{x}\}d\vec{x} = \frac{1}{(2\pi)^3} e^{-i\vec{p}\cdot\vec{a}}$. Straightforward use of delta's main feature.
\item Inverse of the transform is the $\delta(x-a) = \frac{1}{(2\pi)^3} \iiint_{\vec{p} \in \mathbb{R}^3} \exp \{ -i\vec{p}\cdot(\vec{x} - \vec{a})\}d\vec{p}$  
\item This creates \textbf{Plancherel's Theorem}: $ \iiint_{\mathbb{R}^3} |g(x)|^2 = (2\pi)^3 \iiint_{\mathbb{R}^3} |\hat{g}(p)|^2$, since
\begin {itemize}
\item FT of $g$ is $\hat{g}(\vec{p}) = \frac{1}{(2\pi)^3} \iiint_{\vec{x} \in \mathbb{R}^3} g(p)exp \{ -i\vec{p}\cdot\vec{x}\}d\vec{x}$
\item $|g(x)|^2 = g(x)(g^(x))^*$
\item So $(2\pi)^3 \iiint_{\mathbb{R}^3} |\hat{g}(p)|^2 = \int (\int FT(g)) (\int FT(g))^*$
\item $(FT(g))^* = \int g^*e^{iyt}dy$, so $\int (\int g(x)g^*(y) (\int \exp\{ip \cdot (y-x)) = \int \int g(x)g^*(y)\delta(y-x)dydx = \int g(x)g^*(x)$ There were some $(2\pi)^3$s here too.
\item This shows that $(2\pi)^3\iiint \|\hat{u}(p,t)\|^2dp = 1$, and it's something of a probability density too.
\end {itemize}
\end {itemize}

Expected position is then $\vec{X}(t) = \iiint \vec{x} |u(x,t)|^2d\vec{x}$
\begin {itemize}
\item Used together with $u_t = \frac{i}{2}\nabla^2u$ 
\item and a divergence theorem (when $u$ and $\nabla u$ are 0 at inifity): $\iiint \vec{x}\nabla \cdot \vec{V}d\vec{x} = -\iiint \vec{V}\vec{dx}$
\item Gives us that $\frac{dX}{dt} = \frac{d}{dt} [\iiint \vec{x} |u(x,t)|^2d\vec{x}] = \iiint_p \vec{p}(2\pi)^3 |\hat{u}(p,t)|^2 d\vec{p}$, or that the change in expected position is the expected value of the velocity, and the transform, $\iiint_p \vec{p}(2\pi)^3 |\hat{u}(p,t)|^2 d\vec{p}$ measures the likelihood of finding the particle with \emph{velocity} near p at time t.
\end {itemize}

Example: 
If we start=with a known velocity distribution, $\hat{g}(p) = 
\frac{1}{(2\pi\sigma)^{\frac{3}{2}}}
\exp \{-\frac{\pi \|p - p_0\|^2}{2\sigma ^2} \}, \sigma << 1$ , whcih looks like a ``normal'' about point $p_0$ with some $\sigma^2$ variance:
\begin {itemize}
\item Glue it on to $\hat{u} = e^{-\frac{i}{2} \|p\|^2t}$
\item Calculate expected position $\hat{P} = \iiint_p p(2\pi)^3 |\hat{u}(p,t)|^2 dp$
\item Calculate variance $Var(\hat{p}) = \iiint_p (p-P)^2 (2\pi)^3 |\hat{u}(p,t)|^2 dp = \frac{3}{2}\frac{\sigma^2}{\pi^2}$ (requires some identities)
\item Transform $\hat{u}$ back to get Schroedinger wave function $u(x,t) = \frac{1}{(2\pi\sigma)^\frac{3}{2}} \iiint_p \exp \{-\frac{\pi \|p - p_0\|^2}{2\sigma ^2} - \frac{i}{2}\|p\|^2\}$
\item For this one, take $p_0 = \vec{0}, a(t) = \frac{\pi}{a^2} + it$
\item Complete the square to get $u(x,t) = \frac{1}{[\sigma a(t)]^{\frac{3}{2}}} \exp \{ -\frac{\|x\|^2}{2a(t)} \}$
\item The variance of the position is then $ \frac{3}{2}\frac{\sigma^2|a(t)|^2}{\pi^2}$ by our result before (replacing $\sigma$ with $\sigma a(t)$
\item And the product of the two variances $( \frac{3}{2}\frac{\sigma^2|a(t)|^2}{\pi^2})( \frac{3}{2}\frac{\sigma^2}{\pi^2})$ ends up as
 $\frac{9}{4}(1+\frac{\sigma^4}{\pi^2}t^2)$
 \item This is always positive and goes up with time.  Therefore, the \textbf{Heisenberg uncertainty principle} says the product of the variances in position and velocity is alwways positive.  
 \item Perhaps this says ``you can't know both position and velocity with surety at the same time''?
\end {itemize}

\subsection{Conformal Maps}

Visual: a conformal map is like flattening the Earth map out into a circle with a pole at the center.

Main ideas:

\begin {itemize}
\item Solving Laplace on a disk $D = \{(u,v): u^2+v^2 \leq 1 \}$ can't use Fourier transform, since $D$ has an edge.
\item Also, we can find $f(0,0)$ for any harmonic (mean-value) function with $f(0,0) = \frac{1}{2\pi} \int_{\delta D} g dl$, if $g$ is the boundary of the disk $D$.
\item So if we rearrange points using \textbf{conformal maps} (maps that preserve angles) and \textbf{stereographic projection} (move points in a figure along lines mapping to a single focus, which I think makes a conformal map) then we can reshape D and point $(x_0,y_0)$ to the center of a similar disk.
\item The process for this, starting from unit disk $D$:
\begin{itemize}
\item First, a translated disk lying parallel to  $y=0$,  axis, centered at $(0,-1, 1)$
\item Then, a hemisphere via a stereographic projection.
\item Then, the half plane $H$ via projecting through the north pole and the point to the plane
\item Then, the half plane to itself, to align our $(x_0, y_0)$ to something like $(0,2)$
\item Then, reversing all these to have a disk with our point at the center
\end{itemize}
\item We map from $(u,v)$ to $(x,y)$ like $(x(u,v), y(u,v))$.  
\item We have some function on each, representable as either $f(x(u,v), y(u,v))$ or $\tilde{f}(u,v)$

\end {itemize}

Main setup:
\begin {itemize}
\item We map from $(u,v)$ to $(x,y)$ like $(x(u,v), y(u,v))$.  (Note that this chapter has a lot of mapping to e.g. $(x,y,0)$ or $(x, 0, z)$, but that's still 2d-to-2d.)
\item We have some function on each, representable as either $f(x(u,v), y(u,v))$ or $\tilde{f}(u,v)$
\item We need to make sure that $f$ is harmonic ($f_{xx} + f_{yy} = 0$) implies that $\tilde{f}$ is too.
\item We can do this by ensuring $x_u = y_v$ and $x_v = -y_u$.  Why?
\begin {itemize}
\item First, if $\tilde{f}(u,v) = f(x(u,v), y(u,v))$, then $\nabla^2 \tilde{f} = \tilde{f_{uu}} + \tilde{f_{vv}}$.
\item Take the tricky two-step derivatives along $u$ \emph{through} $x$ and $y$: $\tilde{f}_u = (f_xx_u + f_yy_u)$, so $\tilde{f}_{uu} = \frac{d}{du} \tilde{f}_u =   \frac{d}{du}(f_xx_u )+  \frac{d}{du}(f_yy_u) =  (\frac{d}{du}f_x)x_u+ f_x(\frac{d}{du}x_u) +  (\frac{d}{du}f_y)y_u+ f_y(\frac{d}{du}y_u) = (f_{xx}x_u + f_{xy}y_u)x_u + f_xx_{uu} + (f_{yy}y_u + f_{yx}x_u)y_u + f_yy_{uu}$.
\item Similarly, $\tilde{f}_{vv} = (f_{xx}x_v + f_{xy}y_v)x_v + f_xx_{vv} + (f_{yy}y_v + f_{yx}x_v)y_v + f_yy_{vv}$.
\item So $\nabla^2 \tilde{f} = [x_u^2+x_v^2]f_{xx} + [y_u^2+y_v^2]f_{yy} + 2f_{xy}[x_uy_u + x_vy_v] + f_x[x_{uu} + x_{vv}] + f_y[y_{uu} + y_{vv}]$
\item If  $x_u = y_v$ and $x_v = -y_u$ and $f_{xx} + f_{yy} = 0$ then this is 
$([y_u^2+(-y_v)^2][f_{xx} + f_{yy}] + (2f_{xy}[y_vy_u + -y_uy_v]) + (f_x[y_{vu} + -y_{uv}]) + (f_y[-x_{vu} + x_{uv}]) = 0 + 0 + 0 + 0$.  
\item Notice also that $f$ is harmonic, since $x_{uu} + x_{vv} = y_{vu} - y_{uv} = 0 $, similar for $y_{uu} + y_{vv}$
\end {itemize}
\end {itemize}

An example harmonic function would be $\tilde{f} = (x(u,v), y(u,v)) = (e^u\cos(v), e^u\sin(v))$, where it can be easily validated that $x_u=y_v, x_v=-y_u$.  

You can prove the angles are the same before and after the map by:

\begin {itemize}
\item Starting with two curves $(u_1(t), v_1(t)), (u_2(t), v_2(t)) $  meeting in the plane at t=0, with their tangent vectors (pointwise derivatives $\vec{t_1}, \vec{t_2}$.
\item Defining $\cos(\theta)$ with the dot poduct formula.
\item Define $\cos(\theta')$ as meeting of  image $(x(u_1(t), v_1(t)), y(u_1(t), v_1(t))$ and its $u_2, v_2$ mate.
\item We can see these are equal if we build a matrix.  We see $\frac{d}{dt}|_{t=0} [x(u_1(t), v_1(t))] = x_uu_1'(0)+ x_vv_1'(0)$ and the like to a derivative matrix $\begin{pmatrix} x_u & x_v \\ y_u & y_v\end{pmatrix}\langle u_j'(0), v_j'(0)\rangle$  
\item Note that this matrix $A$ also equals $\begin{pmatrix} y_v & -y_u \\ y_u & y_v\end{pmatrix}$, and tehrefore $A^TA = I(y_u^2+y_v^2)$
\item After a bit more linear algebra, the $A-$transformed $\theta'$ in the dot product formula ends up equaling the original $\cos(\theta)$
\end {itemize}

So the derivative conditions mean we preserve angles under that kind of map.  
There's a simple (but not only?) way to make such a map by sliding any three points along lines with $t=0$ (starting point) to a common focus terminus at $t=1$.  The proof ends up being:
\begin{itemize}
\item Set each of three points $\vec{p_j}(t) = (1-t)\vec{p_j} + t\vec{c}$
\item Use the dot product formula $\cos(\theta(t)) = ...$ using vectors $\vec{p_2}(t) - \vec{p_0}(t)$ and $\vec{p_1}(t) - \vec{p_0}(t)$
\item Show that this equals $\cos(\theta(0))$, or the same setup with $\vec{p_2} - \vec{p_0}$ and $\vec{p_1} - \vec{p_0}$
\end {itemize}

The sequence of mappings from initial disk to elevated disk $D$ to unit hemisphere $S^+$ (sitting on origin, top at $(0, 0, 2)$ to half plane $H$ and back basically are just algebra.

Example: $H$ to $S^+: x^2 + y^2 + (z-1)^2 = 1$ 
\begin{itemize}
\item Line between $(x,y,0)$ $(0,0,2)$ is $t \langle x, y, 2(1-t)\rangle, t\in [0,1]$
\item The point touching $S^+$ satisfies $(tx)^2+(ty)^2+(2(1-t)-1)^2 = 1$
\item Simplify to find $t = \frac{4}{x^2+y^2+4}$, and substitute into line $t \langle x, y, 2(1-t)\rangle$
\item Thus $(x,y,z)$ on $S^+$ maps to $\frac{1}{x^2+y^2+4} \langle 4x, 4y, 2x^2+2y^2\rangle$
\item A similar stragegy gives us inverse $(\frac{2x}{2-z}, \frac{2y}{2-z}, 0)$ going from $S^+$ to $H$.
\end{itemize}

Idea for mapping from $S^+$ to a disk $(x,0,z): x^2+(z-1)^2 \leq 1$ is to draw a line from $(0, -1, 1)$ the sphere, and see where we hit the y-parallel disk centered at $(0, 0, 1)$.
\begin{itemize}
\item A similar strategy: Create the line parametrized by $t$, see where $t$ satisfies the intersection condition.
\item We end up with map from $S^+$ to disk as $(x,y,z) \rightarrow  (\frac{x}{y+1},y,\frac{z+y}{y+1}) $
\item The inverse, from a similar path, is $\frac{1}{x^2+ (z-1)^2 + 1}\langle 2x, 1-x^2-(z-1)^2, x^2+z^2 \rangle$
\end{itemize}

Combining all of these maps, including the map that elevates and rotates a unit disk to $(x, 0, z)$ in $D: (x=u, z=1+v)$, maps $(u,v)$ on the unit disk to $(x,y,z) = \frac{1}{u^2+(v-1)^2} \langle 4u, 2(1-u^2-v^2), 0\rangle$, with an inverse of $(u,v) = \frac{1}{x^2+(y+2)^2} \langle 4x, x^2+y^2-4\rangle$

One final map before we can go from unit disk to itself fully: moving $(x_0, y_0) \rightarrow (0,2)$ on $H$:
\begin{itemize}
\item Note: You can shift left/right without changing any angles.  (Can't shift along y, since 0 is the edge of the universe)
\item Note: You can scale without changing any angles.  Can probably confirm this very easily in cross product theorem, but it's clear.
\item So, shifting the coordinate over for x and scaling it for y yields (verifiably) conformal $(x,y) \rightarrow (\frac{2(x-x_0)}{y_0}, \frac{2y}{y_0})$
\end{itemize}

So if we wanted to find the value on the unit disk of some harmonic function $f$ of  $f(0, 0)$ with a known boundary condition $g(u,v) = u^2$, we could use the mean value property and average over the boundary.

Instead, if we wanted to know  $f(0, \frac{1}{2})$, we could: 
\begin{itemize}
\item Make a conformal transformation of $(0,\frac{1}{2})$ to $(0, 0)$
\item Know that that will become a similar subdisk in the original disk with the mean value property still holding.
\item Find the condition on the edge of that destination disk knowing the original $g$ on the first disk.
\item Integrate that around the edge of that destination disk.
\end{itemize}

Knowing the mapping $(\frac{3u}{u^2+(v+2)^2}, \frac{2u^2+2v^2+5v+2}{u^2+(v+2)^2})$ takes $(0,0)$ back to $(0, \frac{1}{2})$ means that we can integrate $r\cos^2(\theta) = [\frac{3u}{u^2+(v+2)^2}]^2$ around the disk to get $\frac{3}{8}$ on that disk, and thus the original.

\subsection{The Laplace Transform}

Main idea: Laplace is similar to Fourier (transform some PDEs into ODEs), but can handle a separate domain.

\begin{itemize}
\item Fourier transforms are from $x \in (-\infty, -\infty)$.  (Note: what about the $[0, 2\pi)$ ones from the first chapter?).  Laplace handles $x \in [0, \infty)$, better suited actually for time $t$!
\item Requirements:
\begin{itemize}
\item $\mathcal{L}[f](s)$ should be an integral, to handle the derivatives of diff eqs
\item Integration limits should be $0, +\infty$
\item $\mathcal{L}$ should be linear in $f(t)$.
\item Finally, we need a handy derivative mapping like Fourier's $\mathcal{F}[f'] = (i\omega) \mathcal{F}[f]$
\end{itemize}
\item Laplace transform of $f(t)$, operating on $s$, is $\mathcal{L}[f](s) = \int_{t=0}^{t=\infty} K(t;s)f(t)dt$.  \item $s$ is a variable like Fourier's $\omega$, and $K(t;s)$ is called the kernel.  
\item Suppose we want a derivative rule like :  $\mathcal{L}[\frac{df}{dt}](s)= \int_{t=0}^{t=\infty}\mathcal{L}K(t;s)f'(t)dt = s\mathcal{L}[f](s) + ...$
\item If $f(t), K(t;s) \rightarrow 0$ as $t \rightarrow \infty$, then selecting $K(t;s) = e^{-st}$ and therefore $\mathcal{L}[f](s) = \int_{t=0}^{t=\infty}e^{-st}f(t)dt$ satisfies all these. (note: do integration by parts with $u = e^{-rs}, v' = f$)
\item Churning the IBP gives you $\mathcal{L}[\frac{df}{dt}](s)=s\mathcal{L}[f](s) -f(0), \mathcal{L}[\frac{d^2f}{dt^2}](s)=s(\mathcal{L}[\frac{df}{dt}](s) -f'(0)) - f(0) = s(s\mathcal{L}[f](s) -f'(0)) - f(0)...$
\item So ultimately, our rule is \fbox{ $\mathcal{L}[\frac{d^nf}{dt^n}](s) = s^n\mathcal{L}[f](s) - \sum_{j=0}^{n-1}s^{n-j-1}f^{(j)}(0)  $}
\item This means we can get rid of all the derivatives on the transformed side and solve for the diff eq for $\mathcal{L}[f]$.  (We can also handle weirdos like $\delta$ below)
\end{itemize}

Major Example to solve: Impulse kicking an oscillating spring with force $\alpha$ at time $t_0$.  

\begin{itemize}
\item Diff Eq to solve: $x''(t) + x(t) = \alpha \delta(t-t_0)$, with Dirac $\delta$.
\item $\mathcal{L}[\alpha\delta_{t-t_0}](s) = \int_{t=0}^{t=\infty}e^{-st}\alpha\delta(t-t_0)dt =\alpha e^{-st_0}$
\item Taking $\mathcal{L}[x''(t) + x(t)] = \mathcal{L}[\alpha\delta(t-t_0)] = \alpha e^{-st_0}$, use the derivative rule to find the left hand side is $-x'(0) -sx(0) + s^2\mathcal{L}[x(t)](s)] + \mathcal{L}[x(t)](s)$.
\item Assuming $x(0) = x_0$ (stretched $x_0$ units) and $x'(0) = 0$ (at arest), then we see $\mathcal{L}[x(t)](s) = \frac{\alpha e^{-st_0} + sx_0}{s^2+1}$
\end{itemize}

Note: The hard part is reversing these BACK into the original domain.  The integral reverse won't be detailed here (it's a lot of hard ideas, apparently.). It turns out you \emph{generally look up the inverse Laplace transform in a table.}  

The Heaviside step function $u(x) = \{ 1, x>0; \frac{1}{2}, x=0; 0, x < 0 \}$ is included in the list, such that $\mathcal{L}[u(t-t_0]f(t-t_0) = e^{-st_0}F(s)$, and $\mathcal{L}[u(t)] = s^{-1}$ (Since $\int_{t=0}^{t=\infty} e^{-st}u(t)dt = \int_{t=0}^{t=\infty} e^{-st} = \frac{-1}{s}-e^{-st}|_{t=0}^{t=\infty} = [0 - \frac{-1}{s}] = s^{-1})$


With this table, we find that $\mathcal{L} = \alpha \mathcal{L}[u(t-t_0)\sin(t-t_0)](s) + x_0\mathcal{L}[\cos(t)](s) = \mathcal{L}[\alpha u(t-t_0)\sin(t-t_0) + x_0 \cos(t)](s)$, which obeys $x(0) = 0, x'(0) = 0$ for $t_0 > 0$.  This is a continuous but non-differentiable function, since the ``kick'' changes the derivative discontinuously.

\subsection{Laplace Transform applications}

Main motivation: RLC Circuit
\begin{itemize}
\item Original equation: $\frac{V_0(t)}{L} = Q''(t)+\frac{R}{L}Q'(t) + \frac{Q(t)}{LC}$.  
\item After nondimensionalization, we have our main equation to solve this section: $x''(t)+2\epsilon x'(t) + x(t) = v_0\sin(\omega t), \epsilon > 0$
\item Looking up in the table, we have $\mathcal{L}[v_0\sin(\omega t)] = \frac{\omega v_0}{s^2+\omega^2}$
\end{itemize}

Main tactic: Take Laplace transform of the side with the derivaties, setting up some $\mathcal{L}[x(t)] = f(s)$, do the same for the fixed side, and figure out $x(t)$ by triangulating in the transform table.
\begin{itemize}
\item General rule is $\mathcal{L}[x'(t)] = s\mathcal{L}[x(t)] - x(0)$.  Sub in $x''(t)$ or whatever for $x'(t)$ as needed.
\item $\mathcal{L}[x''(t)] = s\mathcal{L}[x'(t)] - x'(0)= s(s\mathcal{L}[x(t)] - x(0))- x'(0)$.  Can do the same for $x'(t)$.
\item Adding up, $\mathcal{L}[x''(t)+2\epsilon x'(t) + x(t)] = \mathcal{L}[x(t)](s^2+2\epsilon s + 1) =  \frac{\omega v_0}{s^2+\omega^2}$
\item Then $ \mathcal{L}[x(t)]=  \frac{\omega v_0}{s^2+\omega^2} \frac{1}{s^2+2\epsilon s + 1} $. Settting $\epsilon = 0$, we can get $\mathcal{L}[x(t)]=  \omega v_0\frac{1}{s^2+\omega^2} \frac{1}{s^2 + 1} $
\item Doing partial fractions gets us $= \frac{\omega v_0}{1-\omega^2} [\frac{1}{s^2+\omega^2} + \frac{1}{s^2+1}]$
\item Using the table to find $\mathcal{L}[s=\sin(at)](s) = \frac{a}{s^2+a^2}$, we can work forwards to get $ \frac{v_0}{1-\omega^2}\mathcal{L}[\sin(\omega t) - \omega \sin(t)](s) = \frac{v_0}{1-\omega^2} ( \frac{\omega}{s^2+ \omega^2} -  \frac{\omega}{s^2+ 1})$
\end{itemize}

Now, the derivative trick on $\frac{d}{ds}\mathcal{L}[x(t)]$.  Suppose we're looking for the solution to the above when $\omega = 1$ (and $\epsilon = 0$).  We end up with $\mathcal{L}[x(t)]=  \omega v_0\frac{1}{s^2+\omega^2} \frac{1}{s^2 + 1} =  v_0 \frac{1}{(s^2 + 1)^2}$

\begin{itemize}
\item We know $\mathcal{L}[\cos(t)](s) = \frac{s}{(s^2 + 1)}$.  Guess $\cos(t)$ as our $x(t)$ and take $\frac{d}{ds}$ of both sides.  
\item The right side becomes $- \frac{s^2-1}{(s^2 + 1)^2}$
\item The left side becomes $\frac{d}{ds}\int_{t=0}^{t=\infty} \cos(t)e^{-st}dt = \int_{t=0}^{t=\infty} \frac{d}{ds} e^{-st}\cos(t)dt = \int_{t=0}^{t=\infty} -te^{-st}\cos(t)dt = \mathcal{L}[-t\cos(t)]$
\item The Laplace transform of sine is $\frac{1}{s^2+1} = \frac{s^2+1}{(s^2+1)^2}$ 
\item Subtracting these two yields $x(t) = \frac{v_0}{2}[\sin(t) - t\cos(t)]$
\end{itemize}

Another example: Instead of the wall outlet, we have a battery-operated switch tso that $x''(t)+x(t) = v_0u(t-t_0)$ (Heaviside step function)


\begin{itemize}
\item The Laplace transform of the right side is $\mathcal{L}[u(t-t_0)](s) = \int u(t-t_0)e^{-st}dt = -\frac{1}{s}e^{-st}|_{t=t_0}^{t=\infty} = 0 - (-\frac{1}{s}e^{-st_0}) = \frac{1}{s}e^{-st_0}$
\item Since our solution of the left hand side is the same, $(s^2+1)\mathcal{L}[x(t)](s) = \frac{v_0}{s}e^{-st_0} \Rightarrow \mathcal{L}[x(t)](s) = \frac{v_0e^{-st_0}}{s(s^2+1)}$
\item Partial fractions yield that $ \frac{v_0e^{-st_0}}{s(s^2+1)} = v_0[\frac{1}{s} - \frac{s}{s^2+1}]e^{-st_0}$
\item Looking this up in the table yields that $x(t) = v_0u(t-t_0)(1-\cos(t))$.  So the circuit starts at $t_0$ and oscillates from there.
\end{itemize}

Another example: Third-order equations like $\theta^{(3)}(t) + \theta''(t) - \theta'(t) -\theta(t)= b$.

\begin{itemize}
\item If we assume $\theta(t) = 0, \theta'(t) = 0, \theta''(t) = 0$, then expanding quickly reveals that the left hand is $\mathcal{L}[x(t)](s)(s^3+s^2-s-1)$, exactly mirroring the $\theta$ terms in $s$.
\item The right hand side, transforms as $\mathcal{L}[b] = \frac{b}{s}$, so the whole equation is $\mathcal{L}[x(t)] = \frac{b}{s(s^3+s^2-s-1)} = \frac{b}{s(s+1)(s^2-1)}  = \frac{b}{s(s+1)^2(s-1)}$
\item By Partial Fractions, this becomes $-\frac{1}{s} + \frac{1}{4}\frac{1}{s-1} + \frac{1}{4}\frac{3s+5}{(s+1)^2}$
\item We can tweak this around to $-\frac{1}{s} + \frac{1}{4}\frac{1}{s-1} + \frac{1}{4}\frac{3(s+1)}{(s+1)^2} + \frac{2}{(s+1)^2} $ and use our previous results to get $\theta(t) = -b + \frac{b}{4}e^t + \frac{3b}{4}e^{-t} + \frac{b}{2}te^{-t}$
\end{itemize}

\section{Series Solutions}
\subsection{Power Series (Series Solutions I)}

Main idea: 

\begin{itemize}
\item You can always render $y(x)$ as some $a_0 + a_1x+a_2x^2 + a_3x^3...$
\item This means $y'(x) = a_1+2a_2x+3a_3x^2+...$  and $y''(x) = 2a_2 + 6a_3x+12a_4x^2...$
\item When confronted with, say, $y'(x) = y(x)$, you can directly equate those and see $a_0 = a_1, a_1 = 2a_2, a_2 = 3a_3$, and thus $a_n = a_0\frac{1}{n!}$, leading to $y(x) = e^x$.  
\begin{itemize}
\item Note: This proves you can't solve $y'(x) = y(x)$ with a nontrivial, finite series.
\item Also, in general, we see that solution $a_0e^x = \sum_{n=0}^{\infty} \frac{a_0}{n!}$
\end{itemize}

\item This works with finite sums too, like $y''(x) = 5x^3, y'(0) = 1, y(0) = 0 \Rightarrow a_0 = 0, a_1=1, 20a_5x^3 =5x^3 \rightarrow a_5 = \frac{1}{4} \Rightarrow y(x) = x + \frac{1}{4}x^5$

\end{itemize}

The main idea is that $\sum_{n=0}^{N} a_nx^n$ approaches the real solution as $N \rightarrow \infty$.  Obviously they are equal at $N=0$, but as $x\rightarrow 0$, the solution seems to be more accurate.

Another idea: Using recurrence relations to solve $y'(x) = y(x)$
\begin{itemize}
\item For the $e^x$ example, you can create a recurrence relation like $(n+1)(n+2)a_{n+2} = a_n$
\item Note there are separate even and an odd cascades that land on $a_0$ and $a_1$ respectively.
\item You can eyeball the solution as $\frac{1}{2}\frac{1}{n!}[a_0 + a_1 + (-1)^n(a_0-a_1)]$.
\item POWERFUL TRICK: OR you can use this $a_n, b_n$ POWER RECURRENCE technique
\begin{itemize}
\item Set $a_n = \frac{b_n}{n!}$ (so $b_i$ is ``Blown up'')
\item Note $(n+2)(n+1)a_{n+2}= (n+2)(n+1)\frac{b_{n+2}}{(n+2)!} = \frac{b_n}{n!} = a_n = \frac{b_n}{n!} \Rightarrow b_{n+2}=b_n$.  So, ignoring the factorials in the denominator, b terms are equal all the way down to $a_0$ or $a_1$
\item However, you can hypothesize that $b_n = \alpha r^n$, so $b_{n+2}=\alpha r^{n+2} = b_n = \alpha r^n \Rightarrow r^2 = 1 \Rightarrow r = \pm 1$
\item Thus the solution is any combo of $b_n = \alpha_+ 1^n + \alpha_- (-1)^n$
\item And with $a_0 = b_0, a_1 = b_1$, we can solve to get $\alpha_+ = (a_0 + a_1), \alpha_- = (a_0 - a_1)$, so $b_n =\frac{1}{2}\frac{1}{n!}[(a_0 + a_1)1^n +  (-1)^n(a_0-a_1)]$.
\end{itemize}
\end{itemize}

You can expand this to solve $y''(x) = y(x)$ most generally:
\begin{itemize}
\item Start with $b_n =\frac{1}{2}\frac{1}{n!}[(a_0 + a_1)1^n +  (-1)^n(a_0-a_1)]$.
\item This means y(x)= $\sum_{n=0}^{\infty} \frac{1}{2}\frac{1}{n!}[(a_0 + a_1)1^n +  (-1)^n(a_0-a_1)]x_n$
\item Rearrange to get $y(x) = \frac{a_0+a_1}{2}\sum_{n=0}^{\infty} \frac{1^n}{n!}x^n + \frac{a_0-a_1}{2}\sum_{n=0}^{\infty} \frac{(-1)^n}{n!}x^n =  \frac{a_0+a_1}{2}\sum_{n=0}^{\infty} \frac{1}{n!}x^n + \frac{a_0-a_1}{2}\sum_{n=0}^{\infty} \frac{1}{n!}(-x)^n$
\item This is just $\frac{a_0+a_1}{2}e^x+ \frac{a_0-a_1}{2}e^{-x}$, or since $a_0 = y(0), a_1 = y'(0)$, this is $y(x) = \frac{y(0)+y'(0)}{2}e^x+ \frac{y(0)-y'(0)}{2}e^{-x}$,
\end{itemize}

However, if we're solving $y''(x) = -y(x)$, neat things happen. Follow the above steps exactly, except
\begin{itemize}
\item $b_{n+2}=\alpha r^{n+2} = -b_n = -\alpha r^n \Rightarrow r = \pm i$
\item So our combined solution  is $b_n = [\alpha_+ i^n + \alpha_- (-i)^n]$
\item We then end up with $a_{\pm i} = \pm \frac{1}{2i}$, and $b_n = \frac{i^n}{2i}[1^n-(-1)^n] \Rightarrow a_n = \frac{i^{n-1}}{2n!}[1-(-1)^n]$
\item Noting that $e^{i\theta} = \cos(\theta) + i\sin(\theta)$, we can see that the terms with $i$ on them above are the series for $\sin(x) =  (1 - \frac{x^3}{x!} + \frac{x^5}{5!} - ...$), and the real terms are that of $\cos$
\item Strangely, $\sin$ is periodic, and the infinite sum is too!  Naturally, the finite sum is a polynomial and not periodic.
\end{itemize}

\subsection{Power Series (Series Solutions II)}

Main idea: Centering a series at $c$ rather than at $0$ gives us flexibility (and, I think, converges faster around that point, letting us use fewer terms). The general form: $y(x) = \sum_{n=0}^{\infty}a_n[x-c]^n$


Example: $y'(x)+xy(x) = 0, y(1)=1$

\begin{itemize}
\item A good choice to estimate $y(0.95)$ would be $y(x) = \sum_{n=0}^Na_n[x-1]^n$, since it's close to .95 and we know $y(1)$.
\item Writing terms as recurrence relation:
\begin{itemize}
\item Since we're looking around 1, write as  $y'(x)+[x-1]y(x) + y(x) = 0, y(1)=1$
\item $y = a_0 + a_1[x-1] + a_2[x-1]^2 + a_3[x-1]^3...$
\item $y' = a_1 + 2a_1[x-1] + 3a_2[x-1]^2 + 4a_3[x-1]^3...$
\item $[x-1]y = a_0[x-1] + a_1[x-1]^2 + a_2[x-1]^3...$
\item By writing out terms $y + [x-1]y+y = 0$ shows us $a_n+a_{n-1}+(n+1)a_{n+1} = 0, n \geq 1$
\item Dealing with the boundary cases, we can say $y(x) = a_0 + a_1 + \sum_{n=1}^{\infty} ((n+1)a_{n+1}+a_n+a_{n-1})[x-1]^n=0$
\item Solving for $y(1)=1$, we have $a_0 = 1, a_1=-1$
\end{itemize}

Note: At this point, solving the recurrence relation is difficult, so they embedded a small python script of the series centered around 1. $y(0.95)$ converged to 0.001\% error (against real solution $e^{-\frac{1}{2}(x^2-1)}$) in 4 iterations of the sum (or $N \in [0,3]$).

\end{itemize}

We can also solve the equation analytically:
\begin{itemize}
\item $y'+xy = 0 \Rightarrow \frac{y'}{y} = -x \Rightarrow \int \frac{dy}{y} = \int -x dx \Rightarrow y(x) = Ce^{-\frac{1}{2}x^2}$
\item $Ce^{-\frac{1}{2}} = 1 \Rightarrow C = \sqrt{e} \Rightarrow \sqrt{e}e^{-\frac{1}{2}x^2} = y(x) $
\item Putting this in the form $y(x) = \sum_{n=0}^{\infty} b_nx^n$ means putting $(-\frac{1}{2}x^2)$ in for $x$ and multiplying the result by $\sqrt{e}$.  This means all odd coefficients are 0, and evens are $\sqrt{e}\frac{1}{(-2)^{n/2}}\frac{1}{(n/2)!} = b_n$
\end{itemize}

With the solution in hand we can try our sum over $b_n$ and compare the analytical solution.  
If we center on 0, $y(0.95)$ converges to 0.001\% error in 8 iterations to ($n = 0, 2, ...14$).
This is twice as many as we needed at 1 (4, above)!

Convergence:
\begin{itemize}
\item $\sum_{k=0}^Nc_k = \lim{N\rightarrow \infty} c_k$ converges if and only if $c_k$ decreases sufficiently quickly.
\item If $c_{k+1}/c_k < C < 1$, for some constant $C$, this definitely converges.
\item The \textbf{ratio test} says exactly this: if $\lim_{n \rightarrow \infty} [ \frac{a_{n+1}[x-c]^{n+1}} {a_{n}[x-c]^{n}} ]= \lim_{n \rightarrow \infty} [ \frac{a_{n+1}} {a_{n}} ] |x-c| <1$, the series converges.
\item For example, $a_n = \frac{i^{n-1}}{2n!}[1-(-1)^n]$ converges:
\begin{itemize}
\item First, drop all the zero (even) terms by rewriting to $\sum_{j=0}^{\infty} \frac{(-1)^j}{(2j+1)!}x^{2j+1}$
\item Apply the ratio test between $c_{j+1}x^{2(j+1)+1}$ and $c_jx^{2j}$
\item $|\frac{c_{j+1}}{c_j}| = \frac{1}{(2j+3)(2j+2)}$, so this is less than one at some point regardless of $x^3$'s value (meaning x's value).
\item Note this converges to $\sin(x)$.  I suppose the series form of $\sin(\frac{\pi}{4})$ would converge faster than that of  $\sin(\frac{\pi}{4}+ 200\pi)$
\end{itemize}

\end{itemize}

\subsection{The Airy Equation}

Main motivation: The block-spring system, transferring Energy back and forth between kinetic ($\frac{1}{2}mv^2$) and potential (U(x) = $\frac{1}{2}kx^2)$ to a fixed sum total $E = \frac{1}{2}mv^2 + \frac{1}{2}kx^2$).  The turning point $x_0$ is where $v=0$ and the block switches directions.  The block can never see $x > x_0$ in this case.

There's a quantum version of this in which the ``block'' does escape this bound.

Schroedinger's equation rewritten: $i\frac{\delta \psi}{\delta t} = -\frac{1}{2} \frac{\delta ^2 \psi}{\delta x^2} + U(x)\psi(x,t)$. 
\begin{itemize}
\item Separate variables by saying $\psi(x,t) = X(x)T(t)$ take take derivatives.
\item Divide by $XT$: $iXT'= -\frac{1}{2} X''T + U(x)XT \Rightarrow T'/T= \frac{1}{2} iX''/X + U(x)$
\item Somehow (TODO)  we end up with $\psi(x,t) = y(x)e^{-iEt}$, where $y(x)$, a renamed $X(x)$, has property $-\frac{1}{2} y''(x)+U(x)y(x) = Ey(x) = -\frac{1}{2}y''(x)+\frac{1}{2}x^2y(x)$
\item Near $x_0$, we can approximate y(x) with:
\begin{itemize}
\item TRICK: $\frac{1}{2}x^2 = \frac{1}{2}([x-x_0] + x_0) = \frac{1}{2}[x-x_0]^2 + x_0[x-x_0] + \frac{1}{2}x_0^2$
\item The first term is very small, and the third term is $E$ since $v=0 $ there.
\item So we can rewrite as $ -\frac{1}{2}y''(x)+\frac{1}{2}x^2y(x) = \approx -\frac{1}{2}y''(x)+ x_0[x-x_0]y(x) + Ey(x) = Ey(x) \Rightarrow y''(x) -  2x_0[x-x_0]y(x) = 0$
\end{itemize}
\end{itemize}

Weird: we're defining a new variable $t = (2x_0)^{\frac{1}{3}}[x-x_0]$, which isn't time, but where $t>0$ is a forbidden region inyto which our block tunnels.  Coupled with $y''-ty=0$:
\begin{itemize}
\item Write out terms to see that $y'(t)-ty(t) = \sum_{n=2}^{\infty} a_nn(n-1)t^{n-2} - \sum_{n=0}^{\infty} a_nt^{n+1} =0$.  It looks like the sums skip over terms at a time!
\item Writing out the first few terms, the sum is $2a_2+\sum_{n=1}^{\infty} [(n+2)(n+1)a_{n+2}-a_{n-1}]t^n = 0$
\item TRICK: Since this is always true, try $t=0$ to see that $a_2$ must be 0.
\item TRICK: Try $t \ne 0$ to see that $[(n+2)(n+1)a_{n+2}-a_{n-1}] = 0$ or that $a_{n+2} =\frac{a_{n-1}} {(n+2)(n+1)}, n \geq 1$ 
\item  We can also write the whole thing now as $a_{n} =\frac{a_{n-3}} {(n)(n-1)}, n \geq 3, a_2=0, a_0,a_1\in\mathbb{R}^3$ 
\item Write out terms to see there are three strings then:
\begin{itemize}
\item $n=3k-1$, where $a_n = 0$
\item  $n=3k$, where $a_n = \frac{a_0}{3k \times 3k-1 \times ...\times 3 \times 2}$
\item  $n=3k+1$, where $a_n = \frac{a_1}{3k+1 \times 3k \times ...\times 4  \times 3}$
\item Ratio test: Dividing successive terms by each other (within each string), we see that $y_0$ term $c_k$ has property $|\frac{c_{k+1}}{c_k}| = \frac{|t^3|}{(3k+3)(3k+2)}$, which, for a fixed t, will eventually be less than one, so the series converges after that point.
\item Similar for $y_1$ series ($3k+1$). So, the whole sum $y(t) = \sum_{k=0}^{\infty}a_{3k-1}t^{3k-1} +  \sum_{k=0}^{\infty}a_{3k}t^{3k} +  \sum_{k=0}^{\infty}a_{3k+1}t^{3k+1} = a_0y_0(t) + a_1y_1(t)$ converges for all $t$.
\end{itemize}


However, looking at the truncated sums, we see that while the block oscillates for $t<0$, for $t>0$ (block in the forbidden zone), the sum zooms to infinity.  This is because the truncated sums can't be periodic - they're finite polynomials.  Only infinite ones (like $\sin(t)$, for example) can oscillate.
\end{itemize}

\subsection{The Wronskian (Determinant)}

Main idea: We had a solution of $a_0y_0(t) + a_1y_1(t)$ for the last problem.  How do we know that $y_0(t)$ and $y_1(t)$ are different?  As in, they are not mulitples of each other or really, they are linearly independent?  We use a matrix format for piles of these solutions and check if the determinant is nonzero.  This can be made clear for $n=2$, but it appears subtle for higher orders.

Note: It's not always easy to tell if two functions (esp. sums) are linearly independent, since $\sin(t)$ and $\cos(t-\frac{pi}{2})$ look different but are of course the same.  Use the Wronskian determinant to tell more accurately.
 
\begin{itemize}
\item Two functions $y_0(t), y_1(t)$ are \textbf{linearly dependent} on an open interval $I$ if there are some $a,b \neq 0$ such that $ay_0(t) + by_1(t) = 0$
\item Note this implies their derivatives are linearly dependent too, by differentiating both sides:  $ay_0'(t) + by_1'(t) = 0$
\item Rearranging into a Wronskian matrix $W[y_0, y_1] = \begin{pmatrix} y_0(t) & y_1(t) \\ y_0'(t) & y_1'(t) \end{pmatrix}$, we see that, since $\begin{pmatrix} a \\  b \end{pmatrix}  \neq 0$, that $\begin{pmatrix} y_0(t) & y_1(t) \\ y_0'(t) & y_1'(t) \end{pmatrix} \begin{pmatrix} a \\  b \end{pmatrix} =
 \begin{pmatrix} 0 \\  0 \end{pmatrix}$ must mean the determinant of $W$ is zero.  (Otherwise we could invert $\begin{pmatrix} 0 \\  0 \end{pmatrix}$ to a nontrivial vector)
 \item This is the standard linear algebra test of linear independence, and works for any $n$ functions: $W[f_1 , f_n](t) = \begin{pmatrix} f_1 & ... & f_n \\ f_1' & ... & f_n' \\ ... & ... & ... \\ f_1^{n-1} & ... & f_n^{n-1} \\ \end{pmatrix} $ is singular if and only if the functions are linearly dependent.
\end{itemize}

Note: This works for $n=2$ for functions $a, b$ since $ab' - a'b \Rightarrow a'/a = b'/b \Rightarrow \ln(a) = \ln(b)+C \Rightarrow a = be^C$
\\
Example: 
\begin{itemize}
\item $\cos$ and $\sin$ are linearly independent since $|\begin{pmatrix} \cos(t) & \sin(t) \\ -\sin(t) & \cos(t) \end{pmatrix}| = 1$
\item $\cosh = \frac{e^x+e^{-x}}{2}$ and $\sinh = \frac{e^x-e^{-x}}{2}$ are too since $|\begin{pmatrix} \cosh(t) & \sinh(t) \\ \frac{d}{dt}[\cosh(t)] & \frac{d}{dt}[\sinh(t)] \end{pmatrix}| = 1$
\item For two functions $y_0, y_1$ that solve the Airy equation $y'' - ty = 0$, we can show that $W$ is not dependent on $t$:
\begin{itemize}
\item Starting with the diff eq,, $y_0''-ty_0 = 0, y_01'-ty_1 = 0 \Rightarrow y_0''/y_0 = t = y_1''/y $
\item Then $y_0y_1''=y_0''y_1 \Rightarrow y_0y_1'' - y_0''y_1 = 0$
\item Determinant $W(t) = y_0y_1'-y_0'y_1$
\item $W'(t) = y_0y_1''+y_0'y_1'-y_0''y_1-y_0'y_1' = y_0y_1'' - y_0''y_1$, which is zero by line 2 above.
\item A zero derivative of $W(t)$ means that $W(t)$ does not depend on $t$.
\item This means that we can use ANY value of $W(t)$ for the Wronskian determinant. For the Airy equations $y_0(0)y_1'(0) - y_1(0)y_0'(0) = (1)(1) - (0)(0) = 1$
\end{itemize}

\end{itemize}

Note that Airy is a special case of $y''(t) + p_1(t)y'(t) + p_2(t)y(t) = 0$, with $p_1 = 0, p_2 = -t$.  We can prove that $W = c\exp(-\int p_1(t)dt)$ is a general solution, with some unknown constant $c$.


\begin{itemize}
\item $y''+p_1y'+p_2y=0 \Rightarrow p_1y'=-y''-p_2y$
\item $W = y_0y_1' - y_0'y_1$
\item $p_1W = y_0(p_1y_1') - y_1(p_1y_0') = y_0(-y_1''-p_2y_1) - y_1(-y_0''-p_2y_0) = y_1y_0''-y_0y_1''$
\item This last term is $-W'(t)$ by above results.  So $p_1W = W' \Rightarrow W = c\exp\{-\int p_1(t)dt\}$
\item 
\end{itemize}

This is \textbf{Abel's formula}, and (somehow) generalizes to solve any $y^{(n)} + p_1(t)y^{(n-1)} + ... + p_ny(t) = 0$, as $W(t) = \det W[y_0, ... y_{n-1}](t) = c\exp\{-\int p_1(t)dt\}$

Appllying Abel's formula to find solutions to $y''(t) + 2y'(t) + y(t) = 0$
\begin{itemize}
\item Find the first (homogeneous linear) solution by hypothesizing $y=e^{rt}$ and seeing $(r^2+2r+1)e^{rt} = 0 \Rightarrow (r +1)^2 = 0 \Rightarrow r= -1 \Rightarrow y = ce^{-t}$
\item (Note : Forget the $c$ for now.  It's the same but cleaner)
\item Then, the Wronskian can help us find the other linearly independent solution.
\item We can find the Wronksian with Abel's formula, $p_1(t) = 2 \Rightarrow W(t) = c\exp \{-\int [2]dt \} = ce^{-2t}$. Again, forget the c.
\item So the determinant equation says there's a function f so that $e^{-t}f' - (-e^{-t}f) = e^{-2t}$
\item Multiply all by $e^t$, to get $f'+f = e^{-t}$
\item Solve that diff eq by using integrating factor $e^t$ (multiply all by $e^t$ again) to see $\frac{d}{dt}[e^ty_1(t)] = 1 \Rightarrow e^ty_1(t) = t \Rightarrow y_1(t) = te^{-t}$
\item Note that if we kept constant $c$ in there, then the solution would have been $y_1(t) = te^{-t}+ce^{-t} = te^{-t}+cy_0(t)$, but the last part isn't linearly independent of $y_0$'s solution.
\item So the whole solution set is $Ce^{-t} + Dte^{-t}$
\end{itemize}

\begin{itemize}
\item TODO
\end{itemize}



\end{document}


